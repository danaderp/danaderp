<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://danaderp.github.io/danaderp/feed.xml" rel="self" type="application/atom+xml"/><link href="https://danaderp.github.io/danaderp/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-25T18:38:50+00:00</updated><id>https://danaderp.github.io/danaderp/feed.xml</id><title type="html">David Nader Palacio</title><subtitle>This is a space for blogging about Causal Software Engeering</subtitle><entry><title type="html">ASTxplainer</title><link href="https://danaderp.github.io/danaderp/blog/2023/astxplainer/" rel="alternate" type="text/html" title="ASTxplainer"/><published>2023-10-16T00:00:00+00:00</published><updated>2023-10-16T00:00:00+00:00</updated><id>https://danaderp.github.io/danaderp/blog/2023/astxplainer</id><content type="html" xml:base="https://danaderp.github.io/danaderp/blog/2023/astxplainer/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>Large Language Models (LLMs) for code are a family of high-parameter, transformer-based neural networks pre-trained on massive datasets of both natural and programming languages. These models are rapidly being employed in commercial AI-based developer tools, such as GitHub CoPilot. However, measuring and explaining their effectiveness on programming tasks is a challenging proposition, given their size and complexity. We believe that the methods for <em>evaluating</em> and <em>explaining</em> LLMs for code are inextricably linked. That is, in order to explain a model’s predictions, they must be reliably mapped to fine-grained, understandable concepts that helps developers to detect how good syntactic elements are being predicted by LLMs. Once this mapping is achieved, new methods for detailed model evaluations are possible. However, most current explainability techniques and evaluation benchmarks focus on model robustness or individual task performance, as opposed to interpreting model predictions.</p> <p>To this end, this blog introduces <strong>ASTxplainer</strong>, an explainability method specific to LLMs for code that enables both new methods for LLM evaluation and AST visualizations of LLM predictions that aid end-users in understanding model predictions. At its core, <strong>ASTxplainer</strong> provides an automated method for aligning code token predictions with AST nodes, by extracting and aggregating normalized model logits within AST structures.</p> <p>To demonstrate the practical benefit of <strong>ASTxplainer</strong>, we illustrate the insights that our framework can provide by performing an empirical evaluation on 12 popular LLMs for code using a curated dataset of the most popular GitHub projects. Additionally, we perform a user study examining the usefulness of an <strong>ASTxplainer</strong>-derived visualization of model predictions aimed at enabling model users to explain predictions. The results of these studies illustrate the potential for <strong>ASTxplainer</strong> to provide insights into LLM effectiveness, and aid end-users in understanding predictions (see our ArXiv <a href="https://arxiv.org/abs/2308.03873">paper</a>).</p> <h2 id="introduction">Introduction</h2> <p>The advent and proliferation of online open-source code repositories and rapid advancements in transformer-based neural large language models LLMs have served as a catalyst for the advancement of automated Software Engineering (SE) tools with effectiveness. LLMs for code have demonstrated considerable proficiency across a diverse array of generative SE tasks, inclusive of, but not restricted to, code completion <d-cite key="Raychev2014CodeCW,MSR-Completion"></d-cite>, program repair &lt;d-cite key=Chen2019sequencer,ahmadunified2021&gt;&lt;/d-cite&gt;, and test case generation <d-cite key="Watson:ICSE2"></d-cite>. Moreover, these advancements are rapidly being introduced into commercial developer tools such as GitHub CoPilot <d-cite key="github_copilot"></d-cite> and Replit’s Ghostwriter <d-cite key="ghostwriter"></d-cite>.</p> <p>However, the sheer complexity and size that enable the often surprising effectiveness of LLMs for code is a double-edged sword. That is, while these attributes enable LLMs to capture important patterns in code that allow them to be applied to a range of programming tasks, effectively <em>explaining</em> and <em>evaluating</em> the capabilities of these models is a challenging proposition — they effectively function as <strong>black boxes</strong> that derive predictions from exceedingly complex internal model mechanics. Current research in both designing LLMs for code and in applying them to programming tasks typically makes use of existing benchmarks (e.g., CodeSearchNet~&lt;d-cite key=husain2019codesearchnet}&gt;&lt;/d-cite&gt;, or HumanEval~&lt;d-cite key=chen_evaluating_2021}&gt;&lt;/d-cite&gt; and metrics that have been adapted from the field of natural language processing (NLP) such as accuracy, BLEU, METEOR, and ROUGE, as well as more recent metrics further tailored for code such as CodeBLEU~<d-cite key="ren_codebleu_2020"></d-cite>. However, recent work has illustrated the limitations of benchmarks such as HumanEval~<d-cite key="liu2023code"></d-cite>, and there has been growing criticism of automated metrics within the NLP community~&lt;d-cite key=molnar2019interpret,Kim2018InterpretabilityTCAV,wan_what_2022,liu_reliability_2023&gt;&lt;/d-cite&gt;. These deficiencies largely stem from the fact that such benchmarks and metrics are often targeted at evaluating functional or syntactic correctness of generated code or task performance, but are not able to <em>explain model predictions or capabilities in an interpretable manner</em>.</p> <p>Methods for <em>evaluating</em> (i.e., the <em>what</em>) and <em>explaining</em> (i.e., the <em>why</em>) LLMs for code are inextricably linked to one another. An informative evaluation requires some degree of explainability of model predictions, such that model behavior can be understood at <em>a fine-grained level</em>. However, the fundamental challenge in achieving explainability of LLMs for code lies in establishing a reliable mapping mechanism that can bridge the gap between a given model’s predictions and human-understandable programming language (PL) concepts, which can aid in explaining the model’s decisions. As such, designing both effective evaluations and interpretability techniques for LLMs of code requires that one first establish this conceptual mapping.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/danaderp/assets/img/blog_astxplainer/fig_1_generative_process_astxplainer-480.webp 480w,/danaderp/assets/img/blog_astxplainer/fig_1_generative_process_astxplainer-800.webp 800w,/danaderp/assets/img/blog_astxplainer/fig_1_generative_process_astxplainer-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/danaderp/assets/img/blog_astxplainer/fig_1_generative_process_astxplainer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Our proposed evaluative and explainability method is composed of ASC-EVal, ASC-Causal, and ASC-Viz. </div> <p>To overcome the challenges in explaining and evaluating LLMs for code, we propose a novel method for enabling a reliable conceptual mapping of LLMs predictions (i.e., the <em>what</em>) to PL concepts (i.e, the <em>why</em>), called <strong>ASTxplainer</strong>, which collects and aggregates LLMs token predictions into a construct that we call <strong>Abstract Syntax Concepts</strong> (<em>ASC</em>), derived from Abstract Syntax Trees (ASTs). By explicitly mapping model predictions to code structure, <strong>ASTxplainer</strong> provides a fine-grained methodology for examining <em>how</em> models perform relative to programming language concepts, and can help model end-users reason about <em>why</em> an LLMs may have made a certain set of predictions. <strong>ASTxplainer</strong>’s mapping of model predictions to <em>ASC</em> enables two new types of evaluations for LLMs of code, and one novel interpretability technique that visualizes model <em>ASC</em> to aid end users (i.e., developers using LLMs to auto-complete code) in understanding LLMs predictions. Fig.~1 illustrates these three main components of <strong>ASTxplainer</strong>.</p> <p>The first evaluation technique, called <code class="language-plaintext highlighter-rouge">ASCeval</code>, is able to estimate the structural performance of a predicted syntax element in order to measure the uncertainty of the downstream code generative process (e.g., for code completion). The second evaluation technique called <em>ASCcausal</em>, is capable of generating causal explanations that link these structural performance values with canonical model performance (i.e., Cross-Entropy Loss). Finally, <em>ASCviz</em> implements a practical interpretability technique by visualizing model LLMs prediction uncertainty, organized into AST structures, aiding end-users in understanding the reliability of model predictions in practice. This blog concentrates on explaining <code class="language-plaintext highlighter-rouge">ASCeval</code>. The other techniques can be found on the <a href="https://arxiv.org/abs/2308.03873">preprint</a>. We validate <code class="language-plaintext highlighter-rouge">ASCeval</code> and <em>ASCcausal</em> through a large-scale, comprehensive empirical study that evaluates 12 popular LLMs on a novel dataset of \(\approx\) 10 million tokens that are exclusive of the model’s training data. Furthermore, to evaluate the effectiveness of <em>ASCviz</em>, we conduct a user study examining the utility of multiple visualizations in aiding developers to understand and explaining model predictions. The results of our empirical study lead to novel insights regarding the performance of LLMs for code, and user study illustrates the promising utility of <em>ASCviz</em>.</p> <h2 id="background--related-work">Background &amp; Related Work</h2> <p><strong>ASTxplainer</strong> is an approach that converges the expectation of an evaluative technique with the rigurosity of an explainability technique to quantify the prediction uncertainty of LLMs for code. LLMs are the result of scaling up billions of parameters for context-aware word representations from pre-trained models <d-cite key="zhao_survey_2023"></d-cite>. This section defines and formalizes the basic elements of our approach. We provide a definition of LLMs and how to evaluate them, the definition of Abstract Syntax Trees (ASTs) and how they were employed for probing, and finally, the explainability methods for LLMs.</p> <p>Our research focused on LLMs because of their outstanding performance on code-based generative tasks. While other representations exist, such as graph-based models &lt;d-cite key=allamanis2018learning,Allamanis19&gt;&lt;/d-cite&gt;, we focus our discussion on sequence-based representations for simplicity. The goal of sequence-based models is to statistically learn a representation of a software artifact (e.g., snippet, comments, or test cases). We refer to SE-specific sequence-based data as a software corpus \(\mathcal{S}\). Given the sequential nature of \(\mathcal{S}\), we can decompose \(\mathcal{S}\) into a desired granularity of tokens, words, or sub-words <d-cite key="Karampatsis2019"></d-cite> by using a transformation function \(\Gamma(\mathcal{S})= w_1,...,w_I\) (i.e., <em>tokenizers</em>). This transformation function is a tokenization method for converting a software corpus into a sequence of discrete objects $w_i$ for \(1 \leqslant i \leqslant I\). Note that \(w_i \in V\), where the vocabulary \(V\) is a finite set.</p> <p>Given this definition, a statistical language model is a probability distribution \(P\) over a fixed granularity of sequences of software corpora \(\mathcal{S}\). We can factorize the joint distribution over the \(i-\)dimension as:</p> <p>\(P(\mathcal{S}) = P(w_1,...,w_I) = \prod_{i = 1}^{I} P(w_i | w_{&lt;i})\).</p> <p>Due to the discrete nature of the data, the expression \(P(w_i | w_{&lt;i})\) can be estimated using a machine learning classifier. The classifier, in our particular case, is a Large Language Model (LLM) <d-cite key="Bengio2003AModel"></d-cite>. Hence, rather than using <em>n</em>-grams or Markov Models to approximate \(P(w_i | w_{&lt;i})\) <d-cite key="Karampatsis2020OpenVocabularyAbstract"></d-cite>, it is convenient to use a latent model \(P(w_i | w_{&lt;i} ) \approx P(w_i | h_i )\), where \(h_i\) is known as a <em>hidden state</em> that embeds the sequence information from past observations up to the time step \(i\).</p> <p>Depending on <em>how</em> the sequence is processed, the hidden state \(h_i\) can be computed using either <em>Encoder-Only</em>, <em>Encoder-Decoder</em>, or <em>Decoder-Only</em> architectures according to the <em>transformers’</em> layers <d-cite key="vaswani2017transformers"></d-cite> One popular bidirectional objective function used widely in representation learning is <em>masked language</em> modeling <d-cite key="devlin_bert_2019"></d-cite>. This function aims to predict masked text pieces based on the surrounding context. CodeBERT <d-cite key="feng_codebert_2020"></d-cite>, CuBERT (345M) <d-cite key="kanade_learning_2020"></d-cite> CodeRoBERTa <d-cite key="lin_span_2022"></d-cite>, and GraphCodeBERT <d-cite key="guo_graphcodebert_2021"></d-cite> are examples of <em>Encoder-Only</em> models for code. In programming contexts, these methods provide useful representations of code sequences for downstream tasks such as code classification, clone and defect detection. CodeT5 <d-cite key="wang_codet5_2021"></d-cite> and PLBART <d-cite key="ahmad_unified_2021"></d-cite> are examples of <em>Encoder-Decoder</em> models. These models encode an input sequence and, then, this encoded sequence is decoded with a different architecture. Encoder-Decoder models are trained with the goal of reconstructing masked input sequences <d-cite key="lewis_bart_2019"></d-cite>. Additionally, they have been employed for SE tasks such as code summarization, and code generation using masks<d-cite key="wang_codet5_2021"></d-cite>. Finally, <em>Decoder-Only</em> models predict the probability of a token given a preceding sequence. CodeGPT <d-cite key="lu_codexglue_2021"></d-cite>, CodeParrot <d-cite key="codeparrot"></d-cite>, GPT-Neo <d-cite key="black_sid_2021_5297715"></d-cite>, GPT-J <d-cite key="gptj"></d-cite>, Codex <d-cite key="openai_codex"><d-cite>, GPT-NeoX <d-cite key="GPTNeoX"></d-cite>, and Google's left-to-right decoder-only Transformer language models &lt;d-cite key=vaswani2017transformers,austin2021program&gt;</d-cite> are examples of _Decoder-Only_ models for code.</d-cite></p> <p>Although our proposed approach <strong>ASTxplainer</strong> was designed to be compatible with either type of LLMs, this research concentrated on <em>Decoder-Only</em> models due to their popularity for code-based generative tasks <d-cite key="xu_systematic_2022"></d-cite>. Decoder-based models share a common property: <em>the ability to connect previously processed information to a present task, such as using an initial sequence of tokens to predict new code tokens</em>. The resulting auto-completed sequence should be coherent with respect to the context of the initial sequence. This property is known as the ability to model <strong>long-range dependencies</strong> <d-cite key="karpathy2015understand"></d-cite>.</p> <p><em>Definition 1.</em> <strong>[Decoder-Only Transformers]:</strong> Decoder-Only models update the hidden state \(h_i = f(h_{i-1}, w_{&lt;i})\) using past inputs \(w_{&lt;i}\) and a previous hidden state \(h_{i-1}\). In other words, these models function in a feed-forward manner that predicts future values from historical values directly. LLMs trained on source code have the ability to generate tokens or sub-words given a history. Hence, decoder-only models are employed as generative models:</p> <p>\(\hat{w_i} \approx P(w_i | w_{&lt;i} ) = \sigma(y)_i = \frac{e^{y_{w_i}}}{\Sigma_j e^{y_j}}\).</p> <p>In the previous approximation, the predicted token \(w_i\) is <em>conditioned</em> by the past information. The term \(y_j\) represents the <em>non-normalized log-probabilities</em> for each output token \(j\). We extracted and normalized these <strong>log-probabilities</strong> from the last layer of LLMs to estimate the <strong>Next-token Predictions</strong> (<em>NtP</em>) in <strong>ASTxplainer</strong> . This estimation relies on the softmax function. The softmax \(\sigma_i\) returns a distribution over predicted output classes, in this case, the classes are each token in the previously introduced vocabulary \(V\). It is expected that the predictions contained in \(\sigma_i\) are influenced by previous inputs of the sequence \(w_{&lt;i}\).</p> <p><em>Probing</em> is a supervised analysis to determine which type of parameters (e.g., input code snippets, tokenization process, number of hidden layers, and model size) influence the learning process in machine learning models <d-cite key="troshin_probing_2022"></d-cite>. The purpose of probing is to assess whether hidden representations of machine learning models (i.e., LLMs) encode specific linguistic properties such as syntactic structures of programming languages. For example, Lopez et al., <d-cite key="lopezastprobe2022"></d-cite> trained a linear classifier to show that code syntactic structures are encoded in pre-trained models in the form of Abstract Syntax Trees (ASTs). Lopez et al.’s approach demonstrates that the middle layers of pre-trained models contain ASTs’ information<d-cite key="lopezastprobe2022"></d-cite>.</p> <p>Nonetheless, instead of proposing another syntax probe, our approach <strong>ASTxplainer</strong> adapts AST information to evaluate and explain LLMs. ASTs are defined as a formal representation of syntactical structures built upon linguistic elements of PLs. ASTs are formed according to the production rules defined in Context Free Grammar (CFGs). More precisely, production rules are functions that combine terminal and non-terminal nodes into statements. Terminal nodes are symbols in the source code (e.g., tokens in region (3) of Fig.~2), while non-terminal nodes encapsulate more than one terminal node to define the structure of a statement (e.g., nodes containing children in region (2) of Fig.~2).</p> <p>When designing our approach <strong>ASTxplainer</strong> , we leveraged meaningful and interpretable information defined in Context-Free Grammars (\(CFGs\)). \(CFGs\) are a set of rules containing the syntax and structural information of a language &lt;d-cite key=10.5555/1196416&gt;&lt;/d-cite&gt;. Ultimately CFGs define instructions that specify how different tokens (i.e., Lexemes) are put together to form valid statements in every programming language.</p> <p><em>Definition 2.</em> <strong>[Context Free Grammars]:</strong> $CFG$ \(\mathbb{G}\) is expressed as \(\mathbb{G} = (\alpha, \lambda, \omega, \beta)\) where \(\alpha\) denotes the finite set of non-terminal symbols, \(\lambda\) the finite set of terminal symbols, \(\omega\) the finite set of production rules and \(\beta\) the start symbol. The set of production rules \(\omega\) for any type of statement (e.g., conditional, assignation, operator) is expressed in terms of the terminal and non-terminal symbols.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/danaderp/assets/img/blog_astxplainer/fig_2_AST_tree2-480.webp 480w,/danaderp/assets/img/blog_astxplainer/fig_2_AST_tree2-800.webp 800w,/danaderp/assets/img/blog_astxplainer/fig_2_AST_tree2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/danaderp/assets/img/blog_astxplainer/fig_2_AST_tree2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Local Evaluation for code Completion. </div> <h2 id="the-asc-eval-component">The ASC-Eval Component</h2> <p>LLMs for code can be considered a black box because of their uncertain behavior when predicting tokens. To estimate such uncertainty, we can employ <em>explainability</em> methods on LLMs. Explainability aims to understand how a model operates and comes to decisions either by exploring inner layers or <strong>performing perturbation analysis on the models’ inputs</strong> &lt;d-cite key=belleprinciples2020,molnarinterpretable2020&gt;&lt;/d-cite&gt;. For example, Gholizadeh et al., <d-cite key="gholizadeh_model_2021"></d-cite> propose a local explainability technique, namely layer-wise relevant propagation (LRP), that computes the importance of an interpretable <em>n</em>-gram in classifying a text sequence. LRP calculates a score with the sum of activated weights during the back-propagation to identify the most influential <em>n</em>-grams. This score is employed for explaining the importance of a given <em>n</em>-gram for a canonical (i.e., SVM) and a neural model(i.e., CNN). The authors demonstrated that LRP outperforms the gradient-only-based and permutation-only-based explainability techniques <d-cite key="gholizadeh_model_2021"></d-cite>. It is important to clarify that, in our research, <em>explainability</em> and <em>interpretability</em> are used interchangeably. However, the goal of our research is to introduce a technique that provide a fine-grained explanation of accuracy-based metrics based on syntax elements of Programming Languages.</p> <p>In the context of pre-trained models for code, Liu et al., experimented with Encoder-Decoder models for code2code and comment2code tasks (e.g., T5, CodeText, and CodeTrans). Their research aims at explaining why neural models generate code sequences reliably by identifying tokens that contribute the most to a sequence prediction <d-cite key="liu_reliability_2023"></d-cite>. Moreover, Vasconcelos et al., propose a technique that highlights generated code using an uncertainty threshold. Their approach points out fragments of the sequence where developers can intervene upon the uncertainty threshold <d-cite key="vasconcelos_generation_2023"></d-cite>. On the other hand, we can explain pre-trained models for code using structural information. For instance, Wan et al., conducted an interpretability analysis on Encoder-only models (e.g., CodeBert and GraphCodeBert) focusing on three aspects: 1) how the self-attention weights align with the syntax structure, 2) whether the syntax structure is encoded in the hidden layers, and 3) how pre-trained models induce syntax structure <d-cite key="wan_what_2022"></d-cite>.</p> <p>Even though previous research has introduced explainability techniques to analyze pre-trained models with structural information, those techniques have been tested and designed for modest-size Encoder-Only models (i.e., less than 1B). Conversely, our study <strong>ASTxplainer</strong> proposes not only an explainability technique that contextualizes canonical metrics (i.e., cross-entropy loss) based on <em>causal inference</em> but also an evaluative metric (<code class="language-plaintext highlighter-rouge">ASCeval</code>) for Decoder-only LLMs that predicts ASTs terminal and non-terminal nodes. More importantly, we introduce and control a set of confounders based on code features (e.g., AST-levels, AST-nodes, and number of tokens) to properly estimate the relationship between <code class="language-plaintext highlighter-rouge">ASCeval</code> and canonical metrics (see Tab.~2 in our <a href="https://arxiv.org/abs/2308.03873">preprint</a>).</p> <p>Kim et al., <d-cite key="Kim2018InterpretabilityTCAV"></d-cite> introduce a formal mathematical structure known as a <strong>function for explainability</strong> (\(\varphi\)). We use this definition to formally describe what constitutes an explainable method in SE. Most LLMs for code operate by predicting tokens \(P(w_i | d_i)\) that do not <em>inherently</em> match high-level concepts a human can easily understand. Kim et al., claim that such difficulty can be expressed mathematically as representing the state of LLMs as a vector space (\(\vec{m}\)). Conversely, humans or, in our study, developers operate in a different vector space \(\vec{h}\), which corresponds to an unknown set of <strong>human-interpretable concepts</strong> (\(h\)). As such, our main challenge is to map \(\vec{m} \to \vec{h}\) bridging this gap between the disparate vector spaces. The <em>key insight</em> of <strong>ASTxplainer</strong> is the formalization of an explainability function \(\varphi\) for LLMs of code.</p> <p><em>Definition 3.</em> <strong>[Interpretability Function for Next Token Predictions]:</strong> Consider \(\varphi: \vec{m} \to \vec{h}\). In this formulation, \(\vec{m}\) represents an approximation of a model’s vector space as measured through token prediction performance at different granularity levels (i.e., normalized log-probabilities). This vector space approximation is then mapped to human-understandable concepts \(\vec{h}\) that represent programming language syntactic concepts (i.e., terminal and non-terminal nodes).</p> <p>While LLMs have seen striking advances with regard to code generation and other downstream SE tasks &lt;d-cite key=Chen2021EvaluatingCode,watson2020dl4se&gt;&lt;/d-cite&gt;, researchers are still not able to evaluate what aspects of code are actually statistically learned by these models. In this section, we propose a new metric, <code class="language-plaintext highlighter-rouge">ASCeval</code>, to showcase the statistical behavior of syntactic elements generated by LLMs. Our proposed <code class="language-plaintext highlighter-rouge">ASCeval</code> comprises the basic units for explainability as Abstract Syntax Concepts (<em>ASC</em>), an alignment function \(\delta\) that links tokens with ASTs, and an aggregation function \(\theta\) that estimates the prediction performance of a terminal and non-terminal nodes. We propose an explainability function \(\varphi\) that relies on the alignment function $\delta$ and the aggregation function \(\theta\) to perform the mapping from log-probabilites (i.e., <em>NtP</em>) to developer-understandable concepts (i.e., <em>ASC</em>).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/danaderp/assets/img/blog_astxplainer/fig_1_ast_eval-480.webp 480w,/danaderp/assets/img/blog_astxplainer/fig_1_ast_eval-800.webp 800w,/danaderp/assets/img/blog_astxplainer/fig_1_ast_eval-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/danaderp/assets/img/blog_astxplainer/fig_1_ast_eval.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. _ASC_eval Components. Left: Nodes are employed as ``concepts''. Center: Each token is aligned to the end nodes of the AST with an offset function. Right: Node probabilities are estimated with an aggregation function. </div> <h3 id="abstract-syntax-concepts-asc">Abstract Syntax Concepts ($ASC$)</h3> <p><code class="language-plaintext highlighter-rouge">ASCeval</code> can be formally defined (see Def.~3) as an explainability function \(\varphi\) of token predictions of LLMs using Context Free Grammars. We introduce the term <strong>Abstract Syntax Concepts</strong> (<em>ASC</em>) to represent the terminal and non-terminal symbols in a Context Free Grammar (see Def~.2). Specifically, to approximate a LLMs’ vector space, in \(\vec{m}\), we extract the last layer to calculate <em>NtP</em>, which is, in fact, a generative measure of performance. Then in \(\vec{h}\), we map the model’s prediction performance at the token level (<em>NtP</em>) to <em>ASC</em> (for which we define a set of categories $\mathcal{H}$), to make it easier to interpret what aspects of LLMs are <em>effective</em> or <em>erroneous</em> at predicting.</p> <p>In PLs, terminal and non-terminal nodes retain different semantic meanings. For instance, <code class="language-plaintext highlighter-rouge">identifier</code> and <code class="language-plaintext highlighter-rouge">string</code> nodes correspond to a common <em>Natural Language</em> concept category. As such, we can group nodes $n$ into semantically meaningful <em>categories</em> \(\mathcal{H}\). Fig.~\ref{fig:largeTreeMap} depicts some of our proposed categories for Python. These categories will allow <code class="language-plaintext highlighter-rouge">ASCeval</code> to assign semantic meaning to predicted <em>ASC</em>. <em>ASC</em> are the fundamental mathematical units for enabling the evaluation and explainability of LLMs. Fig.~4 depicts some of the concepts used to evaluate LLMs with <code class="language-plaintext highlighter-rouge">ASCeval</code>. Concepts \(n \in N\) are types of symbols defined by tree-sitter’s $CFG$ &lt;d-cite key=tree-sitter&gt;&lt;/d-cite&gt;. In summary, Each token in a sequence \(s\) can be assigned to a category \(h \in \mathcal{H}\). With our categories \(\mathcal{H}\), researchers and developers can easily associate LLMs’ performance to particular structural code attributes. As such, <code class="language-plaintext highlighter-rouge">ASCeval</code> allows for LLMs Next-token Predictions to be explained in a developer-centric way.</p> <p>Fig~3-A depicts the AST representation of a Python snippet of a naive implementation of the function \(countCharts\). This function counts and returns the number of occurrences of a given character for an input string. In the AST representation, the leaf nodes correspond to the terminal tokens used in the snippet, while the intermediate nodes correspond to non-terminals. Our approach relies on the tree-sitter library &lt;d-cite key=tree-sitter&gt;&lt;/d-cite&gt; to construct the AST representations of the snippets. Once the AST has been parsed, we can access the information for all nodes and retrieve useful properties such as their type, children, and location.</p> <h3 id="ast-alignment-function-delta">AST Alignment function ($\delta$)</h3> <p>Figure~3-B illustrates the process of aligning terminal and non-terminal nodes in the AST representation with their corresponding tokens. Prior to this alignment process, we split the $countCharts$ snippet \(s\) into tokens using the model tokenizer \(\Gamma(s) = (w_1,...,w_i)\). Since the tokenizer may produce a sequence of tokens where each token does not necessarily matches with a single terminal node, a single node in the AST may contain more than one associated token. In fact, intermediate nodes are aligned with a sub-sequence of the original snippet rather than a single token. We define for this purpose the alignment function \(\delta: N \to s_{&lt;=i}\) where \(s_{&lt;=i}\) corresponds to a subsequence of a snippet and $N$ is the set of terminal and non-terminal nodes. We leverage the offset property of each AST node to conduct this process, in other words, we search for all the tokens in \(s\) that are located within the offset range of each node. To illustrate how function $\delta$ works, let’s consider the example in Figure~3-B, in the sub-tree the terminal node <code class="language-plaintext highlighter-rouge">(</code> is aligned with token <code class="language-plaintext highlighter-rouge">{(}</code> while the sibling node <code class="language-plaintext highlighter-rouge">identifier</code> is aligned with tokens <code class="language-plaintext highlighter-rouge">{str}</code> <code class="language-plaintext highlighter-rouge">{ing}</code>. The parent node <code class="language-plaintext highlighter-rouge">parameters</code> will be consequently aligned with <code class="language-plaintext highlighter-rouge">{(}</code> <code class="language-plaintext highlighter-rouge">{str}</code> <code class="language-plaintext highlighter-rouge">{ing}</code> <code class="language-plaintext highlighter-rouge">{,}</code> <code class="language-plaintext highlighter-rouge">{char}</code> <code class="language-plaintext highlighter-rouge">{acter}</code> <code class="language-plaintext highlighter-rouge">{)}</code>.</p> <h3 id="ast-aggregation-function-theta">AST Aggregation function (\(\theta\))</h3> <p>We design an aggregation function \(\theta\) that computes our proposed metric <code class="language-plaintext highlighter-rouge">ASCeval</code>, which represents how confident a terminal or non-terminal node $n$ is predicted by an \llm. By relating these node predictions to an actual node symbol, we gain an understanding of how well a studied model is <em>generating code</em>. These <code class="language-plaintext highlighter-rouge">ASCeval</code> performance values can also uncover specific long-range interactions and map them into an AST visual structure (see Sec.~\ref{sec:approach-3}). <code class="language-plaintext highlighter-rouge">ASCeval</code> performs at two levels of granularity depending on the scope of the analyzed corpus \(\mathcal{S}\). We refer to such granularity as <em>local</em> and <em>global</em> aggregation. Local aggregations operate for a code snippet, while global aggregations operate for a corpus. Although local aggregation can provide a <code class="language-plaintext highlighter-rouge">ASCeval</code> value for a single snippet, this aggregation allows computing an average of aggregated values at snippet granularity.</p> <p>Figure~3-C shows the aggregation function used to compute the prediction probability for each node. Once the tokens are aligned with their corresponding nodes using \(\delta\), we traverse the entire AST and aggregate the <em>NtP</em> probabilities of their associated tokens. The aggregation function \(\theta\) can take the form of a statistical average, median or max values depending on the user configuration. In our study, we set the aggregation \(\theta: N \to median(\delta(N))\) for a subset of tokens \(s_{&lt;=i}\). For example, as illustrated in Fig.~3-C, the parent node <code class="language-plaintext highlighter-rouge">parameters</code> has an associated average value of $0.23$. This parent node average was aggregated with its terminal values: <code class="language-plaintext highlighter-rouge">(</code> with \(0.07\), <code class="language-plaintext highlighter-rouge">identifier</code> with $0.4$, <code class="language-plaintext highlighter-rouge">,</code> with \(0.5\), <code class="language-plaintext highlighter-rouge">identifier</code> with $0.1$, and <code class="language-plaintext highlighter-rouge">)</code> with $0.1$.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/danaderp/assets/img/blog_astxplainer/Concepts_tree_map-480.webp 480w,/danaderp/assets/img/blog_astxplainer/Concepts_tree_map-800.webp 800w,/danaderp/assets/img/blog_astxplainer/Concepts_tree_map-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/danaderp/assets/img/blog_astxplainer/Concepts_tree_map.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4. `ASCeval` for 10 _ASC_ Categories and 2 LLMs (\monoIIII and \gptI). </div> <h2 id="results">Results</h2> <p>In order to illustrate the insights that <strong>ASTxplainer</strong> can enable, we present an empirical evaluation on 12 LLMs, which shows how LLMs behave for each Abstract Syntax Concept, and a user study, which assesses the usability of our approach. This section details the methodological steps and results for only the first research question. Please refer to the <a href="https://arxiv.org/abs/2308.03873">pre-print</a> for the other questions and further details.</p> <p>$RQ_1$: <em>To what extent do Large Language Models for code predict syntactic structures?</em></p> <p>To answer $RQ_1$, we generated the normalized log-probabilities or Next Token Predictions (<em>NtP</em>) for each code snippet in $\mathcal{S}=$ <em>galeras</em>. These log-probabilities were extracted at inference time for each token position for the 12 LLMs. The log-probabilities distributions have a vector size of |\(V\)| for each token position in \(s \in \mathcal{S}\). These distributions are processed to obtain the log-probability that actually matches the expected token in a position \(i\). Therefore, each token position has an associated prediction value that we save for generating the <em>NtP</em> sequence. Such Next-token Prediction sequence is the input for the aggregation function \(\theta\) that generates the corresponding <code class="language-plaintext highlighter-rouge">ASCeval</code> values. Additionally, we computed the cross-entropy loss of each snippet \(s\) in our dataset. To obtain the <code class="language-plaintext highlighter-rouge">ASCeval</code> <em>Global</em> value in Tab.~1 and Fig.~5, we aggregated <code class="language-plaintext highlighter-rouge">ASCeval</code> performance values (i.e., all available $ASC$) by LLM. The values per model are bootstrapped with the median (size of 500 samplings) to enable a fair comparison among models. Similarly, to obtain the <code class="language-plaintext highlighter-rouge">ASCeval</code> per Abstract Syntax Concept Category (e.g., Data Str, Decision, or Scope), we globally aggregated performance values of tokens under these categories. We also explored with Type Model aggregations.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/danaderp/assets/img/blog_astxplainer/table_results-480.webp 480w,/danaderp/assets/img/blog_astxplainer/table_results-800.webp 800w,/danaderp/assets/img/blog_astxplainer/table_results-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/danaderp/assets/img/blog_astxplainer/table_results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Table 1. Large Language Models characteristics and their associated `ASCeval` performance. Erroneous `ASCeval` values are in red. Confident `ASCeval` values are in blue. Best global `ASCeval` is underlined. </div> <p>In this $RQ_1$, we provide an empirical value (bootstrapped median columns in Tab.~1) of the prediction of Abstract Syntax Concepts for the 12 LLMs. We set a threshold of $0.6$ as an acceptable rate of prediction confidence for our <code class="language-plaintext highlighter-rouge">ASCeval</code> metric. Fig.~4, for example, shows our best and worst LLMs, <em>mono-lang [2B]</em> and <em>gpt-3 [125M]</em> respectively, at every proposed Abstract Syntax Concept. We observe that, in general, scaling the parameters of LLMs plays a fundamental role in the prediction of $ASC$. The dashed green boxes show the largest <code class="language-plaintext highlighter-rouge">ASCeval</code> performance increments from the worst to the best concepts. Particularly, <code class="language-plaintext highlighter-rouge">{Exceptions}</code>, <code class="language-plaintext highlighter-rouge">{Natural Language}</code>, <code class="language-plaintext highlighter-rouge">{Operators}</code>, <code class="language-plaintext highlighter-rouge">{Types}</code>, and <code class="language-plaintext highlighter-rouge">{Decisions}</code> present the biggest jumps in syntactic <code class="language-plaintext highlighter-rouge">ASCeval</code> performance.</p> <p>Our empirical evaluation shows that $ASC$ categories that fulfill the $0.6$ threshold for the 12 LLMs are <code class="language-plaintext highlighter-rouge">{Scope}</code> with the highest <code class="language-plaintext highlighter-rouge">ASCeval</code> performance of $0.94$ for the <em>Mono-Language-Type</em> models, <code class="language-plaintext highlighter-rouge">{Iterations}</code> with $0.82$ for <em>codegen-nl [2B]</em>, and <code class="language-plaintext highlighter-rouge">{Testing}</code> with $0.85$ for <em>mono-lang [2B]</em> (see Tab.~1). Conversely, we found some concept categories struggle with <code class="language-plaintext highlighter-rouge">ASCeval</code> performance. We refer to these categories as <code class="language-plaintext highlighter-rouge">{erroneous}</code> since they are below $0.5$. Those categories are mainly <code class="language-plaintext highlighter-rouge">{Natural Language}</code> category with the largest average median of $0.46$ and <code class="language-plaintext highlighter-rouge">{Data Types}</code> with the largest average median of $0.47$ for <em>NL GPT-3</em>.</p> <p>We believe that models poorly behave with low <code class="language-plaintext highlighter-rouge">ASCeval</code> performance because category concepts such as <code class="language-plaintext highlighter-rouge">{Natural Language}</code> and <code class="language-plaintext highlighter-rouge">{Data Types}</code> require more context to be accurately predicted. For instance, the <code class="language-plaintext highlighter-rouge">string</code> concept requires a larger window context before properly being predicted. Similarly, the category <code class="language-plaintext highlighter-rouge">{Data Types}</code> is prone to be erroneous since they may appear more frequently at the beginning of the snippets compared to other categories. Also, bear in mind that <code class="language-plaintext highlighter-rouge">{Data Types}</code> are less frequent concepts due to the dynamic typing for Python. In general, none of the evaluated architectures performed well at predicting <code class="language-plaintext highlighter-rouge">{Data Types}</code> accurately except by <em>mono-lang [2B]</em>, which was trained with a large number of code samples.</p> <p>Table~1 depicts that <code class="language-plaintext highlighter-rouge">{Iteration}</code> category mostly surpasses the threshold for all our models except for <em>codeparrot-small-multi</em> with an average median <code class="language-plaintext highlighter-rouge">ASCeval</code> of $0.6$. From our smaller models (i.e., in a range of millions of parameters), the lowest average median obtained for <em>gpt-3 [125M]</em> is $0.74$, which also surpasses the threshold. This outstanding behavior of <em>NL GPT-3</em> models could be explained as Python reserved words for iterations such as <strong>for</strong> and <strong>while</strong> also appear in natural language with similar semantics.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/danaderp/assets/img/blog_astxplainer/asc_performance-480.webp 480w,/danaderp/assets/img/blog_astxplainer/asc_performance-800.webp 800w,/danaderp/assets/img/blog_astxplainer/asc_performance-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/danaderp/assets/img/blog_astxplainer/asc_performance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 5. `ASCeval` Performance grouped by specific LLMs and `ASCeval` density by Model Type. </div> <p>Fig.~5 indicates that models trained on natural language have more median variability than models fine-tuned on code datasets. For instance, <em>NL GPT-3</em> and <em>NL Codegen</em> report values in a range from $0.2$ to $0.9$. Conversely, fine-tuned models with code such as <em>Mono-Language-Type</em> has a lower variability than <em>NL GPT-3</em> and <em>NL Codegen</em> categories. For example, <em>mono-lang [2B]</em> has a global avg. median <code class="language-plaintext highlighter-rouge">ASCeval</code> of $0.84$ and a variability range between $0.7$ and $1.0$, outperforming the $0.6$ threshold. Furthermore, <em>mono-lang [2B]</em> is our best model with an average global <code class="language-plaintext highlighter-rouge">ASCeval</code> of $0.84$. On one hand, this suggests that fine-tuned models on code are predicting $ASC$ with higher confidence than natural language-only models. On the other hand, although <em>Multi-Language-Type</em> models exhibit high variability (from $0.5$ to $0.9$), their average median <code class="language-plaintext highlighter-rouge">ASCeval</code> (i.e., $0.68$ for <em>multi-lang [110M]</em>) is even better than natural language models (i.e., $0.48$ with variability from $0.2$ to $0.8$ for <em>gpt-3 [125M]</em>).</p> <h2 id="citation">Citation</h2> <div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc<span class="p">{</span>palacio<span class="p">_</span>evaluating<span class="p">_</span>2023,
	title = <span class="p">{</span>Evaluating and Explaining Large Language Models for Code Using Syntactic Structures<span class="p">}</span>,
	url = <span class="p">{</span>http://arxiv.org/abs/2308.03873<span class="p">}</span>,
	publisher = ,
	author = <span class="p">{</span>Palacio, David N. and Velasco, Alejandro and Rodriguez-Cardenas, Daniel and Moran, Kevin and Poshyvanyk, Denys<span class="p">}</span>,
	urldate = <span class="p">{</span>2023-08-22<span class="p">}</span>,
	date = <span class="p">{</span>2023-08-07<span class="p">}</span>,
	langid = <span class="p">{</span>english<span class="p">}</span>,
	eprinttype = <span class="p">{</span>arxiv<span class="p">}</span>,
	eprint = <span class="p">{</span>2308.03873 [cs]<span class="p">}</span>,
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>David A. Nader</name></author><summary type="html"><![CDATA[Explaining Large Language Models for Code Using Syntax Structures]]></summary></entry><entry><title type="html">doCode</title><link href="https://danaderp.github.io/danaderp/blog/2022/docode/" rel="alternate" type="text/html" title="doCode"/><published>2022-12-27T00:00:00+00:00</published><updated>2022-12-27T00:00:00+00:00</updated><id>https://danaderp.github.io/danaderp/blog/2022/docode</id><content type="html" xml:base="https://danaderp.github.io/danaderp/blog/2022/docode/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The combination of large amounts of freely available code-related data, which can be mined from open source repositories, and ever-more sophisticated \textit{Neural Language Model} (NLM) architectures have fueled the development of Software Engineering (SE) tools with increasing effectiveness. NLMs for code have (seemingly) illustrated promising performance across a range of different SE tasks (Watson et al., 2020; White et al., 2020; Ciniselli et al., 2021; Mastropaolo et al., 2021). In particular, \textit{code generation} has been an important area of SE research for decades, enabling tools for downstream tasks such as code completion~\cite{MSR-Completion}, program repair (Chen et al., 2019), and test case generation (Watson et al., 2020). In addition, industry interest in leveraging NLMs for code generation has also grown as evidenced by tools just as Microsoft’s IntelliCode \cite{intellicode}, Tabnine \cite{tabnine}, OpenAI’s Codex \cite{openai_codex}, and GitHub’s Copilot \cite{github_copilot}. Given the prior popularity of code completion engines within IDEs~\cite{murphy2006ide}, and the pending introduction of, and investment in commercial tools, NLMs for code generation will almost certainly be used to help build production software systems in the near future, if they are not already being used.</p>]]></content><author><name>David N. Palacio</name></author><summary type="html"><![CDATA[Toward a Causal Understanding of Neural Language Models for Code Generation.]]></summary></entry></feed>