---
---

@article{tse_10477672_2024,
  author={Palacio, David N. and Velasco, Alejandro and Cooper, Nathan and Rodriguez, Alvaro and Moran, Kevin and Poshyvanyk, Denys},
  journal={IEEE Transactions on Software Engineering}, 
  title={Toward a Theory of Causation for Interpreting Neural Code Models}, 
  year={2024},
  pages={1-28},
  keywords={Codes;Predictive models;Correlation;Adaptation models;Measurement;Task analysis;Software engineering;Causality;Interpretability;Neural Code Models},
  doi={10.1109/TSE.2024.3379943},
  selected={true},
  abbr={TSE},
  html={https://ieeexplore.ieee.org/document/10477672},
  abstract={Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces docode, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. docode is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of docode are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact of spurious correlations by grounding explanations of model behavior in properties of programming languages. To demonstrate the practical benefit of docode, we illustrate the insights that our framework can provide by performing a case study on two popular deep learning architectures and ten NCMs. The results of this case study illustrate that our studied NCMs are sensitive to changes in code syntax. All our NCMs, except for the BERT-like model, statistically learn to predict tokens related to blocks of code (\eg brackets, parenthesis, semicolon) with less confounding bias as compared to other programming language constructs. These insights demonstrate the potential of docode as a useful method to detect and facilitate the elimination of confounding bias in NCMs.},
  code={https://github.com/WM-SEMERU/CausalSE}
}

@article{Velasco2024WhichSC,
  title={Which Syntactic Capabilities Are Statistically Learned by Masked Language Models for Code?},
  author={Alejandro Velasco and David N. Palacio and Daniel Rodr{\'i}guez-C{\'a}rdenas and Denys Poshyvanyk},
  journal={ICSE},
  year={2024},
  volume={abs/2401.01512},
  url={https://api.semanticscholar.org/CorpusID:266741916},
  abbr={ICSE},
  arxiv={https://arxiv.org/abs/2401.01512},
  html={https://api.semanticscholar.org/CorpusID:266741916},
  code={https://github.com/WM-SEMERU/SyntaxEval}
}

@article{rodriguezcardenas2023benchmarking,
	author={Rodriguez-Cardenas, Daniel and Palacio, David N. and Khati, Dipin and Burke, Henry and Poshyvanyk, Denys},
	booktitle={2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
	title={Benchmarking Causal Study to Interpret Large Language Models for Source Code}, 
	year={2023},
	pages={329-334},
	keywords={Measurement;Training;Software maintenance;Codes;Correlation;Source coding;Semantics;Software Engineering;Testbeds;Large Language Models;dl4se;Interpretability},
	doi={10.1109/ICSME58846.2023.00040},
	abbr={ICSME},
	arxiv={https://arxiv.org/abs/2308.12415},
	html={https://ieeexplore.ieee.org/document/10336302},
	code={https://github.com/WM-SEMERU/galeras-benchmark}
  }

@article{patent:20240104001,
	title     = {Patent: Debugging Tool for Code Generation Neural Language Models},
	author    = {Clement, Colin Bruce (SEATTLE, WA, US), Nader Palacio, David Alberto (WILLIAMSBURG, VA, US), Sundaresan, Neelakantan (BELLEVUE, WA, US), Svyatkovskiy, Alexey (BELLEVUE, WA, US), Tufano, Michele (BELLEVUE, WA, US)},
	year      = {2024},
	url       = {https://www.freepatentsonline.com/y2024/0104001.html},
	html      = {https://www.freepatentsonline.com/y2024/0104001.html},
	website = {https://www.freepatentsonline.com/20240104001.pdf},
	abbr = {PATENT}
}

@article{mastropaoloUsing2023,
	title = {Using Transfer Learning for Code Related Tasks},
	html = {https://ieeexplore.ieee.org/document/9797060/},
	author = {Mastropaolo, Antonio and Cooper, Nathan and Palacio, David Nader and Scalabrino, Simone and Poshyvanyk, Denys and Oliveto, Rocco and Bavota, Gabriele},
	abbr = {TSE}
}


