---
---

@article{tse_10477672_2024,
  author={Palacio, David N. and Velasco, Alejandro and Cooper, Nathan and Rodriguez, Alvaro and Moran, Kevin and Poshyvanyk, Denys},
  journal={IEEE Transactions on Software Engineering}, 
  title={Toward a Theory of Causation for Interpreting Neural Code Models}, 
  year={2024},
  pages={1-28},
  keywords={Codes;Predictive models;Correlation;Adaptation models;Measurement;Task analysis;Software engineering;Causality;Interpretability;Neural Code Models},
  doi={10.1109/TSE.2024.3379943},
  selected={true},
  abbr={TSE},
  html={https://ieeexplore.ieee.org/document/10477672},
  abstract={Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces docode, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. docode is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of docode are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact of spurious correlations by grounding explanations of model behavior in properties of programming languages. To demonstrate the practical benefit of docode, we illustrate the insights that our framework can provide by performing a case study on two popular deep learning architectures and ten NCMs. The results of this case study illustrate that our studied NCMs are sensitive to changes in code syntax. All our NCMs, except for the BERT-like model, statistically learn to predict tokens related to blocks of code (\eg brackets, parenthesis, semicolon) with less confounding bias as compared to other programming language constructs. These insights demonstrate the potential of docode as a useful method to detect and facilitate the elimination of confounding bias in NCMs.},
  code={https://github.com/WM-SEMERU/CausalSE}
}

@article{Velasco2024WhichSC,
  title={Which Syntactic Capabilities Are Statistically Learned by Masked Language Models for Code?},
  author={Alejandro Velasco and David N. Palacio and Daniel Rodr{\'i}guez-C{\'a}rdenas and Denys Poshyvanyk},
  journal={ICSE},
  year={2024},
  volume={abs/2401.01512},
  url={https://api.semanticscholar.org/CorpusID:266741916},
  abbr={ICSE},
  arxiv={2401.01512},
  html={https://api.semanticscholar.org/CorpusID:266741916},
  code={https://github.com/WM-SEMERU/SyntaxEval}
}

@article{rodriguezcardenas2023benchmarking,
	author={Rodriguez-Cardenas, Daniel and Palacio, David N. and Khati, Dipin and Burke, Henry and Poshyvanyk, Denys},
	booktitle={2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
	title={Benchmarking Causal Study to Interpret Large Language Models for Source Code}, 
	year={2023},
	pages={329-334},
	keywords={Measurement;Training;Software maintenance;Codes;Correlation;Source coding;Semantics;Software Engineering;Testbeds;Large Language Models;dl4se;Interpretability},
	doi={10.1109/ICSME58846.2023.00040},
	abbr={ICSME},
	arxiv={2308.12415},
	html={https://ieeexplore.ieee.org/document/10336302},
	code={https://github.com/WM-SEMERU/galeras-benchmark}
  }

@article{patent:20240104001,
	title     = {Patent: Debugging Tool for Code Generation Neural Language Models},
	author    = {Clement, Colin Bruce (SEATTLE, WA, US), Nader Palacio, David Alberto (WILLIAMSBURG, VA, US), Sundaresan, Neelakantan (BELLEVUE, WA, US), Svyatkovskiy, Alexey (BELLEVUE, WA, US), Tufano, Michele (BELLEVUE, WA, US)},
	year      = {2024},
	url       = {https://www.freepatentsonline.com/y2024/0104001.html},
	html      = {https://www.freepatentsonline.com/y2024/0104001.html},
	website = {https://www.freepatentsonline.com/20240104001.pdf},
	abbr = {PATENT}
}

@article{mastropaolo_using_2023,
	title = {Using Transfer Learning for Code-Related Tasks},
  year = {2023},
	volume = {49},
	issn = {0098-5589, 1939-3520, 2326-3881},
	doi = {10.1109/TSE.2022.3183297},
	abstract = {Deep learning ({DL}) techniques have been used to support several code-related tasks such as code summarization and bug-ﬁxing. In particular, pre-trained transformer models are on the rise, also thanks to the excellent results they achieved in Natural Language Processing ({NLP}) tasks. The basic idea behind these models is to ﬁrst pre-train them on a generic dataset using a self-supervised task (e.g., ﬁlling masked words in sentences). Then, these models are ﬁne-tuned to support speciﬁc tasks of interest (e.g., language translation). A single model can be ﬁne-tuned to support multiple tasks, possibly exploiting the beneﬁts of transfer learning. This means that knowledge acquired to solve a speciﬁc task (e.g., language translation) can be useful to boost performance on another task (e.g., sentiment classiﬁcation). While the beneﬁts of transfer learning have been widely studied in {NLP}, limited empirical evidence is available when it comes to code-related tasks. In this paper, we assess the performance of the Text-To-Text Transfer Transformer (T5) model in supporting four different code-related tasks: (i) automatic bug-ﬁxing, (ii) injection of code mutants, (iii) generation of assert statements, and (iv) code summarization. We pay particular attention in studying the role played by pre-training and multi-task ﬁne-tuning on the model’s performance. We show that (i) the T5 can achieve better performance as compared to state-of-the-art baselines; and (ii) while pre-training helps the model, not all tasks beneﬁt from a multi-task ﬁne-tuning.},
	pages = {1580--1598},
	number = {4},
	journaltitle = {{IIEEE} Trans. Software Eng.},
	author = {Mastropaolo, Antonio and Cooper, Nathan and Palacio, David Nader and Scalabrino, Simone and Poshyvanyk, Denys and Oliveto, Rocco and Bavota, Gabriele},
	urldate = {2023-07-19},
	date = {2023-04-01},
	langid = {english},
	file = {Mastropaolo et al. - 2023 - Using Transfer Learning for Code-Related Tasks.pdf:C\:\\Users\\David\\Zotero\\storage\\VAYM2QF4\\Mastropaolo et al. - 2023 - Using Transfer Learning for Code-Related Tasks.pdf:application/pdf},
	abbr = {TSE},
  html = {https://ieeexplore.ieee.org/document/9797060/}
}

@article{palacio_evaluating_2023,
	title = {Evaluating and Explaining Large Language Models for Code Using Syntactic Structures},
	year = {2023},
	html = {http://arxiv.org/abs/2308.03873},
	abstract = {Large Language Models ({LLMs}) for code are a family of high-parameter, transformer-based neural networks pre-trained on massive datasets of both natural and programming languages. These models are rapidly being employed in commercial {AI}-based developer tools, such as {GitHub} {CoPilot}. However, measuring and explaining their effectiveness on programming tasks is a challenging proposition, given their size and complexity. The methods for evaluating and explaining {LLMs} for code are inextricably linked. That is, in order to explain a model’s predictions, they must be reliably mapped to fine-grained, understandable concepts. Once this mapping is achieved, new methods for detailed model evaluations are possible. However, most current explainability techniques and evaluation benchmarks focus on model robustness or individual task performance, as opposed to interpreting model predictions. To this end, this paper introduces {ASTxplainer}, an explainability method specific to {LLMs} for code that enables both new methods for {LLM} evaluation and visualizations of {LLM} predictions that aid end-users in understanding model predictions. At its core, {ASTxplainer} provides an automated method for aligning token predictions with {AST} nodes, by extracting and aggregating normalized model logits within {AST} structures. To demonstrate the practical benefit of {ASTxplainer}, we illustrate the insights that our framework can provide by performing an empirical evaluation on 12 popular {LLMs} for code using a curated dataset of the most popular {GitHub} projects. Additionally, we perform a user study examining the usefulness of an {ASTxplainer}-derived visualization of model predictions aimed at enabling model users to explain predictions. The results of these studies illustrate the potential for {ASTxplainer} to provide insights into {LLM} effectiveness, and aid end-users in understanding predictions.},
	publisher = {{arXiv}},
	author = {Palacio, David N. and Velasco, Alejandro and Rodriguez-Cardenas, Daniel and Moran, Kevin and Poshyvanyk, Denys},
	urldate = {2023-08-22},
	date = {2023-08-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2308.03873 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	arxiv = {2308.03873}
}

@article{watson_systematic_2022,
	author = {Watson, Cody and Cooper, Nathan and Palacio, David Nader and Moran, Kevin and Poshyvanyk, Denys},
	title = {A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research},
	year = {2022},
	issue_date = {April 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {31},
	number = {2},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3485275},
	doi = {10.1145/3485275},
	abstract = {An increasingly popular set of techniques adopted by software engineering (SE) researchers to automate development tasks are those rooted in the concept of Deep Learning (DL). The popularity of such techniques largely stems from their automated feature engineering capabilities, which aid in modeling software artifacts. However, due to the rapid pace at which DL techniques have been adopted, it is difficult to distill the current successes, failures, and opportunities of the current research landscape. In an effort to bring clarity to this cross-cutting area of work, from its modern inception to the present, this article presents a systematic literature review of research at the intersection of SE \& DL. The review canvasses work appearing in the most prominent SE and DL conferences and journals and spans 128 papers across 23&nbsp;unique SE tasks. We center our analysis around the components of learning, a set of principles that governs the application of machine learning techniques (ML) to a given problem domain, discussing several aspects of the surveyed work at a granular level. The end result of our analysis is a research roadmap that both delineates the foundations of DL techniques applied to SE research and highlights likely areas of fertile exploration for the future.},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	month = {mar},
	articleno = {32},
	numpages = {58},
	keywords = {Deep learning, neural networks, literature review, software engineering, machine learning},
	abbr = {TOSEM},
	html = {https://dl.acm.org/doi/10.1145/3485275},
  arxiv = {2009.06520},
  selected = {true}
}

@inproceedings{moran_improving_2020,
	title = {Improving the effectiveness of traceability link recovery using hierarchical bayesian networks},
	doi = {10.1145/3377811.3380418},
  isbn = {978-1-4503-7121-6},
	pages = {873-885},
  year = {2020},
  urldate = {2023-07-19},
	date = {2020-06-27},
	booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Moran, Kevin and Palacio, David N. and Bernal-Cárdenas, Carlos and McCrystal, Daniel and Poshyvanyk, Denys and Shenefiel, Chris and Johnson, Jeff},
  html = {https://dl.acm.org/doi/10.1145/3377811.3380418},
  abbr = {ICSE},
  abstract = {Traceability is a fundamental component of the modern software development process that helps to ensure properly functioning, secure programs. Due to the high cost of manually establishing trace links, researchers have developed automated approaches that draw relationships between pairs of textual software artifacts using similarity measures. However, the effectiveness of such techniques are often limited as they only utilize a single measure of artifact similarity and cannot simultaneously model (implicit and explicit) relationships across groups of diverse development artifacts.}
}

@inproceedings{10.1145/3205651.3208294,
	author = {Nader-Palacio, David and Rodr\'{\i}guez-C\'{a}rdenas, Daniel and Gomez, Jonatan},
	title = {Assessing Single-Objective Performance Convergence and Time Complexity for Refactoring Detection},
	year = {2018},
	isbn = {9781450357647},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3205651.3208294},
	doi = {10.1145/3205651.3208294},
	abstract = {The automatic detection of refactoring recommendations has been tackled in prior optimization
	studies involving bad code smells, semantic coherence and importance of classes; however,
	such studies informally addressed formalisms to standardize and replicate refactoring
	models. We propose to assess the refactoring detection by means of performance convergence
	and time complexity. Since the reported approaches are difficult to reproduce, we
	employ an Artificial Refactoring Generation (ARGen) as a formal and naive computational
	solution for the Refactoring Detection Problem. ARGen is able to detect massive refactoring's
	sets in feasible areas of the search space. We used a refactoring formalization to
	adapt search techniques (Hill Climbing, Simulated Annealing and Hybrid Adaptive Evolutionary
	Algorithm) that assess the performance and complexity on three open software systems.
	Combinatorial techniques are limited in solving the Refactoring Detection Problem
	due to the relevance of developers' criteria (human factor) when designing reconstructions.
	Without performance convergence and time complexity analysis, a software empirical
	analysis that utilizes search techniques is incomplete.},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
	pages = {1606–1613},
	numpages = {8},
	keywords = {combinatorial optimization, mathematical software performance, refactoring, software maintenance},
	location = {Kyoto, Japan},
	series = {GECCO '18},
	selected={true},
	html = {https://doi.org/10.1145/3205651.3208294},
  abbr = {GECCO}
}

@book{palacio_2017_a,
  author = {Palacio, Nader and Alberto, David},
  month = {12},
  title = {A computational solution for the software refactoring problem: from a formalism toward an optimization approach},
  html = {https://repositorio.unal.edu.co/handle/unal/62057},
  urldate = {2021-09-23},
  year = {2017},
  journal = {repositorio.unal.edu.co}
}


@INPROCEEDINGS{10187603,
  author={},
  booktitle={2020 IEEE/ACM 7th International Conference on Mobile Software Engineering and Systems (MOBILESoft)}, 
  title={Subreviewers}, 
  year={2020},
  volume={},
  number={},
  pages={17-17},
  keywords={},
  doi={}}

@INPROCEEDINGS{8816769,
  author={},
  booktitle={2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)}, 
  title={Additional Reviewers}, 
  year={2019},
  volume={},
  number={},
  pages={33-33},
  keywords={},
  doi={10.1109/MSR.2019.00010}
}


