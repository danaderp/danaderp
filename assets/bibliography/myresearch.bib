
@article{mastropaolo_using_2023,
	title = {Using Transfer Learning for Code-Related Tasks},
	volume = {49},
	issn = {0098-5589, 1939-3520, 2326-3881},
	url = {https://ieeexplore.ieee.org/document/9797060/},
	doi = {10.1109/TSE.2022.3183297},
	abstract = {Deep learning ({DL}) techniques have been used to support several code-related tasks such as code summarization and bug-ﬁxing. In particular, pre-trained transformer models are on the rise, also thanks to the excellent results they achieved in Natural Language Processing ({NLP}) tasks. The basic idea behind these models is to ﬁrst pre-train them on a generic dataset using a self-supervised task (e.g., ﬁlling masked words in sentences). Then, these models are ﬁne-tuned to support speciﬁc tasks of interest (e.g., language translation). A single model can be ﬁne-tuned to support multiple tasks, possibly exploiting the beneﬁts of transfer learning. This means that knowledge acquired to solve a speciﬁc task (e.g., language translation) can be useful to boost performance on another task (e.g., sentiment classiﬁcation). While the beneﬁts of transfer learning have been widely studied in {NLP}, limited empirical evidence is available when it comes to code-related tasks. In this paper, we assess the performance of the Text-To-Text Transfer Transformer (T5) model in supporting four different code-related tasks: (i) automatic bug-ﬁxing, (ii) injection of code mutants, (iii) generation of assert statements, and (iv) code summarization. We pay particular attention in studying the role played by pre-training and multi-task ﬁne-tuning on the model’s performance. We show that (i) the T5 can achieve better performance as compared to state-of-the-art baselines; and (ii) while pre-training helps the model, not all tasks beneﬁt from a multi-task ﬁne-tuning.},
	pages = {1580--1598},
	number = {4},
	journaltitle = {{IIEEE} Trans. Software Eng.},
	author = {Mastropaolo, Antonio and Cooper, Nathan and Palacio, David Nader and Scalabrino, Simone and Poshyvanyk, Denys and Oliveto, Rocco and Bavota, Gabriele},
	urldate = {2023-07-19},
	date = {2023-04-01},
	langid = {english},
	file = {Mastropaolo et al. - 2023 - Using Transfer Learning for Code-Related Tasks.pdf:C\:\\Users\\David\\Zotero\\storage\\VAYM2QF4\\Mastropaolo et al. - 2023 - Using Transfer Learning for Code-Related Tasks.pdf:application/pdf},
	abbr={TSE},
}

@article{mastropaoloUsing2023,
	title = {Using Transfer Learning for Code Related Tasks},
	html = {https://ieeexplore.ieee.org/document/9797060/},
	author = {Mastropaolo, Antonio and Cooper, Nathan and Palacio, David Nader and Scalabrino, Simone and Poshyvanyk, Denys and Oliveto, Rocco and Bavota, Gabriele},
	abbr = {TSE}
}


@article{palacio_toward_2023,
	title = {Toward a Theory of Causation for Interpreting Neural Code Models},
	url = {http://arxiv.org/abs/2302.03788},
	abstract = {Neural Language Models of Code, or Neural Code Models ({NCMs}), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of {NCMs} appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces docode, a post-hoc interpretability methodology speciﬁc to {NCMs} that is capable of explaining model predictions. docode is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of docode are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact of spurious correlations by grounding explanations of model behavior in properties of programming languages. To demonstrate the practical beneﬁt of docode, we illustrate the insights that our framework can provide by performing a case study on two popular deep learning architectures and nine {NCMs}. The results of this case study illustrate that our studied {NCMs} are sensitive to changes in code syntax and statistically learn to predict tokens related to blocks of code (e.g., brackets, parenthesis, semicolon) with less confounding bias as compared to other programming language constructs. These insights demonstrate the potential of docode as a useful model debugging mechanism that may aid in discovering biases and limitations in {NCMs}.},
	number = {{arXiv}:2302.03788},
	publisher = {{arXiv}},
	author = {Palacio, David N. and Cooper, Nathan and Rodriguez, Alvaro and Moran, Kevin and Poshyvanyk, Denys},
	urldate = {2023-08-22},
	date = {2023-02-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2302.03788 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering, Statistics - Methodology},
	file = {Palacio et al. - 2023 - Toward a Theory of Causation for Interpreting Neur.pdf:C\:\\Users\\David\\Zotero\\storage\\LDMTWLX7\\Palacio et al. - 2023 - Toward a Theory of Causation for Interpreting Neur.pdf:application/pdf},
}

@article{palacio_evaluating_2023,
	title = {Evaluating and Explaining Large Language Models for Code Using Syntactic Structures},
	url = {http://arxiv.org/abs/2308.03873},
	abstract = {Large Language Models ({LLMs}) for code are a family of high-parameter, transformer-based neural networks pre-trained on massive datasets of both natural and programming languages. These models are rapidly being employed in commercial {AI}-based developer tools, such as {GitHub} {CoPilot}. However, measuring and explaining their effectiveness on programming tasks is a challenging proposition, given their size and complexity. The methods for evaluating and explaining {LLMs} for code are inextricably linked. That is, in order to explain a model’s predictions, they must be reliably mapped to fine-grained, understandable concepts. Once this mapping is achieved, new methods for detailed model evaluations are possible. However, most current explainability techniques and evaluation benchmarks focus on model robustness or individual task performance, as opposed to interpreting model predictions. To this end, this paper introduces {ASTxplainer}, an explainability method specific to {LLMs} for code that enables both new methods for {LLM} evaluation and visualizations of {LLM} predictions that aid end-users in understanding model predictions. At its core, {ASTxplainer} provides an automated method for aligning token predictions with {AST} nodes, by extracting and aggregating normalized model logits within {AST} structures. To demonstrate the practical benefit of {ASTxplainer}, we illustrate the insights that our framework can provide by performing an empirical evaluation on 12 popular {LLMs} for code using a curated dataset of the most popular {GitHub} projects. Additionally, we perform a user study examining the usefulness of an {ASTxplainer}-derived visualization of model predictions aimed at enabling model users to explain predictions. The results of these studies illustrate the potential for {ASTxplainer} to provide insights into {LLM} effectiveness, and aid end-users in understanding predictions.},
	number = {{arXiv}:2308.03873},
	publisher = {{arXiv}},
	author = {Palacio, David N. and Velasco, Alejandro and Rodriguez-Cardenas, Daniel and Moran, Kevin and Poshyvanyk, Denys},
	urldate = {2023-08-22},
	date = {2023-08-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2308.03873 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {Palacio et al. - 2023 - Evaluating and Explaining Large Language Models fo.pdf:C\:\\Users\\David\\Zotero\\storage\\UDFCA9EE\\Palacio et al. - 2023 - Evaluating and Explaining Large Language Models fo.pdf:application/pdf},
}

@article{palacio_toward_2023,
	title = {Toward a Theory of Causation for Interpreting Neural Code Models},
	url = {http://arxiv.org/abs/2302.03788},
	abstract = {Neural Language Models of Code, or Neural Code Models ({NCMs}), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of {NCMs} appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces docode, a post-hoc interpretability methodology speciﬁc to {NCMs} that is capable of explaining model predictions. docode is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of docode are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact of spurious correlations by grounding explanations of model behavior in properties of programming languages. To demonstrate the practical beneﬁt of docode, we illustrate the insights that our framework can provide by performing a case study on two popular deep learning architectures and nine {NCMs}. The results of this case study illustrate that our studied {NCMs} are sensitive to changes in code syntax and statistically learn to predict tokens related to blocks of code (e.g., brackets, parenthesis, semicolon) with less confounding bias as compared to other programming language constructs. These insights demonstrate the potential of docode as a useful model debugging mechanism that may aid in discovering biases and limitations in {NCMs}.},
	publisher = {{arXiv}},
	author = {Palacio, David N. and Cooper, Nathan and Rodriguez, Alvaro and Moran, Kevin and Poshyvanyk, Denys},
	urldate = {2023-08-22},
	date = {2023-02-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2302.03788 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering, Statistics - Methodology},
	arxiv={http://arxiv.org/abs/2302.03788}
}

@article{palacio_evaluating_2023,
	title = {Evaluating and Explaining Large Language Models for Code Using Syntactic Structures},
	url = {http://arxiv.org/abs/2308.03873},
	abstract = {Large Language Models ({LLMs}) for code are a family of high-parameter, transformer-based neural networks pre-trained on massive datasets of both natural and programming languages. These models are rapidly being employed in commercial {AI}-based developer tools, such as {GitHub} {CoPilot}. However, measuring and explaining their effectiveness on programming tasks is a challenging proposition, given their size and complexity. The methods for evaluating and explaining {LLMs} for code are inextricably linked. That is, in order to explain a model’s predictions, they must be reliably mapped to fine-grained, understandable concepts. Once this mapping is achieved, new methods for detailed model evaluations are possible. However, most current explainability techniques and evaluation benchmarks focus on model robustness or individual task performance, as opposed to interpreting model predictions. To this end, this paper introduces {ASTxplainer}, an explainability method specific to {LLMs} for code that enables both new methods for {LLM} evaluation and visualizations of {LLM} predictions that aid end-users in understanding model predictions. At its core, {ASTxplainer} provides an automated method for aligning token predictions with {AST} nodes, by extracting and aggregating normalized model logits within {AST} structures. To demonstrate the practical benefit of {ASTxplainer}, we illustrate the insights that our framework can provide by performing an empirical evaluation on 12 popular {LLMs} for code using a curated dataset of the most popular {GitHub} projects. Additionally, we perform a user study examining the usefulness of an {ASTxplainer}-derived visualization of model predictions aimed at enabling model users to explain predictions. The results of these studies illustrate the potential for {ASTxplainer} to provide insights into {LLM} effectiveness, and aid end-users in understanding predictions.},
	publisher = {{arXiv}},
	author = {Palacio, David N. and Velasco, Alejandro and Rodriguez-Cardenas, Daniel and Moran, Kevin and Poshyvanyk, Denys},
	urldate = {2023-08-22},
	date = {2023-08-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2308.03873 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	arxiv = {http://arxiv.org/abs/2308.03873}
}

@inproceedings{moran_improving_2020,
	location = {Seoul South Korea},
	title = {Improving the effectiveness of traceability link recovery using hierarchical bayesian networks},
	isbn = {978-1-4503-7121-6},
	url = {https://dl.acm.org/doi/10.1145/3377811.3380418},
	doi = {10.1145/3377811.3380418},
	abstract = {Traceability is a fundamental component of the modern software development process that helps to ensure properly functioning, secure programs. Due to the high cost of manually establishing trace links, researchers have developed automated approaches that draw relationships between pairs of textual software artifacts using similarity measures. However, the effectiveness of such techniques are often limited as they only utilize a single measure of artifact similarity and cannot simultaneously model (implicit and explicit) relationships across groups of diverse development artifacts.},
	eventtitle = {{ICSE} '20: 42nd International Conference on Software Engineering},
	pages = {873--885},
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Moran, Kevin and Palacio, David N. and Bernal-Cárdenas, Carlos and {McCrystal}, Daniel and Poshyvanyk, Denys and Shenefiel, Chris and Johnson, Jeff},
	urldate = {2023-07-19},
	date = {2020-06-27},
	langid = {english},
	file = {Moran et al. - 2020 - Improving the effectiveness of traceability link r.pdf:C\:\\Users\\David\\Zotero\\storage\\2HVCLTVN\\Moran et al. - 2020 - Improving the effectiveness of traceability link r.pdf:application/pdf},
}

@article{palacio_learning_2019,
	title = {Learning to Identify Security-Related Issues Using Convolutional Neural Networks},
	url = {http://arxiv.org/abs/1908.00614},
	abstract = {Software security is becoming a high priority for both large companies and start-ups alike due to the increasing potential for harm that vulnerabilities and breaches carry with them. However, attaining robust security assurance while delivering features requires a precarious balancing act in the context of agile development practices. One path forward to help aid development teams in securing their software products is through the design and development of security-focused automation. Ergo, we present a novel approach, called {SecureReqNet}, for automatically identifying whether issues in software issue tracking systems describe security-related content. Our approach consists of a two-phase neural net architecture that operates purely on the natural language descriptions of issues. The ﬁrst phase of our approach learns high dimensional word embeddings from hundreds of thousands of vulnerability descriptions listed in the {CVE} database and issue descriptions extracted from open source projects. The second phase then utilizes the semantic ontology represented by these embeddings to train a convolutional neural network capable of predicting whether a given issue is securityrelated. We evaluated {SecureReqNet} by applying it to identify security-related issues from a dataset of thousands of issues mined from popular projects on {GitLab} and {GitHub}. In addition, we also applied our approach to identify security-related requirements from a commercial software project developed by a major telecommunication company. Our preliminary results are encouraging, with {SecureReqNet} achieving an accuracy of 96\% on open source issues and 71.6\% on industrial requirements.},
	number = {{arXiv}:1908.00614},
	publisher = {{arXiv}},
	author = {Palacio, David N. and {McCrystal}, Daniel and Moran, Kevin and Bernal-Cárdenas, Carlos and Poshyvanyk, Denys and Shenefiel, Chris},
	urldate = {2023-07-19},
	date = {2019-08-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1908.00614 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {Palacio et al. - 2019 - Learning to Identify Security-Related Issues Using.pdf:C\:\\Users\\David\\Zotero\\storage\\IMR9JKVY\\Palacio et al. - 2019 - Learning to Identify Security-Related Issues Using.pdf:application/pdf},
}

@article{palacio_learning_2019,
	title = {Learning to Identify Security-Related Issues Using Convolutional Neural Networks},
	html = {http://arxiv.org/abs/1908.00614},
	abstract = {Software security is becoming a high priority for both large companies and start-ups alike due to the increasing potential for harm that vulnerabilities and breaches carry with them. However, attaining robust security assurance while delivering features requires a precarious balancing act in the context of agile development practices. One path forward to help aid development teams in securing their software products is through the design and development of security-focused automation. Ergo, we present a novel approach, called {SecureReqNet}, for automatically identifying whether issues in software issue tracking systems describe security-related content. Our approach consists of a two-phase neural net architecture that operates purely on the natural language descriptions of issues. The ﬁrst phase of our approach learns high dimensional word embeddings from hundreds of thousands of vulnerability descriptions listed in the {CVE} database and issue descriptions extracted from open source projects. The second phase then utilizes the semantic ontology represented by these embeddings to train a convolutional neural network capable of predicting whether a given issue is securityrelated. We evaluated {SecureReqNet} by applying it to identify security-related issues from a dataset of thousands of issues mined from popular projects on {GitLab} and {GitHub}. In addition, we also applied our approach to identify security-related requirements from a commercial software project developed by a major telecommunication company. Our preliminary results are encouraging, with {SecureReqNet} achieving an accuracy of 96\% on open source issues and 71.6\% on industrial requirements.},
	publisher = {{arXiv}},
	author = {Palacio, David N. and {McCrystal}, Daniel and Moran, Kevin and Bernal-Cárdenas, Carlos and Poshyvanyk, Denys and Shenefiel, Chris},
	urldate = {2023-07-19},
	date = {2019-08-05},
	eprint = {1908.00614 [cs]},
	keywords = {Computer Science - Software Engineering},
  	abbr = {ICSME},
  	code = {https://github.com/WM-SEMERU/SecureReqNet}
}