@article{gregor2015draw,
  title={DRAW: A recurrent neural network for image generation},
  author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  journal={arXiv preprint, arXiv:1502.04623},
  year={2015},
  url={https://arxiv.org/pdf/1502.04623.pdf}
}

@misc{liu2023code,
      title={Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation}, 
      author={Jiawei Liu and Chunqiu Steven Xia and Yuyao Wang and Lingming Zhang},
      year={2023},
      eprint={2305.01210},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@article{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1508.07909},
  year={2015}
}

@article{MSR-Completion,
  author    = {Matteo Ciniselli and
               Nathan Cooper and
               Luca Pascarella and
               Denys Poshyvanyk and
               Massimiliano Di Penta and
               Gabriele Bavota},
  title     = {An Empirical Study on the Usage of {BERT} Models for Code Completion},
  journal   = {CoRR},
  volume    = {abs/2103.07115},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.07115},
  eprinttype = {arXiv},
  eprint    = {2103.07115},
  timestamp = {Tue, 23 Mar 2021 16:29:47 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-07115.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{pillutla2021mauve,
  title={Mauve: Measuring the gap between neural text and human text using divergence frontiers},
  author={Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{zhang2021dive,
    title={Dive into Deep Learning},
    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
    journal={arXiv preprint arXiv:2106.11342},
    year={2021}
}

@article{Hendrycks2021apps,
  author    = {Dan Hendrycks and
               Steven Basart and
               Saurav Kadavath and
               Mantas Mazeika and
               Akul Arora and
               Ethan Guo and
               Collin Burns and
               Samir Puranik and
               Horace He and
               Dawn Song and
               Jacob Steinhardt},
  title     = {Measuring Coding Challenge Competence With {APPS}},
  journal   = {CoRR},
  volume    = {abs/2105.09938},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.09938},
  archivePrefix = {arXiv},
  eprint    = {2105.09938},
  timestamp = {Mon, 31 May 2021 16:16:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-09938.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{icodegen,
  author = {Anonymous},
  title = {InterpretingCodeGeneration},
  year = {2022},
  publisher = {Anonymous 4 Open Science},
  journal = {Anonymous 4 Open Science},
  howpublished = {\url{https://anonymous.4open.science/r/InterpretingCodeGeneration-75E4/}}
}

@inproceedings{wolf2020transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}


@inproceedings{sennrich2016bpe,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

@article{Kingma2015AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6980}
}

@misc{github,
  author={github},
  title={GitHub},
  year={2020},
  url={https://github.com/},
}

@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@inproceedings{Cho2014GRU,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}

@article{Chung2014EmpiricalGRU,
  author    = {Junyoung Chung and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               KyungHyun Cho and
               Yoshua Bengio},
  title     = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
               Modeling},
  journal   = {CoRR},
  volume    = {abs/1412.3555},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.3555},
  archivePrefix = {arXiv},
  eprint    = {1412.3555},
  timestamp = {Mon, 13 Aug 2018 16:47:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChungGCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018}
}

@inproceedings{Nguyen:ICSE15, author = {Nguyen, Anh Tuan and Nguyen, Tien N.}, title = {Graph-Based Statistical Language Model for Code}, year = {2015}, isbn = {9781479919345}, publisher = {IEEE Press}, abstract = {n-gram statistical language model has been successfully applied to capture programming patterns to support code completion and suggestion. However, the approaches using n-gram face challenges in capturing the patterns at higher levels of abstraction due to the mismatch between the sequence nature in n-grams and the structure nature of syntax and semantics in source code. This paper presents GraLan, a graph-based statistical language model and its application in code suggestion. GraLan can learn from a source code corpus and compute the appearance probabilities of any graphs given the observed (sub)graphs. We use GraLan to develop an API suggestion engine and an AST-based language model, ASTLan. ASTLan supports the suggestion of the next valid syntactic template and the detection of common syntactic templates. Our empirical evaluation on a large corpus of open-source projects has shown that our engine is more accurate in API code suggestion than the state-of-the-art approaches, and in 75\% of the cases, it can correctly suggest the API with only five candidates. ASTLan also has high accuracy in suggesting the next syntactic template and is able to detect many useful and common syntactic templates.}, booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1}, pages = {858â868}, numpages = {11}, location = {Florence, Italy}, series = {ICSE '15} }

@inproceedings{Allamanis19,
  author={Miltiadis Allamanis},
  title={The adverse effects of code duplication in machine learning models of code},
  year={2019},
  cdate={1546300800000},
  pages={143-153},
  url={https://doi.org/10.1145/3359591.3359735},
  booktitle={Onward! OOPLSA 2019},
}

@MISC {emp-standards,
	title        = {ACM SIGSOFT Empirical SE Standards},
	howpublished = {\url{https://acmsigsoft.github.io/EmpiricalStandards/tools/}},
	year         = {2021},
}

@inproceedings{Baishakhi2016buggy,
    author = {Ray, Baishakhi and Hellendoorn, Vincent and Godhane, Saheel and Tu, Zhaopeng and Bacchelli, Alberto and Devanbu, Premkumar},
    title = {On the "Naturalness" of Buggy Code},
    year = {2016},
    isbn = {9781450339001},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2884781.2884848},
    doi = {10.1145/2884781.2884848},
    abstract = {Real software, the kind working programmers produce by the kLOC to solve real-world
    problems, tends to be "natural", like speech or natural language; it tends to be highly
    repetitive and predictable. Researchers have captured this naturalness of software
    through statistical models and used them to good effect in suggestion engines, porting
    tools, coding standards checkers, and idiom miners. This suggests that code that appears
    improbable, or surprising, to a good statistical language model is "unnatural" in
    some sense, and thus possibly suspicious. In this paper, we investigate this hypothesis.
    We consider a large corpus of bug fix commits (ca. 7,139), from 10 different Java
    projects, and focus on its language statistics, evaluating the naturalness of buggy
    code and the corresponding fixes. We find that code with bugs tends to be more entropic
    (i.e. unnatural), becoming less so as bugs are fixed. Ordering files for inspection
    by their average entropy yields cost-effectiveness scores comparable to popular defect
    prediction methods. At a finer granularity, focusing on highly entropic lines is similar
    in cost-effectiveness to some well-known static bug finders (PMD, FindBugs) and ordering
    warnings from these bug finders using an entropy measure improves the cost-effectiveness
    of inspecting code implicated in warnings. This suggests that entropy may be a valid,
    simple way to complement the effectiveness of PMD or FindBugs, and that search-based
    bug-fixing methods may benefit from using entropy both for fault-localization and
    searching for fixes.},
    booktitle = {Proceedings of the 38th International Conference on Software Engineering},
    pages = {428â439},
    numpages = {12},
    location = {Austin, Texas},
    series = {ICSE '16}
}

@inproceedings{
    allamanis2018learning,
    title={Learning to Represent Programs with Graphs},
    author={Miltiadis Allamanis and Marc Brockschmidt and Mahmoud Khademi},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=BJOFETxR-},
}

@article{RNNs,
    author = {Hochreiter, Sepp and Schmidhuber, Jurgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@inproceedings{Watson:ICSE20, author = {Watson, Cody and Tufano, Michele and Moran, Kevin and Bavota, Gabriele and Poshyvanyk, Denys}, title = {On Learning Meaningful Assert Statements for Unit Test Cases}, year = {2020}, isbn = {9781450371216}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3377811.3380429}, doi = {10.1145/3377811.3380429}, booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering}, pages = {1398â1409}, numpages = {12}, location = {Seoul, South Korea}, series = {ICSE '20} }

@ARTICLE{LeGous:2012,
  author={Le Goues, Claire and Nguyen, ThanhVu and Forrest, Stephanie and Weimer, Westley},
  journal={IEEE Transactions on Software Engineering}, 
  title={GenProg: A Generic Method for Automatic Software Repair}, 
  year={2012},
  volume={38},
  number={1},
  pages={54-72},
  doi={10.1109/TSE.2011.104}}


@article{Hussain2020DeepTL,
  title={Deep Transfer Learning for Source Code Modeling},
  author={Yasir Hussain and Zhiqiu Huang and Yu Zhou and Senzhang Wang},
  journal={Int. J. Softw. Eng. Knowl. Eng.},
  year={2020},
  volume={30},
  pages={649-668}
}

@inproceedings{mahmud2021cmterrs,
    title = "Code to Comment Translation: A Comparative Study on Model Effectiveness {\&} Errors",
    author = "Mahmud, Junayed  and
      Faisal, Fahim  and
      Arnob, Raihan Islam  and
      Anastasopoulos, Antonios  and
      Moran, Kevin",
    booktitle = "Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.nlp4prog-1.1",
    doi = "10.18653/v1/2021.nlp4prog-1.1",
    pages = "1--16",
    abstract = "Automated source code summarization is a popular software engineering research topic wherein machine translation models are employed to {``}translate{''} code snippets into relevant natural language descriptions. Most evaluations of such models are conducted using automatic reference-based metrics. However, given the relatively large semantic gap between programming languages and natural language, we argue that this line of research would benefit from a qualitative investigation into the various error modes of current state-of-the-art models. Therefore, in this work, we perform both a quantitative and qualitative comparison of three recently proposed source code summarization models. In our quantitative evaluation, we compare the models based on the smoothed BLEU-4, METEOR, and ROUGE-L machine translation metrics, and in our qualitative evaluation, we perform a manual open-coding of the most common errors committed by the models when compared to ground truth captions. Our investigation reveals new insights into the relationship between metric-based performance and model prediction errors grounded in an error taxonomy that can be used to drive future research efforts.",
}

@article{Wang2019coset,
  author    = {Ke Wang and
               Mihai Christodorescu},
  title     = {{COSET:} {A} Benchmark for Evaluating Neural Program Embeddings},
  journal   = {CoRR},
  volume    = {abs/1905.11445},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.11445},
  archivePrefix = {arXiv},
  eprint    = {1905.11445},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-11445.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Wang2021sum,
  author    = {Yu Wang and
               Fengjuan Gao and
               Linzhang Wang},
  title     = {Demystifying Code Summarization Models},
  journal   = {CoRR},
  volume    = {abs/2102.04625},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.04625},
  archivePrefix = {arXiv},
  eprint    = {2102.04625},
  timestamp = {Tue, 13 Apr 2021 13:32:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-04625.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Ren2020codebleu,
  author    = {Shuo Ren and
               Daya Guo and
               Shuai Lu and
               Long Zhou and
               Shujie Liu and
               Duyu Tang and
               Neel Sundaresan and
               Ming Zhou and
               Ambrosio Blanco and
               Shuai Ma},
  title     = {CodeBLEU: a Method for Automatic Evaluation of Code Synthesis},
  journal   = {CoRR},
  volume    = {abs/2009.10297},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.10297},
  archivePrefix = {arXiv},
  eprint    = {2009.10297},
  timestamp = {Wed, 30 Sep 2020 08:21:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-10297.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Peng2021understand,
  title = 	 {How could Neural Networks understand Programs?},
  author =       {Peng, Dinglan and Zheng, Shuxin and Li, Yatao and Ke, Guolin and He, Di and Liu, Tie-Yan},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8476--8486},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/peng21b/peng21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/peng21b.html},
  abstract = 	 {Semantic understanding of programs is a fundamental problem for programming language processing (PLP). Recent works that learn representations of code based on pre-training techniques in NLP have pushed the frontiers in this direction. However, the semantics of PL and NL have essential differences. These being ignored, we believe it is difficult to build a model to better understand programs, by either directly applying off-the-shelf NLP pre-training techniques to the source code, or adding features to the model by the heuristic. In fact, the semantics of a program can be rigorously defined by formal semantics in PL theory. For example, the operational semantics, describes the meaning of a valid program as updating the environment (i.e., the memory address-value function) through fundamental operations, such as memory I/O and conditional branching. Inspired by this, we propose a novel program semantics learning paradigm, that the model should learn from information composed of (1) the representations which align well with the fundamental operations in operational semantics, and (2) the information of environment transition, which is indispensable for program understanding. To validate our proposal, we present a hierarchical Transformer-based pre-training model called OSCAR to better facilitate the understanding of programs. OSCAR learns from intermediate representation (IR) and an encoded representation derived from static analysis, which are used for representing the fundamental operations and approximating the environment transitions respectively. OSCAR empirically shows the outstanding capability of program semantics understanding on many practical software engineering tasks. Code and models are released at: \url{https://github.com/pdlan/OSCAR}.}
}


@inproceedings{ahmad2021unified,
    title = "Unified Pre-training for Program Understanding and Generation",
    author = "Ahmad, Wasi  and
      Chakraborty, Saikat  and
      Ray, Baishakhi  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.211",
    doi = "10.18653/v1/2021.naacl-main.211",
    pages = "2655--2668",
    abstract = "Code summarization and generation empower conversion between programming language (PL) and natural language (NL), while code translation avails the migration of legacy code from one PL to another. This paper introduces PLBART, a sequence-to-sequence model capable of performing a broad spectrum of program and language understanding and generation tasks. PLBART is pre-trained on an extensive collection of Java and Python functions and associated NL text via denoising autoencoding. Experiments on code summarization in the English language, code generation, and code translation in seven programming languages show that PLBART outperforms or rivals state-of-the-art models. Moreover, experiments on discriminative tasks, e.g., program repair, clone detection, and vulnerable code detection, demonstrate PLBART{'}s effectiveness in program understanding. Furthermore, analysis reveals that PLBART learns program syntax, style (e.g., identifier naming convention), logical flow (e.g., {``}if{``} block inside an {``}else{``} block is equivalent to {``}else if{``} block) that are crucial to program semantics and thus excels even with limited annotations.",
}

@inproceedings{
    guo2021graphcodebert,
    title={GraphCode{\{}BERT{\}}: Pre-training Code Representations with Data Flow},
    author={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie LIU and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=jLoC4ez43PZ}
}

@inproceedings{feng2020codebert,
    title = "{C}ode{BERT}: A Pre-Trained Model for Programming and Natural Languages",
    author = "Feng, Zhangyin  and
      Guo, Daya  and
      Tang, Duyu  and
      Duan, Nan  and
      Feng, Xiaocheng  and
      Gong, Ming  and
      Shou, Linjun  and
      Qin, Bing  and
      Liu, Ting  and
      Jiang, Daxin  and
      Zhou, Ming",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.139",
    doi = "10.18653/v1/2020.findings-emnlp.139",
    pages = "1536--1547",
    abstract = "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both {``}bimodal{''} data of NL-PL pairs and {``}unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.",
}

@INPROCEEDINGS{White2016clones,  author={White, Martin and Tufano, Michele and Vendome, Christopher and Poshyvanyk, Denys},  booktitle={2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE)},   title={Deep learning code fragments for code clone detection},   year={2016},  volume={},  number={},  pages={87-98},  doi={}}

@inproceedings{Hu2018comment,
    author = {Hu, Xing and Li, Ge and Xia, Xin and Lo, David and Jin, Zhi},
    title = {Deep Code Comment Generation},
    year = {2018},
    isbn = {9781450357142},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi-org.proxy.wm.edu/10.1145/3196321.3196334},
    doi = {10.1145/3196321.3196334},
    abstract = {During software maintenance, code comments help developers comprehend programs and
    reduce additional time spent on reading and navigating source code. Unfortunately,
    these comments are often mismatched, missing or outdated in the software projects.
    Developers have to infer the functionality from the source code. This paper proposes
    a new approach named DeepCom to automatically generate code comments for Java methods.
    The generated comments aim to help developers understand the functionality of Java
    methods. DeepCom applies Natural Language Processing (NLP) techniques to learn from
    a large code corpus and generates comments from learned features. We use a deep neural
    network that analyzes structural information of Java methods for better comments generation.
    We conduct experiments on a large-scale Java corpus built from 9,714 open source projects
    from GitHub. We evaluate the experimental results on a machine translation metric.
    Experimental results demonstrate that our method DeepCom outperforms the state-of-the-art
    by a substantial margin.},
    booktitle = {Proceedings of the 26th Conference on Program Comprehension},
    pages = {200â210},
    numpages = {11},
    keywords = {comment generation, deep learning, program comprehension},
    location = {Gothenburg, Sweden},
    series = {ICPC '18}
}

@inproceedings{Roziere2020transcoder,
 author = {Roziere, Baptiste and Lachaux, Marie-Anne and Chanussot, Lowik and Lample, Guillaume},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {20601--20611},
 publisher = {Curran Associates, Inc.},
 title = {Unsupervised Translation of Programming Languages},
 url = {},
 volume = {33},
 year = {2020}
}


@ARTICLE{Chen2019sequencer,  author={Chen, Zimin and Kommrusch, Steve James and Tufano, Michele and Pouchet, Louis-NoÃ«l and Poshyvanyk, Denys and Monperrus, Martin},  journal={IEEE Transactions on Software Engineering},   title={SEQUENCER: Sequence-to-Sequence Learning for End-to-End Program Repair},   year={2019},  volume={},  number={},  pages={1-1},  doi={10.1109/TSE.2019.2940179}}

@inproceedings{tu2014local,
    author = {Tu, Zhaopeng and Su, Zhendong and Devanbu, Premkumar},
    title = {On the Localness of Software},
    year = {2014},
    isbn = {9781450330565},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi-org.proxy.wm.edu/10.1145/2635868.2635875},
    doi = {10.1145/2635868.2635875},
    abstract = { The n-gram language model, which has its roots in statistical natural language processing,
    has been shown to successfully capture the repetitive and predictable regularities
    (ânaturalness") of source code, and help with tasks such as code suggestion, porting,
    and designing assistive coding devices. However, we show in this paper that this natural-language-based
    model fails to exploit a special property of source code: localness. We find that
    human-written programs are localized: they have useful local regularities that can
    be captured and exploited. We introduce a novel cache language model that consists
    of both an n-gram and an added âcache" component to exploit localness. We show empirically
    that the additional cache component greatly improves the n-gram approach by capturing
    the localness of software, as measured by both cross-entropy and suggestion accuracy.
    Our modelâs suggestion accuracy is actually comparable to a state-of-the-art, semantically
    augmented language model; but it is simpler and easier to implement. Our cache language
    model requires nothing beyond lexicalization, and thus is applicable to all programming
    languages. },
    booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
    pages = {269â280},
    numpages = {12},
    keywords = {Localness, Cache Language Model, Code Suggestion},
    location = {Hong Kong, China},
    series = {FSE 2014}
}

@inproceedings{RahmanICSE'19, author = {Rahman, Musfiqur and Palani, Dharani and Rigby, Peter C.}, title = {Natural Software Revisited}, year = {2019}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/ICSE.2019.00022}, doi = {10.1109/ICSE.2019.00022}, booktitle = {Proceedings of the 41st International Conference on Software Engineering}, pages = {37â48}, numpages = {12}, keywords = {statistical code graphs, language models, basic science, StackOverflow, entropy}, location = {Montreal, Quebec, Canada}, series = {ICSE '19} }

@inproceedings{Nguyen2013ASS,
  title={A statistical semantic language model for source code},
  author={T. Nguyen and A. Nguyen and H. Nguyen},
  booktitle={ESEC/FSE 2013},
  year={2013}
}

@article{Raychev2014CodeCW,
  title={Code completion with statistical language models},
  author={Veselin Raychev and Martin T. Vechev and Eran Yahav},
  journal={Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  year={2014}
}

@inproceedings{hindle2012natural,
    author = {Hindle, Abram and Barr, Earl T. and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar},
    title = {On the Naturalness of Software},
    year = {2012},
    isbn = {9781467310673},
    publisher = {IEEE Press},
    abstract = { Natural languages like English are rich, complex, and powerful. The highly creative
    and graceful use of languages like English and Tamil, by masters like Shakespeare
    and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive
    constraints and the exigencies of daily life, most human utterances are far simpler
    and much more repetitive and predictable. In fact, these utterances can be very usefully
    modeled using modern statistical methods. This fact has led to the phenomenal success
    of statistical approaches to speech recognition, natural language translation, question-answering,
    and text mining and comprehension. We begin with the conjecture that most software
    is also natural, in the sense that it is created by humans at work, with all the attendant
    constraints and limitations---and thus, like natural language, it is also likely to
    be repetitive and predictable. We then proceed to ask whether a) code can be usefully
    modeled by statistical language models and b) such models can be leveraged to support
    software engineers. Using the widely adopted n-gram model, we provide empirical evidence
    supportive of a positive answer to both these questions. We show that code is also
    very repetitive, and in fact even more so than natural languages. As an example use
    of the model, we have developed a simple code completion engine for Java that, despite
    its simplicity, already improves Eclipse's completion capability. We conclude the
    paper by laying out a vision for future research in this area. },
    booktitle = {Proceedings of the 34th International Conference on Software Engineering},
    pages = {837â847},
    numpages = {11},
    location = {Zurich, Switzerland},
    series = {ICSE '12}
}

@inproceedings{gabel2010unique,
    author = {Gabel, Mark and Su, Zhendong},
    title = {A Study of the Uniqueness of Source Code},
    year = {2010},
    isbn = {9781605587912},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi-org.proxy.wm.edu/10.1145/1882291.1882315},
    doi = {10.1145/1882291.1882315},
    abstract = {This paper presents the results of the first study of the uniqueness of source code.
    We define the uniqueness of a unit of source code with respect to the entire body
    of written software, which we approximate with a corpus of 420 million lines of source
    code. Our high-level methodology consists of examining a collection of 6,000 software
    projects and measuring the degree to which each project can be `assembled' solely
    from portions of this corpus, thus providing a precise measure of `uniqueness' that
    we call syntactic redundancy. We parameterized our study over a variety of variables,
    the most important of which being the level of granularity at which we view source
    code. Our suite of experiments together consumed approximately four months of CPU
    time, providing quantitative answers to the following questions: at what levels of
    granularity is software unique, and at a given level of granularity, how unique is
    software? While we believe these questions to be of intrinsic interest, we discuss
    possible applications to genetic programming and developer productivity tools.},
    booktitle = {Proceedings of the Eighteenth ACM SIGSOFT International Symposium on Foundations of Software Engineering},
    pages = {147â156},
    numpages = {10},
    keywords = {software uniqueness, large scale study, source code},
    location = {Santa Fe, New Mexico, USA},
    series = {FSE '10}
}

@inproceedings{khandelwal2018sharp,
    title = "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context",
    author = "Khandelwal, Urvashi  and
      He, He  and
      Qi, Peng  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1027",
    doi = "10.18653/v1/P18-1027",
    pages = "284--294",
    abstract = "We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.",
}

@inproceedings{
    wang2018glue,
    title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
    author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=rJ4km2R5t7},
}

@inproceedings{rajpurkar2018squad,
    title = "Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}",
    author = "Rajpurkar, Pranav  and
      Jia, Robin  and
      Liang, Percy",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-2124",
    doi = "10.18653/v1/P18-2124",
    pages = "784--789",
    abstract = "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD achieves only 66{\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.",
}

@article{goh2021multimodal,
  author = {Goh, Gabriel and â , Nick Cammarata and â , Chelsea Voss and Carter, Shan and Petrov, Michael and Schubert, Ludwig and Radford, Alec and Olah, Chris},
  title = {Multimodal Neurons in Artificial Neural Networks},
  journal = {Distill},
  year = {2021},
  note = {https://distill.pub/2021/multimodal-neurons},
  doi = {10.23915/distill.00030}
}

@article{dai2021knowledge,
  author    = {Damai Dai and
               Li Dong and
               Yaru Hao and
               Zhifang Sui and
               Furu Wei},
  title     = {Knowledge Neurons in Pretrained Transformers},
  journal   = {CoRR},
  volume    = {abs/2104.08696},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.08696},
  archivePrefix = {arXiv},
  eprint    = {2104.08696},
  timestamp = {Wed, 02 Jun 2021 10:17:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08696.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{husain2019codesearchnet,
  title={{CodeSearchNet} challenge: Evaluating the state of semantic code search},
  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
  journal={arXiv preprint arXiv:1909.09436},
  year={2019}
}


@article{lu2021codexglue,
  author    = {Shuai Lu and
               Daya Guo and
               Shuo Ren and
               Junjie Huang and
               Alexey Svyatkovskiy and
               Ambrosio Blanco and
               Colin B. Clement and
               Dawn Drain and
               Daxin Jiang and
               Duyu Tang and
               Ge Li and
               Lidong Zhou and
               Linjun Shou and
               Long Zhou and
               Michele Tufano and
               Ming Gong and
               Ming Zhou and
               Nan Duan and
               Neel Sundaresan and
               Shao Kun Deng and
               Shengyu Fu and
               Shujie Liu},
  title     = {CodeXGLUE: {A} Machine Learning Benchmark Dataset for Code Understanding
               and Generation},
  journal   = {CoRR},
  volume    = {abs/2102.04664},
  year      = {2021}
}

@article{rabin2021generalizability,
  title={On the generalizability of Neural Program Models with respect to semantic-preserving program transformations},
  author={Rabin, Md Rafiqul Islam and Bui, Nghi DQ and Wang, Ke and Yu, Yijun and Jiang, Lingxiao and Alipour, Mohammad Amin},
  journal={Information and Software Technology},
  volume={135},
  pages={106552},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{tenney2019bert,
    title = "{BERT} Rediscovers the Classical {NLP} Pipeline",
    author = "Tenney, Ian  and
      Das, Dipanjan  and
      Pavlick, Ellie",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1452",
    doi = "10.18653/v1/P19-1452",
    pages = "4593--4601",
    abstract = "Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",
}

@inproceedings{rogers2018whats,
    title = "What{'}s in Your Embedding, And How It Predicts Task Performance",
    author = "Rogers, Anna  and
      Hosur Ananthakrishna, Shashwath  and
      Rumshisky, Anna",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1228",
    pages = "2690--2703",
    abstract = "Attempts to find a single technique for general-purpose intrinsic evaluation of word embeddings have so far not been successful. We present a new approach based on scaled-up qualitative analysis of word vector neighborhoods that quantifies interpretable characteristics of a given model (e.g. its preference for synonyms or shared morphological forms as nearest neighbors). We analyze 21 such factors and show how they correlate with performance on 14 extrinsic and intrinsic task datasets (and also explain the lack of correlation between some of them). Our approach enables multi-faceted evaluation, parameter search, and generally {--} a more principled, hypothesis-driven approach to development of distributional semantic representations.",
}

@inproceedings{prabhakaran2019perturbation,
    title = "Perturbation Sensitivity Analysis to Detect Unintended Model Biases",
    author = "Prabhakaran, Vinodkumar  and
      Hutchinson, Ben  and
      Mitchell, Margaret",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1578",
    doi = "10.18653/v1/D19-1578",
    pages = "5740--5745",
    abstract = "Data-driven statistical Natural Language Processing (NLP) techniques leverage large amounts of language data to build models that can understand language. However, most language data reflect the public discourse at the time the data was produced, and hence NLP models are susceptible to learning incidental associations around named referents at a particular point in time, in addition to general linguistic meaning. An NLP system designed to model notions such as sentiment and toxicity should ideally produce scores that are independent of the identity of such entities mentioned in text and their social associations. For example, in a general purpose sentiment analysis system, a phrase such as I hate Katy Perry should be interpreted as having the same sentiment as I hate Taylor Swift. Based on this idea, we propose a generic evaluation framework, Perturbation Sensitivity Analysis, which detects unintended model biases related to named entities, and requires no new annotations or corpora. We demonstrate the utility of this analysis by employing it on two different NLP models {---} a sentiment model and a toxicity model {---} applied on online comments in English language from four different genres.",
}

@inproceedings{kim2019probing,
    title = "Probing What Different {NLP} Tasks Teach Machines about Function Word Comprehension",
    author = "Kim, Najoung  and
      Patel, Roma  and
      Poliak, Adam  and
      Xia, Patrick  and
      Wang, Alex  and
      McCoy, Tom  and
      Tenney, Ian  and
      Ross, Alexis  and
      Linzen, Tal  and
      Van Durme, Benjamin  and
      Bowman, Samuel R.  and
      Pavlick, Ellie",
    booktitle = "Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S19-1026",
    doi = "10.18653/v1/S19-1026",
    pages = "235--249",
    abstract = "We introduce a set of nine challenge tasks that test for the understanding of function words. These tasks are created by structurally mutating sentences from existing datasets to target the comprehension of specific types of function words (e.g., prepositions, wh-words). Using these probing tasks, we explore the effects of various pretraining objectives for sentence encoders (e.g., language modeling, CCG supertagging and natural language inference (NLI)) on the learned representations. Our results show that pretraining on CCG{---}our most syntactic objective{---}performs the best on average across our probing tasks, suggesting that syntactic knowledge helps function word comprehension. Language modeling also shows strong performance, supporting its widespread use for pretraining state-of-the-art NLP models. Overall, no pretraining objective dominates across the board, and our function word probing tasks highlight several intuitive differences between pretraining objectives, e.g., that NLI helps the comprehension of negation.",
}

@inproceedings{wu2019errudite,
    title = "{E}rrudite: Scalable, Reproducible, and Testable Error Analysis",
    author = "Wu, Tongshuang  and
      Ribeiro, Marco Tulio  and
      Heer, Jeffrey  and
      Weld, Daniel",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1073",
    doi = "10.18653/v1/P19-1073",
    pages = "747--763",
    abstract = "Though error analysis is crucial to understanding and improving NLP models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. This paper codifies model and task agnostic principles for informative error analysis, and presents Errudite, an interactive tool for better supporting this process. First, error groups should be precisely defined for reproducibility; Errudite supports this with an expressive domain-specific language. Second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; Errudite enables systematic grouping of relevant instances with filtering queries. Third, hypotheses about the cause of errors should be explicitly tested; Errudite supports this via automated counterfactual rewriting. We validate our approach with a user study, finding that Errudite (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs.",
}

@inproceedings{ribeiro2018semantically,
    title = "Semantically Equivalent Adversarial Rules for Debugging {NLP} models",
    author = "Ribeiro, Marco Tulio  and
      Singh, Sameer  and
      Guestrin, Carlos",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1079",
    doi = "10.18653/v1/P18-1079",
    pages = "856--865",
    abstract = "Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) {--} semantic-preserving perturbations that induce changes in the model{'}s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) {--} simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.",
}


@article{jedlitschka2014reporting,
  title={Reporting experiments to satisfy professionalsâ information needs},
  author={Jedlitschka, Andreas and Juristo, Natalia and Rombach, Dieter},
  journal={Empirical Software Engineering},
  volume={19},
  number={6},
  pages={1921--1955},
  year={2014},
  publisher={Springer}
}

@book{wohlin2012experimentation,
  title={Experimentation in software engineering},
  author={Wohlin, Claes and Runeson, Per and H{\"o}st, Martin and Ohlsson, Magnus C and Regnell, Bj{\"o}rn and Wessl{\'e}n, Anders},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@book{menzies2016ds4se,
  title={Perspectives on data science for software engineering},
  author={Menzies, Tim and Williams, Laurie and Zimmermann, Thomas},
  year={2016},
  publisher={Morgan Kaufmann}
}

@article{arcuri2014guide,
    author = {Arcuri, Andrea and Briand, Lionel},
    title = {A Hitchhiker's Guide to Statistical Tests for Assessing Randomized Algorithms in Software Engineering},
    year = {2014},
    issue_date = {May 2014},
    publisher = {John Wiley and Sons Ltd.},
    address = {GBR},
    volume = {24},
    number = {3},
    issn = {0960-0833},
    url = {https://doi.org/10.1002/stvr.1486},
    doi = {10.1002/stvr.1486},
    abstract = {Randomized algorithms are widely used to address many types of software engineering
    problems, especially in the area of software verification and validation with a strong
    emphasis on test automation. However, randomized algorithms are affected by chance
    and so require the use of appropriate statistical tests to be properly analysed in
    a sound manner. This paper features a systematic review regarding recent publications
    in 2009 and 2010 showing that, overall, empirical analyses involving randomized algorithms
    in software engineering tend to not properly account for the random nature of these
    algorithms. Many of the novel techniques presented clearly appear promising, but the
    lack of soundness in their empirical evaluations casts unfortunate doubts on their
    actual usefulness. In software engineering, although there are guidelines on how to
    carry out empirical analyses involving human subjects, those guidelines are not directly
    and fully applicable to randomized algorithms. Furthermore, many of the textbooks
    on statistical analysis are written from the viewpoints of social and natural sciences,
    which present different challenges from randomized algorithms. To address the questionable
    overall quality of the empirical analyses reported in the systematic review, this
    paper provides guidelines on how to carry out and properly analyse randomized algorithms
    applied to solve software engineering tasks, with a particular focus on software testing,
    which is by far the most frequent application area of randomized algorithms within
    software engineering. Copyright Â© 2012 John Wiley &amp; Sons, Ltd.},
    journal = {Softw. Test. Verif. Reliab.},
    month = may,
    pages = {219â250},
    numpages = {32},
    keywords = {effect size, confidence interval, systematic review, statistical difference, nonparametric test, survey, parametric test, Bonferroni adjustment}
}

@ARTICLE{furia2019bayesianse,  author={Furia, Carlo Alberto and Feldt, Robert and Torkar, Richard},  journal={IEEE Transactions on Software Engineering},   title={Bayesian Data Analysis in Empirical Software Engineering Research},   year={2019},  volume={},  number={},  pages={1-1},  doi={10.1109/TSE.2019.2935974}}

@inproceedings{vaswani2017transformers,
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
    title = {Attention is All You Need},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional
    neural networks that include an encoder and a decoder. The best performing models
    also connect the encoder and decoder through an attention mechanism. We propose a
    new simple network architecture, the Transformer, based solely on attention mechanisms,
    dispensing with recurrence and convolutions entirely. Experiments on two machine translation
    tasks show these models to be superior in quality while being more parallelizable
    and requiring significantly less time to train. Our model achieves 28.4 BLEU on the
    WMT 2014 English-to-German translation task, improving over the existing best results,
    including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation
    task, our model establishes a new single-model state-of-the-art BLEU score of 41.0
    after training for 3.5 days on eight GPUs, a small fraction of the training costs
    of the best models from the literature.},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {6000â6010},
    numpages = {11},
    location = {Long Beach, California, USA},
    series = {NeurIPs'17}
}

@article{dehghani2021benchmark,
  author    = {Mostafa Dehghani and
               Yi Tay and
               Alexey A. Gritsenko and
               Zhe Zhao and
               Neil Houlsby and
               Fernando Diaz and
               Donald Metzler and
               Oriol Vinyals},
  title     = {The Benchmark Lottery},
  journal   = {CoRR},
  volume    = {abs/2107.07002},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.07002},
  archivePrefix = {arXiv},
  eprint    = {2107.07002},
  timestamp = {Wed, 21 Jul 2021 15:55:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-07002.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{jain1999rnn,
    author = {Jain, L. C. and Medsker, L. R.},
    title = {Recurrent Neural Networks: Design and Applications},
    year = {1999},
    isbn = {0849371813},
    publisher = {CRC Press, Inc.},
    address = {USA},
    edition = {1st},
    abstract = {From the Publisher:With applications ranging from motion detection to financial forecasting,
    recurrent neural networks (RNNs) have emerged as an interesting and important part
    of neural network research. Recurrent Neural Networks: Design and Applications reflects
    the tremendous, worldwide interest in and virtually unlimited potential of RNNs -
    providing a summary of the design, applications, current research, and challenges
    of this dynamic and promising field.}
}

@inproceedings{bender2021parrots,
    author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Mitchell, Margaret},
    title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
    year = {2021},
    isbn = {9781450383097},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi-org.proxy.wm.edu/10.1145/3442188.3445922},
    doi = {10.1145/3442188.3445922},
    abstract = {The past 3 years of work in NLP have been characterized by the development and deployment
    of ever larger language models, especially for English. BERT, its variants, GPT-2/3,
    and others, most recently Switch-C, have pushed the boundaries of the possible both
    through architectural innovations and through sheer size. Using these pretrained models
    and the methodology of fine-tuning them for specific tasks, researchers have extended
    the state of the art on a wide array of tasks as measured by leaderboards on specific
    benchmarks for English. In this paper, we take a step back and ask: How big is too
    big? What are the possible risks associated with this technology and what paths are
    available for mitigating those risks? We provide recommendations including weighing
    the environmental and financial costs first, investing resources into curating and
    carefully documenting datasets rather than ingesting everything on the web, carrying
    out pre-development exercises evaluating how the planned approach fits into research
    and development goals and supports stakeholder values, and encouraging research directions
    beyond ever larger language models.},
    booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
    pages = {610â623},
    numpages = {14},
    location = {Virtual Event, Canada},
    series = {FAccT '21}
}

@misc{chen2021evaluating,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
 
 @misc{ciniselli2021empirical,
      title={An Empirical Study on the Usage of Transformer Models for Code Completion}, 
      author={Matteo Ciniselli and Nathan Cooper and Luca Pascarella and Antonio Mastropaolo and Emad Aghajani and Denys Poshyvanyk and Massimiliano Di Penta and Gabriele Bavota},
      year={2021},
      eprint={2108.01585},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{tabnine, title={Code faster with ai code completions}, url={https://www.tabnine.com/}, journal={Code Faster with AI Code Completions}, publisher={Tabnine}} 
 
@misc{openai_codex, title={OpenAI Codex}, url={https://openai.com/blog/openai-codex/}, journal={OpenAI}, publisher={OpenAI}, author={Zaremba, Wojciech and Brockman, Greg and OpenAI}, year={2021}, month={Aug}} 

@misc{github_copilot, title={GitHub Copilot Â· Your AI pair programmer}, url={https://copilot.github.com/}, journal={GitHub Copilot}, author={GitHub} }

@misc{intellicode, title={Visual Studio IntelliCode | Visual Studio - Visual Studio}, url={https://visualstudio.microsoft.com/services/intellicode/}, journal={visualstudio.microsoft.com}, author={Microsoft} }

@article{karpathy2015understand,
  author    = {Andrej Karpathy and
               Justin Johnson and
               Fei{-}Fei Li},
  title     = {Visualizing and Understanding Recurrent Networks},
  journal   = {CoRR},
  volume    = {abs/1506.02078},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.02078},
  archivePrefix = {arXiv},
  eprint    = {1506.02078},
  timestamp = {Sat, 19 Oct 2019 16:30:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KarpathyJL15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{molnar2019interpret,
  title      = {Interpretable Machine Learning},
  author     = {Christoph Molnar},
  note       = {\url{https://christophm.github.io/interpretable-ml-book/}},
  year       = {2019},
  subtitle   = {A Guide for Making Black Box Models Explainable}
}

@article{kocmi2021ship,
  author    = {Tom Kocmi and
               Christian Federmann and
               Roman Grundkiewicz and
               Marcin Junczys{-}Dowmunt and
               Hitokazu Matsushita and
               Arul Menezes},
  title     = {To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics
               for Machine Translation},
  journal   = {CoRR},
  volume    = {abs/2107.10821},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.10821},
  archivePrefix = {arXiv},
  eprint    = {2107.10821},
  timestamp = {Thu, 29 Jul 2021 16:14:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-10821.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{rei2020comet,
    title = "{COMET}: A Neural Framework for {MT} Evaluation",
    author = "Rei, Ricardo  and
      Stewart, Craig  and
      Farinha, Ana C  and
      Lavie, Alon",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.213",
    doi = "10.18653/v1/2020.emnlp-main.213",
    pages = "2685--2702",
    abstract = "We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements. Our framework leverages recent breakthroughs in cross-lingual pretrained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality. To showcase our framework, we train three models with different types of human judgements: Direct Assessments, Human-mediated Translation Edit Rate and Multidimensional Quality Metric. Our models achieve new state-of-the-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems.",
}

@inproceedings{ribeiro2020checklist,
    title = "Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist",
    author = "Ribeiro, Marco Tulio  and
      Wu, Tongshuang  and
      Guestrin, Carlos  and
      Singh, Sameer",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.442",
    doi = "10.18653/v1/2020.acl-main.442",
    pages = "4902--4912",
    abstract = "Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.",
}

@article{watson2020dl4se,
  author    = {Cody Watson and
               Nathan Cooper and
               David Nader{-}Palacio and
               Kevin Moran and
               Denys Poshyvanyk},
  title     = {A Systematic Literature Review on the Use of Deep Learning in Software
               Engineering Research},
  journal   = {CoRR},
  volume    = {abs/2009.06520},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.06520},
  archivePrefix = {arXiv},
  eprint    = {2009.06520},
  timestamp = {Fri, 18 Sep 2020 15:17:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-06520.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{murphy2006ide,
author = {Murphy, Gail C. and Kersten, Mik and Findlater, Leah},
title = {How Are Java Software Developers Using the Eclipse IDE?},
year = {2006},
issue_date = {July 2006},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {23},
number = {4},
issn = {0740-7459},
url = {https://doi.org/10.1109/MS.2006.105},
doi = {10.1109/MS.2006.105},
abstract = {Eclipse is a leading development environment that provides a rich set of features
supporting Java development. However, little data is available about its usage. Usage
data from 41 developers using Java and Eclipse shows that they're using advanced features
such as refactoring and are extending the environment using third-party tools. However,
they rarely use some of the other features, such as bookmarking places in the code.
The article also includes briefly describes the authors' Eclipse-based open-source
analysis framework. Open-source projects such as Eclipse should be gathering and analyzing
more usage data to ensure the tools they're building evolve to meet user communities'
needs.},
journal = {IEEE Softw.},
month = jul,
pages = {76â83},
numpages = {8},
keywords = {programming environments, construction tools, coding tools and techniques}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% FROM MENDELEY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
@article{Karampatsis2019,
    title = {{Maybe deep neural networks are the best choice for modeling source code}},
    year = {2019},
    journal = {arXiv},
    author = {Karampatsis, Rafael Michael and Sutton, Charles},
    number = {Lm},
    issn = {23318422},
    arxivId = {1903.05734},
    keywords = {BPE, Code, Language models, Naturalness, Neural network, Software tools}
}

@inproceedings{White:MSR15,
    title = {{Toward Deep Learning Software Repositories}},
    year = {2015},
    booktitle = {Proceedings of the 12th IEEE Working Conference on Mining Software Repositories (MSR'15)},
    author = {White, Martin and Vendome, Christopher and Linares-V{\'{a}}squez, Mario and Poshyvanyk, Denys},
    pages = {334--345},
    series = {MSR '15},
    publisher = {IEEE Press},
    url = {http://dl.acm.org/citation.cfm?id=2820518.2820559},
    address = {Piscataway, NJ, USA},
    isbn = {978-0-7695-5594-2},
    keywords = {deep learning, machine learning, n-grams, neural networks, software language models, software repositories}
}

@article{Arcuri2014AEngineering,
    title = {{A Hitchhiker's guide to statistical tests for assessing randomized algorithms in software engineering}},
    year = {2014},
    journal = {Software Testing Verification and Reliability},
    author = {Arcuri, Andrea and Briand, Lionel},
    number = {3},
    pages = {219--250},
    volume = {24},
    doi = {10.1002/stvr.1486},
    issn = {10991689},
    keywords = {Bonferroni adjustment, confidence interval, effect size, nonparametric test, parametric test, statistical difference, survey, systematic review}
}

@article{Torkar2021AEngineering,
    title = {{A Method to Assess and Argue for Practical Significance in Software Engineering}},
    year = {2021},
    journal = {IEEE Transactions on Software Engineering},
    author = {Torkar, Richard and Furia, Carlo Alberto and Feldt, Robert and Gomes de Oliveira Neto, Francisco and Gren, Lucas and Lenberg, Per and Ernst, Neil A.},
    number = {X},
    pages = {1--13},
    volume = {XX},
    doi = {10.1109/TSE.2020.3048991},
    issn = {19393520},
    arxivId = {1809.09849},
    keywords = {Analytical models, Bayes methods, Bayesian analysis, Data models, Decision making, Software engineering, Statistical analysis, Testing, empirical software engineering, practical significance, statistical significance}
}

@article{Bengio2003AModel,
    title = {{A neural probabilistic language model}},
    year = {2003},
    journal = {Advances in Neural Information Processing Systems},
    author = {Bengio, Yoshua and Ducharme, RÃ©jean and Vincent, Pascal},
    pages = {1137--1155},
    volume = {3},
    isbn = {0262122413},
    issn = {10495258},
    keywords = {artificial neural networks, curse of dimensionality, distributed representation, statistical language modeling}
}

@article{Hellendoorn2017AreCode,
    title = {{Are deep neural networks the best choice for modeling source code?}},
    year = {2017},
    author = {Hellendoorn, Vincent J. and Devanbu, Premkumar},
    pages = {763--773},
    isbn = {9781450351058},
    doi = {10.1145/3106237.3106290},
    keywords = {2017, acm reference format, are deep neural, hellendoorn and premkumar devanbu, language models, naturalness, software tools, vincent j}
}

@article{Furia2019BayesianResearch,
    title = {{Bayesian Data Analysis in Empirical Software Engineering Research}},
    year = {2019},
    journal = {IEEE Transactions on Software Engineering},
    author = {Furia, Carlo Alberto and Feldt, Robert and Torkar, Richard},
    pages = {1--26},
    doi = {10.1109/TSE.2019.2935974},
    issn = {19393520},
    arxivId = {1811.05422},
    keywords = {Bayesian data analysis, empirical software engineering, statistical analysis, statistical hypothesis testing}
}

@article{Doshi-Velez2018ConsiderationsLearning,
    title = {{Considerations for Evaluation and Generalization in Interpretable Machine Learning}},
    year = {2018},
    author = {Doshi-Velez, Finale and Kim, Been},
    pages = {3--17},
    doi = {10.1007/978-3-319-98131-4{\_}1}
}

@article{Hernan2019DataTasks.,
    title = {{Data science is scienceâs second chance to get causal inference right. A classification of data science tasks.}},
    year = {2019},
    journal = {arXiv},
    author = {Hernan, Miguel A. and Hsu, John and Brian, Healy},
    number = {1},
    pages = {3--4},
    volume = {56}
}

@article{PaulRalphNaumanbinAliSebastianBaltesDomenicoBianculliJessicaDiazYvonneDittrichNeilErnstMichaelFeldererRobertFeldtAntonioFilieriBrenoBernardNicolaudeFrancaCarloAlbertoFuriaGregGayNicolasGoldDanielGraziotinP2020EmpiricalResearch,
    title = {{Empirical Standards for Software Engineering Research}},
    year = {2020},
    journal = {arXiv},
    author = {Paul Ralph, Nauman bin Ali, Sebastian Baltes, Domenico Bianculli, Jessica Diaz, Yvonne Dittrich, Neil Ernst, Michael Felderer, Robert Feldt, Antonio Filieri, Breno Bernard Nicolau de Fran{\c{c}}a, Carlo Alberto Furia, Greg Gay, Nicolas Gold, Daniel Graziotin, P, Sira Vegas},
    pages = {1--20}
}

@article{Velmurugan2020EvaluatingApproach,
    title = {{Evaluating Explainable Methods for Predictive Process Analytics: A Functionally-Grounded Approach}},
    year = {2020},
    author = {Velmurugan, Mythreyi and Ouyang, Chun and Moreira, Catarina and Sindhgatta, Renuka},
    url = {http://arxiv.org/abs/2012.04218},
    arxivId = {2012.04218},
    keywords = {evaluation, explainable ai, explanation fidelity, explanation stability, metrics, predictive process analytics}
}

@article{Chen2021EvaluatingCode,
    title = {{Evaluating Large Language Models Trained on Code}},
    year = {2021},
    author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
    url = {http://arxiv.org/abs/2107.03374},
    arxivId = {2107.03374}
}

@article{Gilpin2019ExplainingLearning,
    title = {{Explaining explanations: An overview of interpretability of machine learning}},
    year = {2019},
    journal = {Proceedings - 2018 IEEE 5th International Conference on Data Science and Advanced Analytics, DSAA 2018},
    author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
    pages = {80--89},
    isbn = {9781538650905},
    doi = {10.1109/DSAA.2018.00018},
    arxivId = {1806.00069},
    keywords = {Deep learning and deep analytics, Fairness and transparency in data science, Machine learning theories, Models and systems}
}

@article{Kim2018InterpretabilityTCAV,
    title = {{Interpretability beyond feature attribution: Quantitative Testing with Concept Activation Vectors (TCAV)}},
    year = {2018},
    journal = {35th International Conference on Machine Learning, ICML 2018},
    author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
    pages = {4186--4195},
    volume = {6},
    isbn = {9781510867963},
    arxivId = {1711.11279}
}

@article{Molnar2020InterpretableChallenges,
    title = {{Interpretable Machine Learning -- A Brief History, State-of-the-Art and Challenges}},
    year = {2020},
    author = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
    number = {01},
    pages = {1--15},
    url = {http://arxiv.org/abs/2010.09337},
    arxivId = {2010.09337},
    keywords = {explainable artificial in-, interpretable machine learning}
}

@book{Bender2021OnBig,
    title = {{On the dangers of stochastic parrots: Can language models be too big?}},
    year = {2021},
    booktitle = {FAccT 2021 - Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
    author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
    number = {1},
    pages = {610--623},
    volume = {1},
    publisher = {Association for Computing Machinery},
    isbn = {9781450383097},
    doi = {10.1145/3442188.3445922}
}

@article{Bommasani2021OnModels,
    title = {{On the Opportunities and Risks of Foundation Models}},
    year = {2021},
    author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Kohd, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and R{\'{e}}, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tram{\`{e}}r, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
    pages = {1--212},
    url = {http://arxiv.org/abs/2108.07258},
    arxivId = {2108.07258}
}

@article{Karampatsis2020Open-VocabularyAbstract,
    title = {{Open-Vocabulary Models for Source Code (Extended Abstract)}},
    year = {2020},
    journal = {Proceedings - 2020 ACM/IEEE 42nd International Conference on Software Engineering: Companion, ICSE-Companion 2020},
    author = {Karampatsis, Rafael Michael and Babii, Hlib and Robbes, Romain and Sutton, Charles and Janes, Andrea},
    pages = {294--295},
    isbn = {9781450371223},
    doi = {10.1145/3377812.3390806},
    issn = {02705257},
    keywords = {Byte-Pair Encoding, Naturalness of code, Neural Language Models}
}

@article{Svajlenko2015EvaluatingBigCloneBench,
    title = {{Evaluating clone detection tools with BigCloneBench}},
    year = {2015},
    journal = {2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings},
    author = {Svajlenko, Jeffrey and Roy, Chanchal K.},
    pages = {131--140},
    publisher = {IEEE},
    isbn = {9781467375320},
    doi = {10.1109/ICSM.2015.7332459},
    keywords = {Benchmark testing, Big data, Cloning, Data mining, Java, Software systems}
}

@article{Tufano2019LearningBug-Fixes,
    title = {{Learning How to Mutate Source Code from Bug-Fixes}},
    year = {2019},
    journal = {Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019},
    author = {Tufano, Michele and Watson, Cody and Bavota, Gabriele and Di Penta, Massimiliano and White, Martin and Poshyvanyk, Denys},
    pages = {301--312},
    isbn = {9781728130941},
    doi = {10.1109/ICSME.2019.00046},
    arxivId = {1812.10772},
    keywords = {Mutation testing, deep learning, neural networks}
}

@article{Karampatsis2020BigCode,
    title = {{Big code != big vocabulary: Open-vocabulary models for source code}},
    year = {2020},
    journal = {Proceedings - International Conference on Software Engineering},
    author = {Karampatsis, Rafael Michael and Babii, Hlib and Robbes, Romain and Sutton, Charles and Janes, Andrea},
    pages = {1073--1085},
    isbn = {9781450371216},
    doi = {10.1145/3377811.3380342},
    issn = {02705257},
    arxivId = {2003.07914},
    keywords = {Byte-pair encoding, Naturalness of code, Neural language models}
}

@article{Austin2021ProgramModels,
    title = {{Program Synthesis with Large Language Models}},
    year = {2021},
    author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles},
    pages = {1--34},
    url = {http://arxiv.org/abs/2108.07732},
    arxivId = {2108.07732}
}

@article{Clement2020PyMT5:Transformers,
    title = {{PyMT5: multi-mode translation of natural language and Python code with transformers}},
    year = {2020},
    author = {Clement, Colin and Drain, Dawn and Timcheck, Jonathan and Svyatkovskiy, Alexey and Sundaresan, Neel},
    pages = {9052--9065},
    doi = {10.18653/v1/2020.emnlp-main.728},
    arxivId = {2010.03150}
}

@article{Agarwal2020QualityTranslation,
    title = {{Quality estimation {\&} interpretability for code translation}},
    year = {2020},
    journal = {arXiv},
    author = {Agarwal, Mayank and Talamadupula, Kartik and Houde, Stephanie and Martinez, Fernando and Muller, Michael and Richards, John and Ross, Steven and Weisz, Justin D.},
    issn = {23318422},
    arxivId = {2012.07581}
}

@article{Amrhein2019RetireSignatories,
    title = {{Retire statistical significance Valentin Amrhein, Sander Greenland, Blake McShane and more than 800 signatories}},
    year = {2019},
    journal = {Nature},
    author = {Amrhein, Valentin and Greenland, Sander and McShane, Blake},
    pages = {305--307},
    volume = {567}
}

@book{Leblanc2015SoftwareEngineers,
    title = {{Software Metrics Fundamentals of Dependable Computing for Software Engineers}},
    year = {2015},
    author = {Leblanc, Richard and Dingle, Adair and Hagar, Jon Duncan and Knight, John},
    isbn = {9781439838235}
}

@article{Mastropaolo2021StudyingTasks,
    title = {{Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks}},
    year = {2021},
    author = {Mastropaolo, Antonio and Scalabrino, Simone and Cooper, Nathan and Nader Palacio, David and Poshyvanyk, Denys and Oliveto, Rocco and Bavota, Gabriele},
    pages = {336--347},
    doi = {10.1109/icse43902.2021.00041},
    arxivId = {2102.02017}
}

@article{Doshi-Velez2017TowardsLearning,
    title = {{Towards A Rigorous Science of Interpretable Machine Learning}},
    year = {2017},
    author = {Doshi-Velez, Finale and Kim, Been},
    number = {Ml},
    pages = {1--13},
    url = {http://arxiv.org/abs/1702.08608},
    arxivId = {1702.08608}
}

@article{Scholkopf2022,
   abstract = {We describe basic ideas underlying research to build and understand artificially intelligent systems: from symbolic approaches via statistical learning to interventional models relying on concepts of causality. Some of the hard open problems of machine learning and AI are intrinsically related to causality, and progress may require advances in our understanding of how to model and infer causality from data.},
   author = {Bernhard SchÃ¶lkopf and Julius von KÃ¼gelgen},
   month = {4},
   title = {From Statistical to Causal Learning},
   url = {http://arxiv.org/abs/2204.00607},
   year = {2022},
}

@article{Hermans2013TrainingNetworks,
    title = {{Training and Analyzing Deep Recurrent Neural Networks}},
    year = {2013},
    journal = {Advances in Neural Information Processing Systems},
    author = {Hermans, Michiel and Schrauwen, Benjamin},
    issn = {04706625},
    pmid = {5404171}
}

@article{DAmour2020UnderspecificationLearning,
    title = {{Underspecification Presents Challenges for Credibility in Modern Machine Learning}},
    year = {2020},
    author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
    url = {http://arxiv.org/abs/2011.03395},
    arxivId = {2011.03395}
}

@article{Karpathy2015VisualizingNetworks,
    title = {{Visualizing and Understanding Recurrent Networks}},
    year = {2015},
    author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
    pages = {1--12},
    url = {http://arxiv.org/abs/1506.02078},
    arxivId = {1506.02078}
}

@article{DBLP:journals/corr/abs-2102-04664,
  author    = {Shuai Lu and
               Daya Guo and
               Shuo Ren and
               Junjie Huang and
               Alexey Svyatkovskiy and
               Ambrosio Blanco and
               Colin B. Clement and
               Dawn Drain and
               Daxin Jiang and
               Duyu Tang and
               Ge Li and
               Lidong Zhou and
               Linjun Shou and
               Long Zhou and
               Michele Tufano and
               Ming Gong and
               Ming Zhou and
               Nan Duan and
               Neel Sundaresan and
               Shao Kun Deng and
               Shengyu Fu and
               Shujie Liu},
  title     = {CodeXGLUE: {A} Machine Learning Benchmark Dataset for Code Understanding
               and Generation},
  journal   = {CoRR},
  volume    = {abs/2102.04664},
  year      = {2021}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END FROM MENDELEY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@article{molnar_interpretable_2020,
	title = {Interpretable {Machine} {Learning} -- {A} {Brief} {History}, {State}-of-the-{Art} and {Challenges}},
	url = {http://arxiv.org/abs/2010.09337},
	abstract = {We present a brief history of the field of interpretable machine learning (IML), give an overview of state-of-the-art interpretation methods, and discuss challenges. Research in IML has boomed in recent years. As young as the field is, it has over 200 years old roots in regression modeling and rule-based machine learning, starting in the 1960s. Recently, many new IML methods have been proposed, many of them model-agnostic, but also interpretation techniques specific to deep learning and tree-based ensembles. IML methods either directly analyze model components, study sensitivity to input perturbations, or analyze local or global surrogate approximations of the ML model. The field approaches a state of readiness and stability, with many methods not only proposed in research, but also implemented in open-source software. But many important challenges remain for IML, such as dealing with dependent features, causal interpretation, and uncertainty estimation, which need to be resolved for its successful application to scientific problems. A further challenge is a missing rigorous definition of interpretability, which is accepted by the community. To address the challenges and advance the field, we urge to recall our roots of interpretable, data-driven modeling in statistics and (rule-based) ML, but also to consider other areas such as sensitivity analysis, causal inference, and the social sciences.},
	number = {01},
	author = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
	year = {2020},
	note = {arXiv: 2010.09337},
	keywords = {explainable artificial in-, interpretable machine learning},
	pages = {1--15},
	file = {PDF:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/7MMPE7H9/[interpretability]2010.09337v1.pdf:application/pdf},
}

@misc{doshi-velez_towards_2017,
	title = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	language = {en},
	urldate = {2023-02-01},
	publisher = {arXiv},
	author = {Doshi-Velez, Finale and Kim, Been},
	month = mar,
	year = {2017},
	note = {arXiv:1702.08608 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/ZNG5XLMX/Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf:application/pdf},
}

@inproceedings{vollert_interpretable_2021,
	address = {Vasteras, Sweden},
	title = {Interpretable {Machine} {Learning}: {A} brief survey from the predictive maintenance perspective},
	isbn = {978-1-72812-989-1},
	shorttitle = {Interpretable {Machine} {Learning}},
	url = {https://ieeexplore.ieee.org/document/9613467/},
	doi = {10.1109/ETFA45728.2021.9613467},
	abstract = {In the ï¬eld of predictive maintenance (PdM), machine learning (ML) has gained importance over the last years. Accompanying this development, an increasing number of papers use non-interpretable ML to address PdM problems. While ML has achieved unprecedented performance in recent years, the lack of model explainability or interpretability may manifest itself in a lack of trust. The interpretability of ML models is researched under the terms explainable AI (XAI) and interpretable ML. In this paper, we review publications addressing PdM problems which are motivated by model interpretability. This comprises intrinsically interpretable models and post-hoc explanations. We identify challenges of interpretable ML for PdM, including (1) evaluation of interpretability, (2) the observation that explanation methods explaining black box models may show black box behavior themselves, (3) non-consistent use of terminology, (4) a lack of research for time series data, (5) coverage of explanations, and ï¬nally (6) the inclusion of domain knowledge.},
	language = {en},
	urldate = {2023-02-01},
	booktitle = {2021 26th {IEEE} {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA} )},
	publisher = {IEEE},
	author = {Vollert, Simon and Atzmueller, Martin and Theissler, Andreas},
	month = sep,
	year = {2021},
	pages = {01--08},
	file = {Vollert et al. - 2021 - Interpretable Machine Learning A brief survey fro.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/I9C6AMQH/Vollert et al. - 2021 - Interpretable Machine Learning A brief survey fro.pdf:application/pdf},
}

@article{belle_principles_2020,
	title = {Principles and {Practice} of {Explainable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2009.11698},
	abstract = {Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with significant challenges: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods -- machine learning (ML) and pattern recognition models in particular -- so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature, or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions.},
	author = {Belle, Vaishak and Papantonis, Ioannis},
	year = {2020},
	note = {arXiv: 2009.11698},
	file = {PDF:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/894XV8R9/[interpretability]2009.11698v1.pdf:application/pdf},
}

@article{wainakh_evaluating_2019,
	title = {Evaluating semantic representations of source code},
	issn = {23318422},
	abstract = {Learned representations of source code enable various software developer tools, e.g., to detect bugs or to predict program properties. At the core of code representations often are word embeddings of identifier names in source code, because identifiers account for the majority of source code vocabulary and convey important semantic information. Unfortunately, there currently is no generally accepted way of evaluating the quality of word embeddings of identifiers, and current evaluations are biased toward specific downstream tasks. This paper presents IdBench, the first benchmark for evaluating to what extent word embeddings of identifiers represent semantic relatedness and similarity. The benchmark is based on thousands of ratings gathered by surveying 500 software developers. We use IdBench to evaluate state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions, as these are often used in current developer tools. Our results show that the effectiveness of embeddings varies significantly across different embedding techniques and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing embedding provides a satisfactory representation of semantic similarities, e.g., because embeddings consider identifiers with opposing meanings as similar, which may lead to fatal mistakes in downstream developer tools. IdBench provides a gold standard to guide the development of novel embeddings that address the current limitations.},
	journal = {arXiv},
	author = {Wainakh, Yaza and Pradel, Michael and Rauf, Moiz},
	year = {2019},
	note = {arXiv: 1910.05177},
	file = {PDF:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/2YGG2CMT/[interpret] IDBENCH.pdf:application/pdf},
}

@article{manning_emergent_2020,
	title = {Emergent linguistic structure in artificial neural networks trained by self-supervision},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1907367117},
	doi = {10.1073/pnas.1907367117},
	abstract = {This paper explores the knowledge of linguistic structure learned by large artificial neural networks, trained via self-supervision, whereby the model simply tries to predict a masked word in a given context. Human language communication is via sequences of words, but language understanding requires constructing rich hierarchical structures that are never observed explicitly. The mechanisms for this have been a prime mystery of human language acquisition, while engineering work has mainly proceeded by supervised learning on treebanks of sentences hand labeled for this latent structure. However, we demonstrate that modern deep contextual language models learn major aspects of this structure, without any explicit supervision. We develop methods for identifying linguistic hierarchical structure emergent in artificial neural networks and demonstrate that components in these models focus on syntactic grammatical relationships and anaphoric coreference. Indeed, we show that a linear transformation of learned embeddings in these models captures parse tree distances to a surprising degree, allowing approximate reconstruction of the sentence tree structures normally assumed by linguists. These results help explain why these models have brought such large improvements across many language-understanding tasks.},
	language = {en},
	number = {48},
	urldate = {2023-02-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Manning, Christopher D. and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
	month = dec,
	year = {2020},
	pages = {30046--30054},
	file = {Manning et al. - 2020 - Emergent linguistic structure in artificial neural.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/XVPZWTA6/Manning et al. - 2020 - Emergent linguistic structure in artificial neural.pdf:application/pdf},
}

@misc{vasconcelos_generation_2023,
	title = {Generation {Probabilities} {Are} {Not} {Enough}: {Exploring} the {Effectiveness} of {Uncertainty} {Highlighting} in {AI}-{Powered} {Code} {Completions}},
	shorttitle = {Generation {Probabilities} {Are} {Not} {Enough}},
	url = {http://arxiv.org/abs/2302.07248},
	abstract = {Large-scale generative models enabled the development of AI-powered code completion tools to assist programmers in writing code. However, much like other AI-powered tools, AI-powered code completions are not always accurate, potentially introducing bugs or even security vulnerabilities into code if not properly detected and corrected by a human programmer. One technique that has been proposed and implemented to help programmers identify potential errors is to highlight uncertain tokens. However, there have been no empirical studies exploring the eï¬ectiveness of this techniqueânor investigating the diï¬erent and not-yet-agreed-upon notions of uncertainty in the context of generative models. We explore the question of whether conveying information about uncertainty enables programmers to more quickly and accurately produce code when collaborating with an AI-powered code completion tool, and if so, what measure of uncertainty best ï¬ts programmersâ needs. Through a mixed-methods study with 30 programmers, we compare three conditions: providing the AI systemâs code completion alone, highlighting tokens with the lowest likelihood of being generated by the underlying generative model, and highlighting tokens with the highest predicted likelihood of being edited by a programmer. We ï¬nd that highlighting tokens with the highest predicted likelihood of being edited leads to faster task completion and more targeted edits, and is subjectively preferred by study participants. In contrast, highlighting tokens according to their probability of being generated does not provide any beneï¬t over the baseline with no highlighting. We further explore the design space of how to convey uncertainty in AI-powered code completion tools, and ï¬nd that programmers prefer highlights that are granular, informative, interpretable, and not overwhelming. This work contributes to building an understanding of what uncertainty means for generative models and how to convey it eï¬ectively.},
	language = {en},
	urldate = {2023-03-20},
	publisher = {arXiv},
	author = {Vasconcelos, Helena and Bansal, Gagan and Fourney, Adam and Liao, Q. Vera and Vaughan, Jennifer Wortman},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07248 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {Vasconcelos et al. - 2023 - Generation Probabilities Are Not Enough Exploring.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/ELXA3EL2/Vasconcelos et al. - 2023 - Generation Probabilities Are Not Enough Exploring.pdf:application/pdf},
}

@misc{xu_-ide_2021,
	title = {In-{IDE} {Code} {Generation} from {Natural} {Language}: {Promise} and {Challenges}},
	shorttitle = {In-{IDE} {Code} {Generation} from {Natural} {Language}},
	url = {http://arxiv.org/abs/2101.11149},
	abstract = {A great part of software development involves conceptualizing or communicating the underlying procedures and logic that needs to be expressed in programs. One major difficulty of programming is turning concept into code, especially when dealing with the APIs of unfamiliar libraries. Recently, there has been a proliferation of machine learning methods for code generation and retrieval from natural language queries, but these have primarily been evaluated purely based on retrieval accuracy or overlap of generated code with developer-written code, and the actual effect of these methods on the developer workflow is surprisingly unattested. We perform the first comprehensive investigation of the promise and challenges of using such technology inside the IDE, asking "at the current state of technology does it improve developer productivity or accuracy, how does it affect the developer experience, and what are the remaining gaps and challenges?" We first develop a plugin for the IDE that implements a hybrid of code generation and code retrieval functionality, and orchestrate virtual environments to enable collection of many user events. We ask developers with various backgrounds to complete 14 Python programming tasks ranging from basic file manipulation to machine learning or data visualization, with or without the help of the plugin. While qualitative surveys of developer experience are largely positive, quantitative results with regards to increased productivity, code quality, or program correctness are inconclusive. Analysis identifies several pain points that could improve the effectiveness of future machine learning based code generation/retrieval developer assistants, and demonstrates when developers prefer code generation over code retrieval and vice versa. We release all data and software to pave the road for future empirical studies and development of better models.},
	language = {en},
	urldate = {2023-03-20},
	publisher = {arXiv},
	author = {Xu, Frank F. and Vasilescu, Bogdan and Neubig, Graham},
	month = sep,
	year = {2021},
	note = {arXiv:2101.11149 [cs]},
	keywords = {Computer Science - Software Engineering},
	annote = {Comment: 47 pages, accepted to ACM Transactions on Software Engineering and Methodology},
	file = {Xu et al. - 2021 - In-IDE Code Generation from Natural Language Prom.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/AKHARIW7/Xu et al. - 2021 - In-IDE Code Generation from Natural Language Prom.pdf:application/pdf},
}

@misc{lopezastprobe2022,
	title = {{AST}-{Probe}: {Recovering} abstract syntax trees from hidden representations of pre-trained language models},
	shorttitle = {{AST}-{Probe}},
	url = {http://arxiv.org/abs/2206.11719},
	abstract = {The objective of pre-trained language models is to learn contextual representations of textual data. Pre-trained language models have become mainstream in natural language processing and code modeling. Using probes, a technique to study the linguistic properties of hidden vector spaces, previous works have shown that these pre-trained language models encode simple linguistic properties in their hidden representations. However, none of the previous work assessed whether these models encode the whole grammatical structure of a programming language. In this paper, we prove the existence of a syntactic subspace, lying in the hidden representations of pre-trained language models, which contain the syntactic information of the programming language. We show that this subspace can be extracted from the modelsâ representations and define a novel probing method, the AST-Probe, that enables recovering the whole abstract syntax tree (AST) of an input code snippet. In our experimentations, we show that this syntactic subspace exists in five state-of-the-art pre-trained language models. In addition, we highlight that the middle layers of the models are the ones that encode most of the AST information. Finally, we estimate the optimal size of this syntactic subspace and show that its dimension is substantially lower than those of the modelsâ representation spaces. This suggests that pre-trained language models use a small part of their representation spaces to encode syntactic information of the programming languages.},
	language = {en},
	urldate = {2023-03-20},
	publisher = {arXiv},
	author = {LÃ³pez, JosÃ© Antonio HernÃ¡ndez and Weyssow, Martin and Cuadrado, JesÃºs SÃ¡nchez and Sahraoui, Houari},
	month = sep,
	year = {2022},
	note = {arXiv:2206.11719 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Programming Languages},
	file = {LÃ³pez et al. - 2022 - AST-Probe Recovering abstract syntax trees from h.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/LUU6JW5I/LÃ³pez et al. - 2022 - AST-Probe Recovering abstract syntax trees from h.pdf:application/pdf},
}

@misc{tree-sitter,
  author       = {Max Brunsfeld and
                  Andrew Hlynskyi and
                  Patrick Thomson and
                  Josh Vera and
                  Phil Turnbull and
                  Timothy Clem and
                  Douglas Creager and
                  Andrew Helwer and
                  Rob Rix and
                  Hendrik van Antwerpen and
                  Michael Davis and
                  Ika and
                  Tuan-Anh Nguyen and
                  Stafford Brunk and
                  Niranjan Hasabnis and
                  bfredl and
                  Mingkai Dong and
                  Matt Massicotte and
                  Jonathan Arnett and
                  Vladimir Panteleev and
                  Steven Kalt and
                  Kolja Lampe and
                  Alex Pinkus and
                  Mark Schmitz and
                  Matthew Krupcale and
                  narpfel and
                  Santos Gallegos and
                  Vicent MartÃ­ and
                  Edgar},
  title        = {tree-sitter/tree-sitter: v0.20.8},
  month        = apr,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.20.8},
  doi          = {10.5281/zenodo.7798573},
  url          = {https://doi.org/10.5281/zenodo.7798573}
}

@misc{liu_reliability_2023,
	title = {On the {Reliability} and {Explainability} of {Automated} {Code} {Generation} {Approaches}},
	url = {http://arxiv.org/abs/2302.09587},
	abstract = {Automatic code generation, the task of generating new code snippets from existing code or comments, has long been of interest. Numerous code generation models have been proposed and proven on different benchmark datasets. However, little is known about whether this objective has been achieved and why code generation models effectively transform code sequences automatically. In other words, can we totally trust these automated code generation models? Consequently, there is a pressing need to understand the inner logic of code generation models and to investigate their replicability, reliability, and explainability. To bridge these research gaps, we conduct a thorough empirical study of five code generation models on four representative code generation datasets to assess the limits and capabilities of automatic code generation approaches. We further employ advanced explainable AI approaches to highlight the tokens that significantly contribute to the code generation. Experiments demonstrate that we successfully replicate state-of-the-art code generation approaches. We discover that state-of-the-art approaches suffer from severe data duplication and input insensitivity, which are subtle issues with significant implications. Our explainability analysis reveals that, in various experimental scenarios, code generation models can recognize code grammar and structural information, but can not capture key tokens that need to be updated. Our results draw several lessons and guidelines for future work in this area. CCS Concepts: â¢ Software and its engineering â Software maintenance tools; â¢ General and reference â Reliability; â¢ Computing methodologies â Natural language processing.},
	language = {en},
	urldate = {2023-03-20},
	publisher = {arXiv},
	author = {Liu, Yue and Tantithamthavorn, Chakkrit and Liu, Yonghui and Li, Li},
	month = feb,
	year = {2023},
	note = {arXiv:2302.09587 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {Liu et al. - 2023 - On the Reliability and Explainability of Automated.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/AWLIFCH8/Liu et al. - 2023 - On the Reliability and Explainability of Automated.pdf:application/pdf},
}

@misc{poesia_synchromesh_2022,
	title = {Synchromesh: {Reliable} code generation from pre-trained language models},
	shorttitle = {Synchromesh},
	url = {http://arxiv.org/abs/2201.11227},
	abstract = {Large pre-trained language models have been used to generate code, providing a ï¬exible interface for synthesizing programs from natural language speciï¬cations. However, they often violate syntactic and semantic rules of their output language, limiting their practical usability. In this paper, we propose SYNCHROMESH: a framework for substantially improving the reliability of pre-trained models for code generation. SYNCHROMESH comprises two components. First, it retrieves few-shot examples from a training bank using Target Similarity Tuning (TST), a novel method for semantic example selection. TST learns to recognize utterances that describe similar target programs despite differences in surface natural language features. Then, SYNCHROMESH feeds the examples to a pre-trained language model and samples programs using Constrained Semantic Decoding (CSD): a general framework for constraining the output to a set of valid programs in the target language. CSD leverages constraints on partial outputs to sample complete correct programs, and needs neither re-training nor ï¬ne-tuning of the language model. We evaluate our methods by synthesizing code from natural language descriptions using GPT-3 and Codex in three real-world languages: SQL queries, Vega-Lite visualizations and SMCalFlow programs. These domains showcase rich constraints that CSD is able to enforce, including syntax, scope, typing rules, and contextual logic. We observe substantial complementary gains from CSD and TST in prediction accuracy and in effectively preventing run-time errors.},
	language = {en},
	urldate = {2023-03-20},
	publisher = {arXiv},
	author = {Poesia, Gabriel and Polozov, Oleksandr and Le, Vu and Tiwari, Ashish and Soares, Gustavo and Meek, Christopher and Gulwani, Sumit},
	month = jan,
	year = {2022},
	note = {arXiv:2201.11227 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
	annote = {Comment: 10 pages, 9 additional pages of Appendix},
	file = {Poesia et al. - 2022 - Synchromesh Reliable code generation from pre-tra.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/EHKBN5RT/Poesia et al. - 2022 - Synchromesh Reliable code generation from pre-tra.pdf:application/pdf},
}

@article{chen_generation_2021,
	title = {Generation {Probabilities} are {Not} {Enough}: {Improving} {Error} {Highlighting} for {AI} {Code} {Suggestions}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2107.03374},
	doi = {10.48550/ARXIV.2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	language = {en},
	urldate = {2023-03-20},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG)},
	annote = {Other
corrected typos, added references, added authors, added acknowledgements},
	file = {Vasconcelos et al. - 2022 - Generation Probabilities are Not EnoughL Improving Error Highligting for AI Code Suggestions:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/6TXPHNV7/Vasconcelos et al. - 2022 - Generation Probabilities are Not EnoughL Improving Error Highligting for AI Code Suggestions.pdf:application/pdf},
}

@misc{wan_what_2022,
	title = {What {Do} {They} {Capture}? -- {A} {Structural} {Analysis} of {Pre}-{Trained} {Language} {Models} for {Source} {Code}},
	shorttitle = {What {Do} {They} {Capture}?},
	url = {http://arxiv.org/abs/2202.06840},
	abstract = {Recently, many pre-trained language models for source code have been proposed to model the context of code and serve as a basis for downstream code intelligence tasks such as code completion, code search, and code summarization. These models leverage masked pre-training and Transformer and have achieved promising results. However, currently there is still little progress regarding interpretability of existing pre-trained code models. It is not clear why these models work and what feature correlations they can capture. In this paper, we conduct a thorough structural analysis aiming to provide an interpretation of pre-trained language models for source code (e.g., CodeBERT, and GraphCodeBERT) from three distinctive perspectives: (1) attention analysis, (2) probing on the word embedding, and (3) syntax tree induction. Through comprehensive analysis, this paper reveals several insightful findings that may inspire future studies: (1) Attention aligns strongly with the syntax structure of code. (2) Pre-training language models of code can preserve the syntax structure of code in the intermediate representations of each Transformer layer. (3) The pre-trained models of code have the ability of inducing syntax trees of code. Theses findings suggest that it may be helpful to incorporate the syntax structure of code into the process of pre-training for better code representations.},
	language = {en},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Wan, Yao and Zhao, Wei and Zhang, Hongyu and Sui, Yulei and Xu, Guandong and Jin, Hai},
	month = feb,
	year = {2022},
	note = {arXiv:2202.06840 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	annote = {Comment: Accepted by ICSE 2022 (The 44th International Conference on Software Engineering)},
	file = {Wan et al. - 2022 - What Do They Capture -- A Structural Analysis of .pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/63Y6VF9X/Wan et al. - 2022 - What Do They Capture -- A Structural Analysis of .pdf:application/pdf},
}

@misc{xu_systematic_2022,
	title = {A {Systematic} {Evaluation} of {Large} {Language} {Models} of {Code}},
	url = {http://arxiv.org/abs/2202.13169},
	abstract = {Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex (Chen et al., 2021)) are not publicly available, leaving many questions about their model and data design decisions. We aim to ï¬ll in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we ï¬nd that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area.},
	language = {en},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Xu, Frank F. and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent J.},
	month = may,
	year = {2022},
	note = {arXiv:2202.13169 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Programming Languages},
	annote = {Comment: DL4C@ICLR 2022, and MAPS@PLDI 2022},
	file = {Xu et al. - 2022 - A Systematic Evaluation of Large Language Models o.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/UMKD4FHG/Xu et al. - 2022 - A Systematic Evaluation of Large Language Models o.pdf:application/pdf},
}

@misc{karmakar_what_2021,
	title = {What do pre-trained code models know about code?},
	url = {http://arxiv.org/abs/2108.11308},
	abstract = {Pre-trained models of code built on the transformer architecture have performed well on software engineering (SE) tasks such as predictive code generation, code summarization, among others. However, whether the vector representations from these pre-trained models comprehensively encode characteristics of source code well enough to be applicable to a broad spectrum of downstream tasks remains an open question.},
	language = {en},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Karmakar, Anjan and Robbes, Romain},
	month = aug,
	year = {2021},
	note = {arXiv:2108.11308 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {Karmakar and Robbes - 2021 - What do pre-trained code models know about code.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/YVHSL9FW/Karmakar and Robbes - 2021 - What do pre-trained code models know about code.pdf:application/pdf},
}

@article{rabin_memorization_2023,
	title = {Memorization and {Generalization} in {Neural} {Code} {Intelligence} {Models}},
	volume = {153},
	issn = {09505849},
	url = {http://arxiv.org/abs/2106.08704},
	doi = {10.1016/j.infsof.2022.107066},
	abstract = {Objective: The goal of this paper is to evaluate and compare the extent of memorization and generalization in neural code intelligence models. It aims to provide insights on how memorization may impact the learning behavior of neural models in code intelligence systems.
Method: To observe the extent of memorization in models, we add random noise to the original training dataset and use various metrics to quantify the impact of noise on various aspects of training and testing. We evaluate several state-of-the-art neural code intelligence models and benchmarks based on Java, Python, and Ruby codebases.
Results: Our results highlight important risks: millions of trainable parameters allow the neural networks to memorize anything, including noisy data, and provide a false sense of generalization. We observed all models manifest some forms of memorization. This can be potentially troublesome in most code intelligence tasks where they rely on rather noise-prone and repetitive data sources, such as code from GitHub.
Conclusion: To the best of our knowledge, we provide the first study to quantify memorization effects in the domain of software engineering and code intelligence systems. This work raises awareness and provides new insights into important issues of training neural models in code intelligence systems that are usually overlooked by software engineering researchers.},
	language = {en},
	urldate = {2023-03-21},
	journal = {Information and Software Technology},
	author = {Rabin, Md Rafiqul Islam and Hussain, Aftab and Alipour, Mohammad Amin and Hellendoorn, Vincent J.},
	month = jan,
	year = {2023},
	note = {arXiv:2106.08704 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Computer Science - Programming Languages},
	pages = {107066},
	annote = {Comment: Information and Software Technology, IST Journal 2022, Elsevier},
	file = {Rabin et al. - 2023 - Memorization and Generalization in Neural Code Int.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/G2HGWEPB/Rabin et al. - 2023 - Memorization and Generalization in Neural Code Int.pdf:application/pdf},
}

@misc{chen_evaluating_2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	abstract = {We introduce Codex, a GPT language model ï¬netuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we ï¬nd that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difï¬cult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difï¬culty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	language = {en},
	urldate = {2023-03-23},
	publisher = {arXiv},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {arXiv:2107.03374 [cs]},
	keywords = {Computer Science - Machine Learning, Processed},
	annote = {Comment: corrected typos, added references, added authors, added acknowledgements},
	file = {Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/D42L4QQE/Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf:application/pdf},
}

@misc{troshin_probing_2022,
	title = {Probing {Pretrained} {Models} of {Source} {Code}},
	url = {http://arxiv.org/abs/2202.08975},
	abstract = {Deep learning models are widely used for solving challenging code processing tasks, such as code generation or code summarization. Traditionally, a speciï¬c model architecture was carefully built to solve a particular code processing task. However, recently general pretrained models such as CodeBERT or CodeT5 have been shown to outperform task-speciï¬c models in many applications. While pretrained models are known to learn complex patterns from data, they may fail to understand some properties of source code. To test diverse aspects of code understanding, we introduce a set of diagnostic probing tasks. We show that pretrained models of code indeed contain information about code syntactic structure, the notions of identiï¬ers, and namespaces, but they may fail to recognize more complex code properties such as semantic equivalence. We also investigate how probing results are affected by using code-speciï¬c pretraining objectives, varying the model size, or ï¬netuning.},
	language = {en},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Troshin, Sergey and Chirkova, Nadezhda},
	month = nov,
	year = {2022},
	note = {arXiv:2202.08975 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {2210.13966.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/GWDK4CM6/2210.13966.pdf:application/pdf;Troshin and Chirkova - 2022 - Probing Pretrained Models of Source Code.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/Q5H79KZA/Troshin and Chirkova - 2022 - Probing Pretrained Models of Source Code.pdf:application/pdf},
}

@misc{karpathy_visualizing_2015,
	title = {Visualizing and {Understanding} {Recurrent} {Networks}},
	url = {http://arxiv.org/abs/1506.02078},
	abstract = {Recurrent Neural Networks (RNNs), and speciï¬cally a variant with Long ShortTerm Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with ï¬nite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
	language = {en},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
	month = nov,
	year = {2015},
	note = {arXiv:1506.02078 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: changing style, adding references, minor changes to text},
	file = {Karpathy et al. - 2015 - Visualizing and Understanding Recurrent Networks.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/4IAPGHHB/Karpathy et al. - 2015 - Visualizing and Understanding Recurrent Networks.pdf:application/pdf},
}

@article{ren_codebleu_2020,
	title = {{CodeBLEU}: a {Method} for {Automatic} {Evaluation} of {Code} {Synthesis}},
	volume = {1949},
	url = {http://arxiv.org/abs/2009.10297},
	abstract = {Evaluation metrics play a vital role in the growth of an area as it defines the standard of distinguishing between good and bad models. In the area of code synthesis, the commonly used evaluation metric is BLEU or perfect accuracy, but they are not suitable enough to evaluate codes, because BLEU is originally designed to evaluate the natural language, neglecting important syntactic and semantic features of codes, and perfect accuracy is too strict thus it underestimates different outputs with the same semantic logic. To remedy this, we introduce a new automatic evaluation metric, dubbed CodeBLEU. It absorbs the strength of BLEU in the n-gram match and further injects code syntax via abstract syntax trees (AST) and code semantics via data-flow. We conduct experiments by evaluating the correlation coefficient between CodeBLEU and quality scores assigned by the programmers on three code synthesis tasks, i.e., text-to-code, code translation, and code refinement. Experimental results show that our proposed CodeBLEU can achieve a better correlation with programmer assigned scores compared with BLEU and accuracy.},
	number = {Weaver 1955},
	author = {Ren, Shuo and Guo, Daya and Lu, Shuai and Zhou, Long and Liu, Shujie and Tang, Duyu and Zhou, Ming and Blanco, Ambrosio and Ma, Shuai},
	year = {2020},
	note = {arXiv: 2009.10297},
	file = {PDF:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/TQVUM6NX/codeBlue.pdf:application/pdf},
}

@misc{karimi_relationship_2023,
	title = {On the {Relationship} {Between} {Explanation} and {Prediction}: {A} {Causal} {View}},
	shorttitle = {On the {Relationship} {Between} {Explanation} and {Prediction}},
	url = {http://arxiv.org/abs/2212.06925},
	abstract = {Being able to provide explanations for a modelâs decision has become a central requirement for the development, deployment, and adoption of machine learning models. However, we are yet to understand what explanation methods can and cannot do. How do upstream factors such as data, model prediction, hyperparameters, and random initialization inï¬uence downstream explanations? While previous work raised concerns that explanations (E) may have little relationship with the prediction (Y ), there is a lack of conclusive study to quantify this relationship. Our work borrows tools from causal inference to systematically assay this relationship. More speciï¬cally, we study the relationship between E and Y by measuring the treatment effect when intervening on their causal ancestors, i.e., on hyperparameters and inputs used to generate saliency-based Es or Y s. Our results suggest that the relationships between E and Y is far from ideal. In fact, the gap between âidealâ case only increase in higher-performing modelsâmodels that are likely to be deployed. Our work is a promising ï¬rst step towards providing a quantitative measure of the relationship between E and Y , which could also inform the future development of methods for E with a quantitative metric.},
	language = {en},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Karimi, Amir-Hossein and Muandet, Krikamol and Kornblith, Simon and SchÃ¶lkopf, Bernhard and Kim, Been},
	month = jan,
	year = {2023},
	note = {arXiv:2212.06925 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {Karimi et al. - 2023 - On the Relationship Between Explanation and Predic.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/XFEF64KX/Karimi et al. - 2023 - On the Relationship Between Explanation and Predic.pdf:application/pdf},
}

@misc{kim_interpretability_2018,
	title = {Interpretability {Beyond} {Feature} {Attribution}: {Quantitative} {Testing} with {Concept} {Activation} {Vectors} ({TCAV})},
	shorttitle = {Interpretability {Beyond} {Feature} {Attribution}},
	url = {http://arxiv.org/abs/1711.11279},
	abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classiï¬ers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural netâs internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-deï¬ned concept is important to a classiï¬cation resultâfor example, how sensitive a prediction of zebra is to the presence of stripes. Using the domain of image classiï¬cation as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classiï¬cation network as well as a medical application.},
	language = {en},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
	month = jun,
	year = {2018},
	note = {arXiv:1711.11279 [stat]},
	keywords = {Statistics - Machine Learning},
	file = {Kim et al. - 2018 - Interpretability Beyond Feature Attribution Quant.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/YYWZ25VJ/Kim et al. - 2018 - Interpretability Beyond Feature Attribution Quant.pdf:application/pdf},
}

@misc{koh_concept_2020,
	title = {Concept {Bottleneck} {Models}},
	url = {http://arxiv.org/abs/2007.04612},
	abstract = {We seek to learn models that we can interact with using high-level concepts: if the model did not think there was a bone spur in the x-ray, would it still predict severe arthritis? State-of-the-art models today do not typically support the manipulation of concepts like âthe existence of bone spursâ, as they are trained end-to-end to go directly from raw input (e.g., pixels) to output (e.g., arthritis severity). We revisit the classic idea of ï¬rst predicting concepts that are provided at training time, and then using these concepts to predict the label. By construction, we can intervene on these concept bottleneck models by editing their predicted concept values and propagating these changes to the ï¬nal prediction. On x-ray grading and bird identiï¬cation, concept bottleneck models achieve competitive accuracy with standard end-to-end models, while enabling interpretation in terms of high-level clinical concepts (âbone spursâ) or bird attributes (âwing colorâ). These models also allow for richer human-model interaction: accuracy improves signiï¬cantly if we can correct model mistakes on concepts at test time.},
	language = {en},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
	month = dec,
	year = {2020},
	note = {arXiv:2007.04612 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Edited for clarity from the ICML 2020 version},
	file = {Koh et al. - 2020 - Concept Bottleneck Models.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/SJGC6CWL/Koh et al. - 2020 - Concept Bottleneck Models.pdf:application/pdf},
}

@book{Pearl2009Causality,
    title = {{Causality: models, reasoning, and inference}},
    year = {2009},
    author = {Pearl, Judea},
    isbn = {978-0-521-89560-0}
}

@book{Pearl2016Causality,
    title = {{Causal Inference in Statistics, A Primer}},
    year = {2016},
    author = {Pearl, Judea and Glymour, Madelyn and P.Jewell, Nicholas},
    isbn = {978-0-521-89560-0}
}

@book{Pearl2018Causality,
    title = {{The book of why: The New Science of Cause and Effect}},
    year = {2018},
    author = {Pearl, Judea and Mackenzie, Dana},
    isbn = {978-0465097609}
}

@article{Sharma2021DoWhyAssumptions,
    title = {{DoWhy : Addressing Challenges in Expressing and Validating Causal Assumptions}},
    year = {2021},
    author = {Sharma, Amit and Syrgkanis, Vasilis and Zhang, Cheng and KÄ±cÄ±man, Emre}
}

@misc{palacio_toward_2023,
	title = {Toward a {Theory} of {Causation} for {Interpreting} {Neural} {Code} {Models}},
	url = {http://arxiv.org/abs/2302.03788},
	abstract = {Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces docode, a post-hoc interpretability methodology speciï¬c to NCMs that is capable of explaining model predictions. docode is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of docode are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact of spurious correlations by grounding explanations of model behavior in properties of programming languages. To demonstrate the practical beneï¬t of docode, we illustrate the insights that our framework can provide by performing a case study on two popular deep learning architectures and nine NCMs. The results of this case study illustrate that our studied NCMs are sensitive to changes in code syntax and statistically learn to predict tokens related to blocks of code (e.g., brackets, parenthesis, semicolon) with less confounding bias as compared to other programming language constructs. These insights demonstrate the potential of docode as a useful model debugging mechanism that may aid in discovering biases and limitations in NCMs.},
	language = {en},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Palacio, David N. and Cooper, Nathan and Rodriguez, Alvaro and Moran, Kevin and Poshyvanyk, Denys},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03788 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering, Statistics - Methodology},
	file = {Palacio et al. - 2023 - Toward a Theory of Causation for Interpreting Neur.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/RK5TSRY2/Palacio et al. - 2023 - Toward a Theory of Causation for Interpreting Neur.pdf:application/pdf},
}

@misc{ghorbani_towards_2019,
	title = {Towards {Automatic} {Concept}-based {Explanations}},
	url = {http://arxiv.org/abs/1902.03129},
	abstract = {Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for concept based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that ACE discovers concepts that are human-meaningful, coherent and important for the neural networkâs predictions.},
	language = {en},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Ghorbani, Amirata and Wexler, James and Zou, James and Kim, Been},
	month = oct,
	year = {2019},
	note = {arXiv:1902.03129 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Concept-base explanations 
},
	file = {Ghorbani et al. - 2019 - Towards Automatic Concept-based Explanations.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/EWAKIZSH/Ghorbani et al. - 2019 - Towards Automatic Concept-based Explanations.pdf:application/pdf},
}

@misc{jiang_how_2021,
	title = {How {Can} {We} {Know} {When} {Language} {Models} {Know}? {On} the {Calibration} of {Language} {Models} for {Question} {Answering}},
	shorttitle = {How {Can} {We} {Know} {When} {Language} {Models} {Know}?},
	url = {http://arxiv.org/abs/2012.00955},
	abstract = {Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question âhow can we know when language models know, with conï¬dence, the answer to a particular query?â We examine this question from the point of view of calibration, the property of a probabilistic modelâs predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models â T5, BART, and GPT-2 âand study whether their probabilities on QA tasks are well calibrated, ï¬nding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their conï¬dence scores correlate better with the likelihood of correctness through ï¬ne-tuning, post-hoc probability modiï¬cation, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github. com/jzbjyb/lm-calibration.},
	language = {en},
	urldate = {2023-03-28},
	publisher = {arXiv},
	author = {Jiang, Zhengbao and Araki, Jun and Ding, Haibo and Neubig, Graham},
	month = may,
	year = {2021},
	note = {arXiv:2012.00955 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: TACL 2021},
	file = {Jiang et al. - 2021 - How Can We Know When Language Models Know On the .pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/8P7MF8GJ/Jiang et al. - 2021 - How Can We Know When Language Models Know On the .pdf:application/pdf},
}

@misc{bansal_does_2021,
	title = {Does the {Whole} {Exceed} its {Parts}? {The} {Effect} of {AI} {Explanations} on {Complementary} {Team} {Performance}},
	shorttitle = {Does the {Whole} {Exceed} its {Parts}?},
	url = {http://arxiv.org/abs/2006.14779},
	abstract = {Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI's recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?},
	language = {en},
	urldate = {2023-03-28},
	publisher = {arXiv},
	author = {Bansal, Gagan and Wu, Tongshuang and Zhou, Joyce and Fok, Raymond and Nushi, Besmira and Kamar, Ece and Ribeiro, Marco Tulio and Weld, Daniel S.},
	month = jan,
	year = {2021},
	note = {arXiv:2006.14779 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	file = {Bansal et al. - 2021 - Does the Whole Exceed its Parts The Effect of AI .pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/S6IGCQJN/Bansal et al. - 2021 - Does the Whole Exceed its Parts The Effect of AI .pdf:application/pdf},
}
@inproceedings{vaithilingam_expectation_2022,
	address = {New Orleans LA USA},
	title = {Expectation vs. {Experience}: {Evaluating} the {Usability} of {Code} {Generation} {Tools} {Powered} by {Large} {Language} {Models}},
	isbn = {978-1-4503-9156-6},
	shorttitle = {Expectation vs. {Experience}},
	url = {https://dl.acm.org/doi/10.1145/3491101.3519665},
	doi = {10.1145/3491101.3519665},
	abstract = {Recent advances in Large Language Models (LLM) have made automatic code generation possible for real-world programming tasks in general-purpose programming languages such as Python. However, there are few human studies on the usability of these tools and how they fit the programming workflow. In this work, we conducted a within-subjects user study with 24 participants to understand how programmers use and perceive Copilot, a LLM-based code generation tool. We found that, while Copilot did not necessarily improve the task completion time or success rate, most participants preferred to use Copilot in daily programming tasks, since Copilot often provided a useful starting point and saved the effort of searching online. However, participants did face difficulties in understanding, editing, and debugging code snippets generated by Copilot, which significantly hindered their task-solving effectiveness. Finally, we highlighted several promising directions for improving the design of Copilot based on our observations and participantsâ feedback.},
	language = {en},
	urldate = {2023-03-29},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems} {Extended} {Abstracts}},
	publisher = {ACM},
	author = {Vaithilingam, Priyan and Zhang, Tianyi and Glassman, Elena L.},
	month = apr,
	year = {2022},
	pages = {1--7},
	file = {Vaithilingam et al. - 2022 - Expectation vs. Experience Evaluating the Usabili.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/YAMNE6YG/Vaithilingam et al. - 2022 - Expectation vs. Experience Evaluating the Usabili.pdf:application/pdf},
}
@misc{barke_grounded_2022,
	title = {Grounded {Copilot}: {How} {Programmers} {Interact} with {Code}-{Generating} {Models}},
	shorttitle = {Grounded {Copilot}},
	url = {http://arxiv.org/abs/2206.15000},
	abstract = {SHRADDHA BARKEâ, UC San Diego, USA MICHAEL B. JAMESâ, UC San Diego, USA NADIA POLIKARPOVA, UC San Diego, USA Powered by recent advances in code-generating models, AI assistants like Github Copilot promise to change the face of programming forever. But what is this new face of programming? We present the first grounded theory analysis of how programmers interact with Copilot, based on observing 20 participantsâwith a range of prior experience using the assistantâas they solve diverse programming tasks across four languages. Our main finding is that interactions with programming assistants are bimodal: in acceleration mode, the programmer knows what to do next and uses Copilot to get there faster; in exploration mode, the programmer is unsure how to proceed and uses Copilot to explore their options. Based on our theory, we provide recommendations for improving the usability of future AI programming assistants.},
	language = {en},
	urldate = {2023-03-29},
	publisher = {arXiv},
	author = {Barke, Shraddha and James, Michael B. and Polikarpova, Nadia},
	month = oct,
	year = {2022},
	note = {arXiv:2206.15000 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Programming Languages},
	file = {Barke et al. - 2022 - Grounded Copilot How Programmers Interact with Co.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/73KSI7U4/Barke et al. - 2022 - Grounded Copilot How Programmers Interact with Co.pdf:application/pdf},
 }

 @misc{guo2021graphcodebert,
      title={GraphCodeBERT: Pre-training Code Representations with Data Flow}, 
      author={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie Liu and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},
      year={2021},
      eprint={2009.08366},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{lu_codexglue_2021,
	title = {{CodeXGLUE}: A Machine Learning Benchmark Dataset for Code Understanding and Generation},
	url = {http://arxiv.org/abs/2102.04664},
	shorttitle = {{CodeXGLUE}},
	abstract = {Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce {CodeXGLUE}, a benchmark dataset to foster machine learning research for program understanding and generation. {CodeXGLUE} includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. {CodeXGLUE} also features three baseline systems, including the {BERT}-style, {GPT}-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems 1.},
	number = {{arXiv}:2102.04664},
	publisher = {{arXiv}},
	author = {Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and Li, Ge and Zhou, Lidong and Shou, Linjun and Zhou, Long and Tufano, Michele and Gong, Ming and Zhou, Ming and Duan, Nan and Sundaresan, Neel and Deng, Shao Kun and Fu, Shengyu and Liu, Shujie},
	urldate = {2023-04-14},
	date = {2021-03-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2102.04664 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {Lu et al. - 2021 - CodeXGLUE A Machine Learning Benchmark Dataset fo.pdf:/Users/danielrcardenas/Zotero/storage/PZD7Z7HR/Lu et al. - 2021 - CodeXGLUE A Machine Learning Benchmark Dataset fo.pdf:application/pdf},
}

@misc{ahmadunified2021,
	title = {Unified Pre-training for Program Understanding and Generation},
	url = {http://arxiv.org/abs/2103.06333},
	abstract = {Code summarization and generation empower conversion between programming language ({PL}) and natural language ({NL}), while code translation avails the migration of legacy code from one {PL} to another. This paper introduces {PLBART}, a sequence-to-sequence model capable of performing a broad spectrum of program and language understanding and generation tasks. {PLBART} is pre-trained on an extensive collection of Java and Python functions and associated {NL} text via denoising autoencoding. Experiments on code summarization in the English language, code generation, and code translation in seven programming languages show that {PLBART} outperforms or rivals state-of-the-art models. Moreover, experiments on discriminative tasks, e.g., program repair, clone detection, and vulnerable code detection, demonstrate {PLBART}âs effectiveness in program understanding. Furthermore, analysis reveals that {PLBART} learns program syntax, style (e.g., identiï¬er naming convention), logical ï¬ow (e.g., if block inside an else block is equivalent to else if block) that are crucial to program semantics and thus excels even with limited annotations.},
	number = {{arXiv}:2103.06333},
	publisher = {{arXiv}},
	author = {Ahmad, Wasi Uddin and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},
	urldate = {2023-04-14},
	date = {2021-04-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2103.06333 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Programming Languages},
	file = {Ahmad et al. - 2021 - Unified Pre-training for Program Understanding and.pdf:/Users/danielrcardenas/Zotero/storage/54RNBSM8/Ahmad et al. - 2021 - Unified Pre-training for Program Understanding and.pdf:application/pdf},
}

@misc{wang_codet5_2021,
	title = {{CodeT}5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
	url = {http://arxiv.org/abs/2109.00859},
	shorttitle = {{CodeT}5},
	abstract = {Pre-trained models for Natural Languages ({NL}) like {BERT} and {GPT} have been recently shown to transfer well to Programming Languages ({PL}) and largely beneï¬t a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as {NL}, neglecting the special characteristics of {PL} such as token types. We present {CodeT}5, a uniï¬ed pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identiï¬ers. Our model employs a uniï¬ed framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identiï¬er-aware pre-training task that enables the model to distinguish which code tokens are identiï¬ers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better {NL}-{PL} alignment. Comprehensive experiments show that {CodeT}5 signiï¬cantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including {PL}-{NL}, {NL}-{PL}, and {PL}-{PL}. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https: //github.com/salesforce/{CodeT}5.},
	number = {{arXiv}:2109.00859},
	publisher = {{arXiv}},
	author = {Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven C. H.},
	urldate = {2023-04-14},
	date = {2021-09-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2109.00859 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Programming Languages},
	file = {Wang et al. - 2021 - CodeT5 Identifier-aware Unified Pre-trained Encod.pdf:/Users/danielrcardenas/Zotero/storage/I3K2SABB/Wang et al. - 2021 - CodeT5 Identifier-aware Unified Pre-trained Encod.pdf:application/pdf},
}

@misc{lewis_bart_2019,
	title = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
	url = {http://arxiv.org/abs/1910.13461},
	shorttitle = {{BART}},
	abstract = {We present {BART}, a denoising autoencoder for pretraining sequence-to-sequence models. {BART} is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing {BERT} (due to the bidirectional encoder), {GPT} (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, ï¬nding the best performance by both randomly shufï¬ing the order of the original sentences and using a novel in-ï¬lling scheme, where spans of text are replaced with a single mask token. {BART} is particularly effective when ï¬ne tuned for text generation but also works well for comprehension tasks. It matches the performance of {RoBERTa} with comparable training resources on {GLUE} and {SQuAD}, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 {ROUGE}. {BART} also provides a 1.1 {BLEU} increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the {BART} framework, to better measure which factors most inï¬uence end-task performance.},
	number = {{arXiv}:1910.13461},
	publisher = {{arXiv}},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	urldate = {2023-04-14},
	date = {2019-10-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1910.13461 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf:/Users/danielrcardenas/Zotero/storage/N9LJA4ML/Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf:application/pdf},
}

@misc{guo_graphcodebert_2021,
	title = {{GraphCodeBERT}: Pre-training Code Representations with Data Flow},
	url = {http://arxiv.org/abs/2009.08366},
	shorttitle = {{GraphCodeBERT}},
	abstract = {Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present {GraphCodeBERT}, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree ({AST}), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of "where-the-value-comes-from" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of {AST}, the property of which makes the model more efficient. We develop {GraphCodeBERT} based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve {GraphCodeBERT} and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.},
	number = {{arXiv}:2009.08366},
	publisher = {{arXiv}},
	author = {Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and Tufano, Michele and Deng, Shao Kun and Clement, Colin and Drain, Dawn and Sundaresan, Neel and Yin, Jian and Jiang, Daxin and Zhou, Ming},
	urldate = {2023-04-14},
	date = {2021-09-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2009.08366 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {Guo et al. - 2021 - GraphCodeBERT Pre-training Code Representations w.pdf:/Users/danielrcardenas/Zotero/storage/ZEWM6Y9L/Guo et al. - 2021 - GraphCodeBERT Pre-training Code Representations w.pdf:application/pdf},
}

@article{lin_span_2022,
	title = {Xcode: Towards Cross-Language Code Representation with Large-Scale Pre-Training},
	volume = {31},
	issn = {1049-331X, 1557-7392},
	url = {https://dl.acm.org/doi/10.1145/3506696},
	doi = {10.1145/3506696},
	shorttitle = {{\textless}span style="font-variant},
	abstract = {Source code representation learning is the basis of applying artificial intelligence to many software engineering tasks such as code clone detection, algorithm classification, and code summarization. Recently, many works have tried to improve the performance of source code representation from various perspectives, e.g., introducing the structural information of programs into latent representation. However, when dealing with rapidly expanded unlabeled cross-language source code datasets from the Internet, there are still two issues. Firstly, deep learning models for many code-specific tasks still suffer from the lack of high-quality labels. Secondly, the structural differences among programming languages make it more difficult to process multiple languages in a single neural architecture.
            
              To address these issues, in this article, we propose a novel
              Cross
              -language
              Code
              representation with a large-scale pre-training (
              {XCode}
              ) method. Concretely, we propose to use several abstract syntax trees and {ELMo}-enhanced variational autoencoders to obtain multiple pre-trained source code language models trained on about 1.5 million code snippets. To fully utilize the knowledge across programming languages, we further propose a Shared Encoder-Decoder ({SED}) architecture which uses the multi-teacher single-student method to transfer knowledge from the aforementioned pre-trained models to the distilled {SED}. The pre-trained models and {SED} will cooperate to better represent the source code. For evaluation, we examine our approach on three typical downstream cross-language tasks, i.e., source code translation, code clone detection, and code-to-code search, on a real-world dataset composed of programming exercises with multiple solutions. Experimental results demonstrate the effectiveness of our proposed approach on cross-language code representations. Meanwhile, our approach performs significantly better than several code representation baselines on different downstream tasks in terms of multiple automatic evaluation metrics.},
	pages = {1--44},
	number = {3},
	journaltitle = {{ACM} Transactions on Software Engineering and Methodology},
	shortjournal = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Lin, Zehao and Li, Guodun and Zhang, Jingfeng and Deng, Yue and Zeng, Xiangji and Zhang, Yin and Wan, Yao},
	urldate = {2023-04-14},
	date = {2022-07-31},
	langid = {english},
	file = {Lin et al. - 2022 - XCodespan.pdf:/Users/danielrcardenas/Zotero/storage/VMZVJWFE/Lin et al. - 2022 - XCodespan.pdf:application/pdf},
}

@misc{kanade_learning_2020,
	title = {Learning and Evaluating Contextual Embedding of Source Code},
	url = {http://arxiv.org/abs/2001.00059},
	abstract = {Recent research has achieved impressive results on understanding and improving source code by building up on machine-learning techniques developed for natural languages. A signiï¬cant advancement in natural-language understanding has come with the development of pre-trained contextual embeddings, such as {BERT}, which can be ï¬ne-tuned for downstream tasks with less labeled data and training budget, while achieving better accuracies. However, there is no attempt yet to obtain a high-quality contextual embedding of source code, and to evaluate it on multiple program-understanding tasks simultaneously; that is the gap that this paper aims to mitigate. Speciï¬cally, ï¬rst, we curate a massive, deduplicated corpus of 7.4M Python ï¬les from {GitHub}, which we use to pre-train {CuBERT}, an open-sourced codeunderstanding {BERT} model; and, second, we create an open-sourced benchmark that comprises ï¬ve classiï¬cation tasks and one program-repair task, akin to code-understanding tasks proposed in the literature before. We ï¬ne-tune {CuBERT} on our benchmark tasks, and compare the resulting models to different variants of Word2Vec token embeddings, {BiLSTM} and Transformer models, as well as published state-of-the-art models, showing that {CuBERT} outperforms them all, even with shorter training, and with fewer labeled examples. Future work on source-code embedding can beneï¬t from reusing our benchmark, and from comparing against {CuBERT} models as a strong baseline.},
	number = {{arXiv}:2001.00059},
	publisher = {{arXiv}},
	author = {Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
	urldate = {2023-04-14},
	date = {2020-08-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2001.00059 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Software Engineering},
	file = {Kanade et al. - 2020 - Learning and Evaluating Contextual Embedding of So.pdf:/Users/danielrcardenas/Zotero/storage/JCMU9IU3/Kanade et al. - 2020 - Learning and Evaluating Contextual Embedding of So.pdf:application/pdf},
}

@misc{feng_codebert_2020,
	title = {{CodeBERT}: A Pre-Trained Model for Programming and Natural Languages},
	url = {http://arxiv.org/abs/2002.08155},
	shorttitle = {{CodeBERT}},
	abstract = {We present {CodeBERT}, a bimodal pre-trained model for programming language ({PL}) and nat-ural language ({NL}). {CodeBERT} learns general-purpose representations that support downstream {NL}-{PL} applications such as natural language codesearch, code documentation generation, etc. We develop {CodeBERT} with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both bimodal data of {NL}-{PL} pairs and unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate {CodeBERT} on two {NL}-{PL} applications by fine-tuning model parameters. Results show that {CodeBERT} achieves state-of-the-art performance on both natural language code search and code documentation generation tasks. Furthermore, to investigate what type of knowledge is learned in {CodeBERT}, we construct a dataset for {NL}-{PL} probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that {CodeBERT} performs better than previous pre-trained models on {NL}-{PL} probing.},
	number = {{arXiv}:2002.08155},
	publisher = {{arXiv}},
	author = {Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and Zhou, Ming},
	urldate = {2023-04-14},
	date = {2020-09-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2002.08155 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Programming Languages},
	file = {Feng et al. - 2020 - CodeBERT A Pre-Trained Model for Programming and .pdf:/Users/danielrcardenas/Zotero/storage/R527ET4P/Feng et al. - 2020 - CodeBERT A Pre-Trained Model for Programming and .pdf:application/pdf},
}

@misc{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), {BERT} is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be ï¬netuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciï¬c architecture modiï¬cations.},
	number = {{arXiv}:1810.04805},
	publisher = {{arXiv}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2023-04-14},
	date = {2019-05-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:/Users/danielrcardenas/Zotero/storage/D98UUBKG/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@misc{ghostwriter,
  author = {Replit},
  title = {{Replit Ghostwriter}},
  howpublished = {\url{https://replit.com/site/ghostwriter}},
  year = 2023,
  month = May
}

@misc{gptj,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@misc{GPTNeoX,
  doi = {10.48550/ARXIV.2204.06745},
  
  url = {https://arxiv.org/abs/2204.06745},
  
  author = {Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {GPT-NeoX-20B: An Open-Source Autoregressive Language Model},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{zhao_survey_2023,
	title = {A {Survey} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2303.18223},
	abstract = {Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a signiï¬cant challenge to develop capable artiï¬cial intelligence (AI) algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pretraining Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing (NLP) tasks. Since the researchers have found that model scaling can lead to an improved model capacity, they further investigate the scaling effect by increasing the parameter scale to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a signiï¬cant performance improvement, but also exhibit some special abilities (e.g., incontext learning) that are not present in small-scale language models (e.g., BERT). To discriminate the language models in different parameter scales, the research community has coined the term large language models (LLM) for the PLMs of signiï¬cant size (e.g., containing tens or hundreds of billions of parameters). Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. Considering this rapid technical progress, in this survey, we review the recent advances of LLMs by introducing the background, key ï¬ndings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions. This survey provides an up-to-date review of the literature on LLMs, which can be a useful resource for both researchers and engineers.},
	language = {en},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	month = apr,
	year = {2023},
	note = {arXiv:2303.18223 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Zhao et al. - 2023 - A Survey of Large Language Models.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/FJS3LPBM/Zhao et al. - 2023 - A Survey of Large Language Models.pdf:application/pdf},
}
@misc{black_sid_2021_5297715,
  author       = {Black, Sid and
                  Leo, Gao and
                  Wang, Phil and
                  Leahy, Connor and
                  Biderman, Stella},
  title        = {{GPT-Neo: Large Scale Autoregressive Language 
                   Modeling with Mesh-Tensorflow}},
  month        = mar,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.5297715},
  url          = {https://doi.org/10.5281/zenodo.5297715}
}

@misc{gholizadeh_model_2021,
	title = {Model {Explainability} in {Deep} {Learning} {Based} {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2106.07410},
	abstract = {Machine learning (ML) model explainability has received growing attention, especially in the area related to model risk and regulations. In this paper, we reviewed and compared some popular ML model explainability methodologies, especially those related to Natural Language Processing (NLP) models. We then applied one of the NLP explainability methods Layer-wise Relevance Propagation (LRP) to a NLP classiï¬cation model. We used the LRP method to derive a relevance score for each word in an instance, which is a local explainability. The relevance scores are then aggregated together to achieve global variable importance of the model. Through the case study, we also demonstrated how to apply the local explainability method to false positive and false negative instances to discover the weakness of a NLP model. These analysis can help us to understand NLP models better and reduce the risk due to the black-box nature of NLP models. We also identiï¬ed some common issues due to the special natures of NLP models and discussed how explainability analysis can act as a control to detect these issues after the model has been trained.},
	language = {en},
	urldate = {2023-02-01},
	publisher = {arXiv},
	author = {Gholizadeh, Shafie and Zhou, Nengfeng},
	month = jun,
	year = {2021},
	note = {arXiv:2106.07410 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Gholizadeh and Zhou - 2021 - Model Explainability in Deep Learning Based Natura.pdf:/Users/daniel/Library/CloudStorage/GoogleDrive-dhrodriguezc@unal.edu.co/My Drive/zotero/storage/3PBGKQEN/Gholizadeh and Zhou - 2021 - Model Explainability in Deep Learning Based Natura.pdf:application/pdf},
}
@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{codeparrot,
  title={Codeparrot},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  url={https://github.com/huggingface/blog/blob/main/codeparrot.md},
  year={2021}
}
@article{Nijkamp2022ACP,
  title={A Conversational Paradigm for Program Synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint},
  year={2022}
}
@misc{austin2021program,
      title={Program Synthesis with Large Language Models}, 
      author={Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie Cai and Michael Terry and Quoc Le and Charles Sutton},
      year={2021},
      eprint={2108.07732},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}

@book{10.5555/1196416,
author = {Hopcroft, John E. and Motwani, Rajeev and Ullman, Jeffrey D.},
title = {Introduction to Automata Theory, Languages, and Computation (3rd Edition)},
year = {2006},
isbn = {0321455363},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA}
}

@misc{gao2020pile,
      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2020},
      eprint={2101.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bigquery,
      title={Bigquery dataset}, 
      url={https:// cloud.google.com/bigquery}
}

@misc{nijkamp2023codegen,
      title={CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis}, 
      author={Erik Nijkamp and Bo Pang and Hiroaki Hayashi and Lifu Tu and Huan Wang and Yingbo Zhou and Silvio Savarese and Caiming Xiong},
      year={2023},
      eprint={2203.13474},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{palacio2023theory,
      title={Toward a Theory of Causation for Interpreting Neural Code Models}, 
      author={David N. Palacio and Nathan Cooper and Alvaro Rodriguez and Kevin Moran and Denys Poshyvanyk},
      year={2023},
      eprint={2302.03788},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
