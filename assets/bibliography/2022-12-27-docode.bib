@article{MSR-Completion,
  author    = {Matteo Ciniselli and
               Nathan Cooper and
               Luca Pascarella and
               Denys Poshyvanyk and
               Massimiliano Di Penta and
               Gabriele Bavota},
  title     = {An Empirical Study on the Usage of {BERT} Models for Code Completion},
  journal   = {CoRR},
  volume    = {abs/2103.07115},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.07115},
  eprinttype = {arXiv},
  eprint    = {2103.07115},
  timestamp = {Tue, 23 Mar 2021 16:29:47 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-07115.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{pillutla2021mauve,
  title={Mauve: Measuring the gap between neural text and human text using divergence frontiers},
  author={Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{zhang2021dive,
    title={Dive into Deep Learning},
    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
    journal={arXiv preprint arXiv:2106.11342},
    year={2021}
}

@article{Hendrycks2021apps,
  author    = {Dan Hendrycks and
               Steven Basart and
               Saurav Kadavath and
               Mantas Mazeika and
               Akul Arora and
               Ethan Guo and
               Collin Burns and
               Samir Puranik and
               Horace He and
               Dawn Song and
               Jacob Steinhardt},
  title     = {Measuring Coding Challenge Competence With {APPS}},
  journal   = {CoRR},
  volume    = {abs/2105.09938},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.09938},
  archivePrefix = {arXiv},
  eprint    = {2105.09938},
  timestamp = {Mon, 31 May 2021 16:16:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-09938.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{icodegen,
  author = {Anonymous},
  title = {InterpretingCodeGeneration},
  year = {2022},
  publisher = {Anonymous 4 Open Science},
  journal = {Anonymous 4 Open Science},
  howpublished = {\url{https://anonymous.4open.science/r/InterpretingCodeGeneration-75E4/}}
}

@inproceedings{wolf2020transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}


@inproceedings{sennrich2016bpe,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

@article{Kingma2015AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6980}
}

@misc{github,
  author={github},
  title={GitHub},
  year={2020},
  url={https://github.com/},
}

@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@inproceedings{Cho2014GRU,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}

@article{Chung2014EmpiricalGRU,
  author    = {Junyoung Chung and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               KyungHyun Cho and
               Yoshua Bengio},
  title     = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
               Modeling},
  journal   = {CoRR},
  volume    = {abs/1412.3555},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.3555},
  archivePrefix = {arXiv},
  eprint    = {1412.3555},
  timestamp = {Mon, 13 Aug 2018 16:47:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChungGCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018}
}

@inproceedings{Nguyen:ICSE15, author = {Nguyen, Anh Tuan and Nguyen, Tien N.}, title = {Graph-Based Statistical Language Model for Code}, year = {2015}, isbn = {9781479919345}, publisher = {IEEE Press}, abstract = {n-gram statistical language model has been successfully applied to capture programming patterns to support code completion and suggestion. However, the approaches using n-gram face challenges in capturing the patterns at higher levels of abstraction due to the mismatch between the sequence nature in n-grams and the structure nature of syntax and semantics in source code. This paper presents GraLan, a graph-based statistical language model and its application in code suggestion. GraLan can learn from a source code corpus and compute the appearance probabilities of any graphs given the observed (sub)graphs. We use GraLan to develop an API suggestion engine and an AST-based language model, ASTLan. ASTLan supports the suggestion of the next valid syntactic template and the detection of common syntactic templates. Our empirical evaluation on a large corpus of open-source projects has shown that our engine is more accurate in API code suggestion than the state-of-the-art approaches, and in 75% of the cases, it can correctly suggest the API with only five candidates. ASTLan also has high accuracy in suggesting the next syntactic template and is able to detect many useful and common syntactic templates.}, booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1}, pages = {858–868}, numpages = {11}, location = {Florence, Italy}, series = {ICSE '15} }

@inproceedings{Allamanis19,
  author={Miltiadis Allamanis},
  title={The adverse effects of code duplication in machine learning models of code},
  year={2019},
  cdate={1546300800000},
  pages={143-153},
  url={https://doi.org/10.1145/3359591.3359735},
  booktitle={Onward! OOPLSA 2019},
}

@MISC {emp-standards,
	title        = {ACM SIGSOFT Empirical SE Standards},
	howpublished = {\url{https://acmsigsoft.github.io/EmpiricalStandards/tools/}},
	year         = {2021},
}

@inproceedings{Baishakhi2016buggy,
    author = {Ray, Baishakhi and Hellendoorn, Vincent and Godhane, Saheel and Tu, Zhaopeng and Bacchelli, Alberto and Devanbu, Premkumar},
    title = {On the "Naturalness" of Buggy Code},
    year = {2016},
    isbn = {9781450339001},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2884781.2884848},
    doi = {10.1145/2884781.2884848},
    abstract = {Real software, the kind working programmers produce by the kLOC to solve real-world
    problems, tends to be "natural", like speech or natural language; it tends to be highly
    repetitive and predictable. Researchers have captured this naturalness of software
    through statistical models and used them to good effect in suggestion engines, porting
    tools, coding standards checkers, and idiom miners. This suggests that code that appears
    improbable, or surprising, to a good statistical language model is "unnatural" in
    some sense, and thus possibly suspicious. In this paper, we investigate this hypothesis.
    We consider a large corpus of bug fix commits (ca. 7,139), from 10 different Java
    projects, and focus on its language statistics, evaluating the naturalness of buggy
    code and the corresponding fixes. We find that code with bugs tends to be more entropic
    (i.e. unnatural), becoming less so as bugs are fixed. Ordering files for inspection
    by their average entropy yields cost-effectiveness scores comparable to popular defect
    prediction methods. At a finer granularity, focusing on highly entropic lines is similar
    in cost-effectiveness to some well-known static bug finders (PMD, FindBugs) and ordering
    warnings from these bug finders using an entropy measure improves the cost-effectiveness
    of inspecting code implicated in warnings. This suggests that entropy may be a valid,
    simple way to complement the effectiveness of PMD or FindBugs, and that search-based
    bug-fixing methods may benefit from using entropy both for fault-localization and
    searching for fixes.},
    booktitle = {Proceedings of the 38th International Conference on Software Engineering},
    pages = {428–439},
    numpages = {12},
    location = {Austin, Texas},
    series = {ICSE '16}
}

@inproceedings{
    allamanis2018learning,
    title={Learning to Represent Programs with Graphs},
    author={Miltiadis Allamanis and Marc Brockschmidt and Mahmoud Khademi},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=BJOFETxR-},
}

@article{RNNs,
    author = {Hochreiter, Sepp and Schmidhuber, Jurgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@inproceedings{Watson:ICSE20, author = {Watson, Cody and Tufano, Michele and Moran, Kevin and Bavota, Gabriele and Poshyvanyk, Denys}, title = {On Learning Meaningful Assert Statements for Unit Test Cases}, year = {2020}, isbn = {9781450371216}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3377811.3380429}, doi = {10.1145/3377811.3380429}, booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering}, pages = {1398–1409}, numpages = {12}, location = {Seoul, South Korea}, series = {ICSE '20} }

@ARTICLE{LeGous:2012,
  author={Le Goues, Claire and Nguyen, ThanhVu and Forrest, Stephanie and Weimer, Westley},
  journal={IEEE Transactions on Software Engineering}, 
  title={GenProg: A Generic Method for Automatic Software Repair}, 
  year={2012},
  volume={38},
  number={1},
  pages={54-72},
  doi={10.1109/TSE.2011.104}}


@article{Hussain2020DeepTL,
  title={Deep Transfer Learning for Source Code Modeling},
  author={Yasir Hussain and Zhiqiu Huang and Yu Zhou and Senzhang Wang},
  journal={Int. J. Softw. Eng. Knowl. Eng.},
  year={2020},
  volume={30},
  pages={649-668}
}

@inproceedings{mahmud2021cmterrs,
    title = "Code to Comment Translation: A Comparative Study on Model Effectiveness {\&} Errors",
    author = "Mahmud, Junayed  and
      Faisal, Fahim  and
      Arnob, Raihan Islam  and
      Anastasopoulos, Antonios  and
      Moran, Kevin",
    booktitle = "Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.nlp4prog-1.1",
    doi = "10.18653/v1/2021.nlp4prog-1.1",
    pages = "1--16",
    abstract = "Automated source code summarization is a popular software engineering research topic wherein machine translation models are employed to {``}translate{''} code snippets into relevant natural language descriptions. Most evaluations of such models are conducted using automatic reference-based metrics. However, given the relatively large semantic gap between programming languages and natural language, we argue that this line of research would benefit from a qualitative investigation into the various error modes of current state-of-the-art models. Therefore, in this work, we perform both a quantitative and qualitative comparison of three recently proposed source code summarization models. In our quantitative evaluation, we compare the models based on the smoothed BLEU-4, METEOR, and ROUGE-L machine translation metrics, and in our qualitative evaluation, we perform a manual open-coding of the most common errors committed by the models when compared to ground truth captions. Our investigation reveals new insights into the relationship between metric-based performance and model prediction errors grounded in an error taxonomy that can be used to drive future research efforts.",
}

@article{Wang2019coset,
  author    = {Ke Wang and
               Mihai Christodorescu},
  title     = {{COSET:} {A} Benchmark for Evaluating Neural Program Embeddings},
  journal   = {CoRR},
  volume    = {abs/1905.11445},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.11445},
  archivePrefix = {arXiv},
  eprint    = {1905.11445},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-11445.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Wang2021sum,
  author    = {Yu Wang and
               Fengjuan Gao and
               Linzhang Wang},
  title     = {Demystifying Code Summarization Models},
  journal   = {CoRR},
  volume    = {abs/2102.04625},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.04625},
  archivePrefix = {arXiv},
  eprint    = {2102.04625},
  timestamp = {Tue, 13 Apr 2021 13:32:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-04625.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Ren2020codebleu,
  author    = {Shuo Ren and
               Daya Guo and
               Shuai Lu and
               Long Zhou and
               Shujie Liu and
               Duyu Tang and
               Neel Sundaresan and
               Ming Zhou and
               Ambrosio Blanco and
               Shuai Ma},
  title     = {CodeBLEU: a Method for Automatic Evaluation of Code Synthesis},
  journal   = {CoRR},
  volume    = {abs/2009.10297},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.10297},
  archivePrefix = {arXiv},
  eprint    = {2009.10297},
  timestamp = {Wed, 30 Sep 2020 08:21:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-10297.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Peng2021understand,
  title = 	 {How could Neural Networks understand Programs?},
  author =       {Peng, Dinglan and Zheng, Shuxin and Li, Yatao and Ke, Guolin and He, Di and Liu, Tie-Yan},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8476--8486},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/peng21b/peng21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/peng21b.html},
  abstract = 	 {Semantic understanding of programs is a fundamental problem for programming language processing (PLP). Recent works that learn representations of code based on pre-training techniques in NLP have pushed the frontiers in this direction. However, the semantics of PL and NL have essential differences. These being ignored, we believe it is difficult to build a model to better understand programs, by either directly applying off-the-shelf NLP pre-training techniques to the source code, or adding features to the model by the heuristic. In fact, the semantics of a program can be rigorously defined by formal semantics in PL theory. For example, the operational semantics, describes the meaning of a valid program as updating the environment (i.e., the memory address-value function) through fundamental operations, such as memory I/O and conditional branching. Inspired by this, we propose a novel program semantics learning paradigm, that the model should learn from information composed of (1) the representations which align well with the fundamental operations in operational semantics, and (2) the information of environment transition, which is indispensable for program understanding. To validate our proposal, we present a hierarchical Transformer-based pre-training model called OSCAR to better facilitate the understanding of programs. OSCAR learns from intermediate representation (IR) and an encoded representation derived from static analysis, which are used for representing the fundamental operations and approximating the environment transitions respectively. OSCAR empirically shows the outstanding capability of program semantics understanding on many practical software engineering tasks. Code and models are released at: \url{https://github.com/pdlan/OSCAR}.}
}


@inproceedings{ahmad2021unified,
    title = "Unified Pre-training for Program Understanding and Generation",
    author = "Ahmad, Wasi  and
      Chakraborty, Saikat  and
      Ray, Baishakhi  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.211",
    doi = "10.18653/v1/2021.naacl-main.211",
    pages = "2655--2668",
    abstract = "Code summarization and generation empower conversion between programming language (PL) and natural language (NL), while code translation avails the migration of legacy code from one PL to another. This paper introduces PLBART, a sequence-to-sequence model capable of performing a broad spectrum of program and language understanding and generation tasks. PLBART is pre-trained on an extensive collection of Java and Python functions and associated NL text via denoising autoencoding. Experiments on code summarization in the English language, code generation, and code translation in seven programming languages show that PLBART outperforms or rivals state-of-the-art models. Moreover, experiments on discriminative tasks, e.g., program repair, clone detection, and vulnerable code detection, demonstrate PLBART{'}s effectiveness in program understanding. Furthermore, analysis reveals that PLBART learns program syntax, style (e.g., identifier naming convention), logical flow (e.g., {``}if{``} block inside an {``}else{``} block is equivalent to {``}else if{``} block) that are crucial to program semantics and thus excels even with limited annotations.",
}

@inproceedings{
    guo2021graphcodebert,
    title={GraphCode{\{}BERT{\}}: Pre-training Code Representations with Data Flow},
    author={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie LIU and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=jLoC4ez43PZ}
}

@inproceedings{feng2020codebert,
    title = "{C}ode{BERT}: A Pre-Trained Model for Programming and Natural Languages",
    author = "Feng, Zhangyin  and
      Guo, Daya  and
      Tang, Duyu  and
      Duan, Nan  and
      Feng, Xiaocheng  and
      Gong, Ming  and
      Shou, Linjun  and
      Qin, Bing  and
      Liu, Ting  and
      Jiang, Daxin  and
      Zhou, Ming",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.139",
    doi = "10.18653/v1/2020.findings-emnlp.139",
    pages = "1536--1547",
    abstract = "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both {``}bimodal{''} data of NL-PL pairs and {``}unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.",
}

@INPROCEEDINGS{White2016clones,  author={White, Martin and Tufano, Michele and Vendome, Christopher and Poshyvanyk, Denys},  booktitle={2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE)},   title={Deep learning code fragments for code clone detection},   year={2016},  volume={},  number={},  pages={87-98},  doi={}}

@inproceedings{Hu2018comment,
    author = {Hu, Xing and Li, Ge and Xia, Xin and Lo, David and Jin, Zhi},
    title = {Deep Code Comment Generation},
    year = {2018},
    isbn = {9781450357142},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi-org.proxy.wm.edu/10.1145/3196321.3196334},
    doi = {10.1145/3196321.3196334},
    abstract = {During software maintenance, code comments help developers comprehend programs and
    reduce additional time spent on reading and navigating source code. Unfortunately,
    these comments are often mismatched, missing or outdated in the software projects.
    Developers have to infer the functionality from the source code. This paper proposes
    a new approach named DeepCom to automatically generate code comments for Java methods.
    The generated comments aim to help developers understand the functionality of Java
    methods. DeepCom applies Natural Language Processing (NLP) techniques to learn from
    a large code corpus and generates comments from learned features. We use a deep neural
    network that analyzes structural information of Java methods for better comments generation.
    We conduct experiments on a large-scale Java corpus built from 9,714 open source projects
    from GitHub. We evaluate the experimental results on a machine translation metric.
    Experimental results demonstrate that our method DeepCom outperforms the state-of-the-art
    by a substantial margin.},
    booktitle = {Proceedings of the 26th Conference on Program Comprehension},
    pages = {200–210},
    numpages = {11},
    keywords = {comment generation, deep learning, program comprehension},
    location = {Gothenburg, Sweden},
    series = {ICPC '18}
}

@inproceedings{Roziere2020transcoder,
 author = {Roziere, Baptiste and Lachaux, Marie-Anne and Chanussot, Lowik and Lample, Guillaume},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {20601--20611},
 publisher = {Curran Associates, Inc.},
 title = {Unsupervised Translation of Programming Languages},
 url = {https://proceedings.neurips.cc/paper/2020/file/ed23fbf18c2cd35f8c7f8de44f85c08d-Paper.pdf},
 volume = {33},
 year = {2020}
}


@ARTICLE{Chen2019sequencer,  author={Chen, Zimin and Kommrusch, Steve James and Tufano, Michele and Pouchet, Louis-Noël and Poshyvanyk, Denys and Monperrus, Martin},  journal={IEEE Transactions on Software Engineering},   title={SEQUENCER: Sequence-to-Sequence Learning for End-to-End Program Repair},   year={2019},  volume={},  number={},  pages={1-1},  doi={10.1109/TSE.2019.2940179}}

@inproceedings{tu2014local,
    author = {Tu, Zhaopeng and Su, Zhendong and Devanbu, Premkumar},
    title = {On the Localness of Software},
    year = {2014},
    isbn = {9781450330565},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi-org.proxy.wm.edu/10.1145/2635868.2635875},
    doi = {10.1145/2635868.2635875},
    abstract = { The n-gram language model, which has its roots in statistical natural language processing,
    has been shown to successfully capture the repetitive and predictable regularities
    (“naturalness") of source code, and help with tasks such as code suggestion, porting,
    and designing assistive coding devices. However, we show in this paper that this natural-language-based
    model fails to exploit a special property of source code: localness. We find that
    human-written programs are localized: they have useful local regularities that can
    be captured and exploited. We introduce a novel cache language model that consists
    of both an n-gram and an added “cache" component to exploit localness. We show empirically
    that the additional cache component greatly improves the n-gram approach by capturing
    the localness of software, as measured by both cross-entropy and suggestion accuracy.
    Our model’s suggestion accuracy is actually comparable to a state-of-the-art, semantically
    augmented language model; but it is simpler and easier to implement. Our cache language
    model requires nothing beyond lexicalization, and thus is applicable to all programming
    languages. },
    booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
    pages = {269–280},
    numpages = {12},
    keywords = {Localness, Cache Language Model, Code Suggestion},
    location = {Hong Kong, China},
    series = {FSE 2014}
}

@inproceedings{RahmanICSE'19, author = {Rahman, Musfiqur and Palani, Dharani and Rigby, Peter C.}, title = {Natural Software Revisited}, year = {2019}, publisher = {IEEE Press}, url = {https://doi.org/10.1109/ICSE.2019.00022}, doi = {10.1109/ICSE.2019.00022}, booktitle = {Proceedings of the 41st International Conference on Software Engineering}, pages = {37–48}, numpages = {12}, keywords = {statistical code graphs, language models, basic science, StackOverflow, entropy}, location = {Montreal, Quebec, Canada}, series = {ICSE '19} }

@inproceedings{Nguyen2013ASS,
  title={A statistical semantic language model for source code},
  author={T. Nguyen and A. Nguyen and H. Nguyen},
  booktitle={ESEC/FSE 2013},
  year={2013}
}

@article{Raychev2014CodeCW,
  title={Code completion with statistical language models},
  author={Veselin Raychev and Martin T. Vechev and Eran Yahav},
  journal={Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  year={2014}
}

@inproceedings{hindle2012natural,
    author = {Hindle, Abram and Barr, Earl T. and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar},
    title = {On the Naturalness of Software},
    year = {2012},
    isbn = {9781467310673},
    publisher = {IEEE Press},
    abstract = { Natural languages like English are rich, complex, and powerful. The highly creative
    and graceful use of languages like English and Tamil, by masters like Shakespeare
    and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive
    constraints and the exigencies of daily life, most human utterances are far simpler
    and much more repetitive and predictable. In fact, these utterances can be very usefully
    modeled using modern statistical methods. This fact has led to the phenomenal success
    of statistical approaches to speech recognition, natural language translation, question-answering,
    and text mining and comprehension. We begin with the conjecture that most software
    is also natural, in the sense that it is created by humans at work, with all the attendant
    constraints and limitations---and thus, like natural language, it is also likely to
    be repetitive and predictable. We then proceed to ask whether a) code can be usefully
    modeled by statistical language models and b) such models can be leveraged to support
    software engineers. Using the widely adopted n-gram model, we provide empirical evidence
    supportive of a positive answer to both these questions. We show that code is also
    very repetitive, and in fact even more so than natural languages. As an example use
    of the model, we have developed a simple code completion engine for Java that, despite
    its simplicity, already improves Eclipse's completion capability. We conclude the
    paper by laying out a vision for future research in this area. },
    booktitle = {Proceedings of the 34th International Conference on Software Engineering},
    pages = {837–847},
    numpages = {11},
    location = {Zurich, Switzerland},
    series = {ICSE '12}
}

@inproceedings{gabel2010unique,
    author = {Gabel, Mark and Su, Zhendong},
    title = {A Study of the Uniqueness of Source Code},
    year = {2010},
    isbn = {9781605587912},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi-org.proxy.wm.edu/10.1145/1882291.1882315},
    doi = {10.1145/1882291.1882315},
    abstract = {This paper presents the results of the first study of the uniqueness of source code.
    We define the uniqueness of a unit of source code with respect to the entire body
    of written software, which we approximate with a corpus of 420 million lines of source
    code. Our high-level methodology consists of examining a collection of 6,000 software
    projects and measuring the degree to which each project can be `assembled' solely
    from portions of this corpus, thus providing a precise measure of `uniqueness' that
    we call syntactic redundancy. We parameterized our study over a variety of variables,
    the most important of which being the level of granularity at which we view source
    code. Our suite of experiments together consumed approximately four months of CPU
    time, providing quantitative answers to the following questions: at what levels of
    granularity is software unique, and at a given level of granularity, how unique is
    software? While we believe these questions to be of intrinsic interest, we discuss
    possible applications to genetic programming and developer productivity tools.},
    booktitle = {Proceedings of the Eighteenth ACM SIGSOFT International Symposium on Foundations of Software Engineering},
    pages = {147–156},
    numpages = {10},
    keywords = {software uniqueness, large scale study, source code},
    location = {Santa Fe, New Mexico, USA},
    series = {FSE '10}
}

@inproceedings{khandelwal2018sharp,
    title = "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context",
    author = "Khandelwal, Urvashi  and
      He, He  and
      Qi, Peng  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1027",
    doi = "10.18653/v1/P18-1027",
    pages = "284--294",
    abstract = "We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.",
}

@inproceedings{
    wang2018glue,
    title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
    author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=rJ4km2R5t7},
}

@inproceedings{rajpurkar2018squad,
    title = "Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}",
    author = "Rajpurkar, Pranav  and
      Jia, Robin  and
      Liang, Percy",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-2124",
    doi = "10.18653/v1/P18-2124",
    pages = "784--789",
    abstract = "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD achieves only 66{\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.",
}

@article{goh2021multimodal,
  author = {Goh, Gabriel and †, Nick Cammarata and †, Chelsea Voss and Carter, Shan and Petrov, Michael and Schubert, Ludwig and Radford, Alec and Olah, Chris},
  title = {Multimodal Neurons in Artificial Neural Networks},
  journal = {Distill},
  year = {2021},
  note = {https://distill.pub/2021/multimodal-neurons},
  doi = {10.23915/distill.00030}
}

@article{dai2021knowledge,
  author    = {Damai Dai and
               Li Dong and
               Yaru Hao and
               Zhifang Sui and
               Furu Wei},
  title     = {Knowledge Neurons in Pretrained Transformers},
  journal   = {CoRR},
  volume    = {abs/2104.08696},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.08696},
  archivePrefix = {arXiv},
  eprint    = {2104.08696},
  timestamp = {Wed, 02 Jun 2021 10:17:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08696.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{husain2019codesearchnet,
  title={{CodeSearchNet} challenge: Evaluating the state of semantic code search},
  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
  journal={arXiv preprint arXiv:1909.09436},
  year={2019}
}


@article{lu2021codexglue,
  author    = {Shuai Lu and
               Daya Guo and
               Shuo Ren and
               Junjie Huang and
               Alexey Svyatkovskiy and
               Ambrosio Blanco and
               Colin B. Clement and
               Dawn Drain and
               Daxin Jiang and
               Duyu Tang and
               Ge Li and
               Lidong Zhou and
               Linjun Shou and
               Long Zhou and
               Michele Tufano and
               Ming Gong and
               Ming Zhou and
               Nan Duan and
               Neel Sundaresan and
               Shao Kun Deng and
               Shengyu Fu and
               Shujie Liu},
  title     = {CodeXGLUE: {A} Machine Learning Benchmark Dataset for Code Understanding
               and Generation},
  journal   = {CoRR},
  volume    = {abs/2102.04664},
  year      = {2021}
}

@article{rabin2021generalizability,
  title={On the generalizability of Neural Program Models with respect to semantic-preserving program transformations},
  author={Rabin, Md Rafiqul Islam and Bui, Nghi DQ and Wang, Ke and Yu, Yijun and Jiang, Lingxiao and Alipour, Mohammad Amin},
  journal={Information and Software Technology},
  volume={135},
  pages={106552},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{tenney2019bert,
    title = "{BERT} Rediscovers the Classical {NLP} Pipeline",
    author = "Tenney, Ian  and
      Das, Dipanjan  and
      Pavlick, Ellie",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1452",
    doi = "10.18653/v1/P19-1452",
    pages = "4593--4601",
    abstract = "Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",
}

@inproceedings{rogers2018whats,
    title = "What{'}s in Your Embedding, And How It Predicts Task Performance",
    author = "Rogers, Anna  and
      Hosur Ananthakrishna, Shashwath  and
      Rumshisky, Anna",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1228",
    pages = "2690--2703",
    abstract = "Attempts to find a single technique for general-purpose intrinsic evaluation of word embeddings have so far not been successful. We present a new approach based on scaled-up qualitative analysis of word vector neighborhoods that quantifies interpretable characteristics of a given model (e.g. its preference for synonyms or shared morphological forms as nearest neighbors). We analyze 21 such factors and show how they correlate with performance on 14 extrinsic and intrinsic task datasets (and also explain the lack of correlation between some of them). Our approach enables multi-faceted evaluation, parameter search, and generally {--} a more principled, hypothesis-driven approach to development of distributional semantic representations.",
}

@inproceedings{prabhakaran2019perturbation,
    title = "Perturbation Sensitivity Analysis to Detect Unintended Model Biases",
    author = "Prabhakaran, Vinodkumar  and
      Hutchinson, Ben  and
      Mitchell, Margaret",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1578",
    doi = "10.18653/v1/D19-1578",
    pages = "5740--5745",
    abstract = "Data-driven statistical Natural Language Processing (NLP) techniques leverage large amounts of language data to build models that can understand language. However, most language data reflect the public discourse at the time the data was produced, and hence NLP models are susceptible to learning incidental associations around named referents at a particular point in time, in addition to general linguistic meaning. An NLP system designed to model notions such as sentiment and toxicity should ideally produce scores that are independent of the identity of such entities mentioned in text and their social associations. For example, in a general purpose sentiment analysis system, a phrase such as I hate Katy Perry should be interpreted as having the same sentiment as I hate Taylor Swift. Based on this idea, we propose a generic evaluation framework, Perturbation Sensitivity Analysis, which detects unintended model biases related to named entities, and requires no new annotations or corpora. We demonstrate the utility of this analysis by employing it on two different NLP models {---} a sentiment model and a toxicity model {---} applied on online comments in English language from four different genres.",
}

@inproceedings{kim2019probing,
    title = "Probing What Different {NLP} Tasks Teach Machines about Function Word Comprehension",
    author = "Kim, Najoung  and
      Patel, Roma  and
      Poliak, Adam  and
      Xia, Patrick  and
      Wang, Alex  and
      McCoy, Tom  and
      Tenney, Ian  and
      Ross, Alexis  and
      Linzen, Tal  and
      Van Durme, Benjamin  and
      Bowman, Samuel R.  and
      Pavlick, Ellie",
    booktitle = "Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S19-1026",
    doi = "10.18653/v1/S19-1026",
    pages = "235--249",
    abstract = "We introduce a set of nine challenge tasks that test for the understanding of function words. These tasks are created by structurally mutating sentences from existing datasets to target the comprehension of specific types of function words (e.g., prepositions, wh-words). Using these probing tasks, we explore the effects of various pretraining objectives for sentence encoders (e.g., language modeling, CCG supertagging and natural language inference (NLI)) on the learned representations. Our results show that pretraining on CCG{---}our most syntactic objective{---}performs the best on average across our probing tasks, suggesting that syntactic knowledge helps function word comprehension. Language modeling also shows strong performance, supporting its widespread use for pretraining state-of-the-art NLP models. Overall, no pretraining objective dominates across the board, and our function word probing tasks highlight several intuitive differences between pretraining objectives, e.g., that NLI helps the comprehension of negation.",
}

@inproceedings{wu2019errudite,
    title = "{E}rrudite: Scalable, Reproducible, and Testable Error Analysis",
    author = "Wu, Tongshuang  and
      Ribeiro, Marco Tulio  and
      Heer, Jeffrey  and
      Weld, Daniel",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1073",
    doi = "10.18653/v1/P19-1073",
    pages = "747--763",
    abstract = "Though error analysis is crucial to understanding and improving NLP models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. This paper codifies model and task agnostic principles for informative error analysis, and presents Errudite, an interactive tool for better supporting this process. First, error groups should be precisely defined for reproducibility; Errudite supports this with an expressive domain-specific language. Second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; Errudite enables systematic grouping of relevant instances with filtering queries. Third, hypotheses about the cause of errors should be explicitly tested; Errudite supports this via automated counterfactual rewriting. We validate our approach with a user study, finding that Errudite (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs.",
}

@inproceedings{ribeiro2018semantically,
    title = "Semantically Equivalent Adversarial Rules for Debugging {NLP} models",
    author = "Ribeiro, Marco Tulio  and
      Singh, Sameer  and
      Guestrin, Carlos",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1079",
    doi = "10.18653/v1/P18-1079",
    pages = "856--865",
    abstract = "Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) {--} semantic-preserving perturbations that induce changes in the model{'}s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) {--} simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.",
}


@article{jedlitschka2014reporting,
  title={Reporting experiments to satisfy professionals’ information needs},
  author={Jedlitschka, Andreas and Juristo, Natalia and Rombach, Dieter},
  journal={Empirical Software Engineering},
  volume={19},
  number={6},
  pages={1921--1955},
  year={2014},
  publisher={Springer}
}

@book{wohlin2012experimentation,
  title={Experimentation in software engineering},
  author={Wohlin, Claes and Runeson, Per and H{\"o}st, Martin and Ohlsson, Magnus C and Regnell, Bj{\"o}rn and Wessl{\'e}n, Anders},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@book{menzies2016ds4se,
  title={Perspectives on data science for software engineering},
  author={Menzies, Tim and Williams, Laurie and Zimmermann, Thomas},
  year={2016},
  publisher={Morgan Kaufmann}
}

@article{arcuri2014guide,
    author = {Arcuri, Andrea and Briand, Lionel},
    title = {A Hitchhiker's Guide to Statistical Tests for Assessing Randomized Algorithms in Software Engineering},
    year = {2014},
    issue_date = {May 2014},
    publisher = {John Wiley and Sons Ltd.},
    address = {GBR},
    volume = {24},
    number = {3},
    issn = {0960-0833},
    url = {https://doi.org/10.1002/stvr.1486},
    doi = {10.1002/stvr.1486},
    abstract = {Randomized algorithms are widely used to address many types of software engineering
    problems, especially in the area of software verification and validation with a strong
    emphasis on test automation. However, randomized algorithms are affected by chance
    and so require the use of appropriate statistical tests to be properly analysed in
    a sound manner. This paper features a systematic review regarding recent publications
    in 2009 and 2010 showing that, overall, empirical analyses involving randomized algorithms
    in software engineering tend to not properly account for the random nature of these
    algorithms. Many of the novel techniques presented clearly appear promising, but the
    lack of soundness in their empirical evaluations casts unfortunate doubts on their
    actual usefulness. In software engineering, although there are guidelines on how to
    carry out empirical analyses involving human subjects, those guidelines are not directly
    and fully applicable to randomized algorithms. Furthermore, many of the textbooks
    on statistical analysis are written from the viewpoints of social and natural sciences,
    which present different challenges from randomized algorithms. To address the questionable
    overall quality of the empirical analyses reported in the systematic review, this
    paper provides guidelines on how to carry out and properly analyse randomized algorithms
    applied to solve software engineering tasks, with a particular focus on software testing,
    which is by far the most frequent application area of randomized algorithms within
    software engineering. Copyright © 2012 John Wiley &amp; Sons, Ltd.},
    journal = {Softw. Test. Verif. Reliab.},
    month = may,
    pages = {219–250},
    numpages = {32},
    keywords = {effect size, confidence interval, systematic review, statistical difference, nonparametric test, survey, parametric test, Bonferroni adjustment}
}

@ARTICLE{furia2019bayesianse,  author={Furia, Carlo Alberto and Feldt, Robert and Torkar, Richard},  journal={IEEE Transactions on Software Engineering},   title={Bayesian Data Analysis in Empirical Software Engineering Research},   year={2019},  volume={},  number={},  pages={1-1},  doi={10.1109/TSE.2019.2935974}}

@inproceedings{vaswani2017transformers,
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
    title = {Attention is All You Need},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional
    neural networks that include an encoder and a decoder. The best performing models
    also connect the encoder and decoder through an attention mechanism. We propose a
    new simple network architecture, the Transformer, based solely on attention mechanisms,
    dispensing with recurrence and convolutions entirely. Experiments on two machine translation
    tasks show these models to be superior in quality while being more parallelizable
    and requiring significantly less time to train. Our model achieves 28.4 BLEU on the
    WMT 2014 English-to-German translation task, improving over the existing best results,
    including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation
    task, our model establishes a new single-model state-of-the-art BLEU score of 41.0
    after training for 3.5 days on eight GPUs, a small fraction of the training costs
    of the best models from the literature.},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {6000–6010},
    numpages = {11},
    location = {Long Beach, California, USA},
    series = {NeurIPs'17}
}

@article{dehghani2021benchmark,
  author    = {Mostafa Dehghani and
               Yi Tay and
               Alexey A. Gritsenko and
               Zhe Zhao and
               Neil Houlsby and
               Fernando Diaz and
               Donald Metzler and
               Oriol Vinyals},
  title     = {The Benchmark Lottery},
  journal   = {CoRR},
  volume    = {abs/2107.07002},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.07002},
  archivePrefix = {arXiv},
  eprint    = {2107.07002},
  timestamp = {Wed, 21 Jul 2021 15:55:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-07002.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{jain1999rnn,
    author = {Jain, L. C. and Medsker, L. R.},
    title = {Recurrent Neural Networks: Design and Applications},
    year = {1999},
    isbn = {0849371813},
    publisher = {CRC Press, Inc.},
    address = {USA},
    edition = {1st},
    abstract = {From the Publisher:With applications ranging from motion detection to financial forecasting,
    recurrent neural networks (RNNs) have emerged as an interesting and important part
    of neural network research. Recurrent Neural Networks: Design and Applications reflects
    the tremendous, worldwide interest in and virtually unlimited potential of RNNs -
    providing a summary of the design, applications, current research, and challenges
    of this dynamic and promising field.}
}

@inproceedings{bender2021parrots,
    author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Mitchell, Margaret},
    title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
    year = {2021},
    isbn = {9781450383097},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi-org.proxy.wm.edu/10.1145/3442188.3445922},
    doi = {10.1145/3442188.3445922},
    abstract = {The past 3 years of work in NLP have been characterized by the development and deployment
    of ever larger language models, especially for English. BERT, its variants, GPT-2/3,
    and others, most recently Switch-C, have pushed the boundaries of the possible both
    through architectural innovations and through sheer size. Using these pretrained models
    and the methodology of fine-tuning them for specific tasks, researchers have extended
    the state of the art on a wide array of tasks as measured by leaderboards on specific
    benchmarks for English. In this paper, we take a step back and ask: How big is too
    big? What are the possible risks associated with this technology and what paths are
    available for mitigating those risks? We provide recommendations including weighing
    the environmental and financial costs first, investing resources into curating and
    carefully documenting datasets rather than ingesting everything on the web, carrying
    out pre-development exercises evaluating how the planned approach fits into research
    and development goals and supports stakeholder values, and encouraging research directions
    beyond ever larger language models.},
    booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
    pages = {610–623},
    numpages = {14},
    location = {Virtual Event, Canada},
    series = {FAccT '21}
}

@misc{chen2021evaluating,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
 
 @misc{ciniselli2021empirical,
      title={An Empirical Study on the Usage of Transformer Models for Code Completion}, 
      author={Matteo Ciniselli and Nathan Cooper and Luca Pascarella and Antonio Mastropaolo and Emad Aghajani and Denys Poshyvanyk and Massimiliano Di Penta and Gabriele Bavota},
      year={2021},
      eprint={2108.01585},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

 @misc{tabnine, title={Code faster with ai code completions}, url={https://www.tabnine.com/}, journal={Code Faster with AI Code Completions}, publisher={Tabnine}} 
 
 @misc{openai_codex, title={OpenAI Codex}, url={https://openai.com/blog/openai-codex/}, journal={OpenAI}, publisher={OpenAI}, author={Zaremba, Wojciech and Brockman, Greg and OpenAI}, year={2021}, month={Aug}} 

@misc{github_copilot, title={GitHub Copilot · Your AI pair programmer}, url={https://copilot.github.com/}, journal={GitHub Copilot}, author={GitHub} }

@misc{intellicode, title={Visual Studio IntelliCode | Visual Studio - Visual Studio}, url={https://visualstudio.microsoft.com/services/intellicode/}, journal={visualstudio.microsoft.com}, author={Microsoft} }

@article{karpathy2015understand,
  author    = {Andrej Karpathy and
               Justin Johnson and
               Fei{-}Fei Li},
  title     = {Visualizing and Understanding Recurrent Networks},
  journal   = {CoRR},
  volume    = {abs/1506.02078},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.02078},
  archivePrefix = {arXiv},
  eprint    = {1506.02078},
  timestamp = {Sat, 19 Oct 2019 16:30:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KarpathyJL15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{molnar2019interpret,
  title      = {Interpretable Machine Learning},
  author     = {Christoph Molnar},
  note       = {\url{https://christophm.github.io/interpretable-ml-book/}},
  year       = {2019},
  subtitle   = {A Guide for Making Black Box Models Explainable}
}

@article{kocmi2021ship,
  author    = {Tom Kocmi and
               Christian Federmann and
               Roman Grundkiewicz and
               Marcin Junczys{-}Dowmunt and
               Hitokazu Matsushita and
               Arul Menezes},
  title     = {To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics
               for Machine Translation},
  journal   = {CoRR},
  volume    = {abs/2107.10821},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.10821},
  archivePrefix = {arXiv},
  eprint    = {2107.10821},
  timestamp = {Thu, 29 Jul 2021 16:14:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-10821.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{rei2020comet,
    title = "{COMET}: A Neural Framework for {MT} Evaluation",
    author = "Rei, Ricardo  and
      Stewart, Craig  and
      Farinha, Ana C  and
      Lavie, Alon",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.213",
    doi = "10.18653/v1/2020.emnlp-main.213",
    pages = "2685--2702",
    abstract = "We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements. Our framework leverages recent breakthroughs in cross-lingual pretrained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality. To showcase our framework, we train three models with different types of human judgements: Direct Assessments, Human-mediated Translation Edit Rate and Multidimensional Quality Metric. Our models achieve new state-of-the-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems.",
}

@inproceedings{ribeiro2020checklist,
    title = "Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist",
    author = "Ribeiro, Marco Tulio  and
      Wu, Tongshuang  and
      Guestrin, Carlos  and
      Singh, Sameer",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.442",
    doi = "10.18653/v1/2020.acl-main.442",
    pages = "4902--4912",
    abstract = "Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.",
}

@article{watson2020dl4se,
  author    = {Cody Watson and
               Nathan Cooper and
               David Nader{-}Palacio and
               Kevin Moran and
               Denys Poshyvanyk},
  title     = {A Systematic Literature Review on the Use of Deep Learning in Software
               Engineering Research},
  journal   = {CoRR},
  volume    = {abs/2009.06520},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.06520},
  archivePrefix = {arXiv},
  eprint    = {2009.06520},
  timestamp = {Fri, 18 Sep 2020 15:17:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-06520.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{murphy2006ide,
author = {Murphy, Gail C. and Kersten, Mik and Findlater, Leah},
title = {How Are Java Software Developers Using the Eclipse IDE?},
year = {2006},
issue_date = {July 2006},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {23},
number = {4},
issn = {0740-7459},
url = {https://doi.org/10.1109/MS.2006.105},
doi = {10.1109/MS.2006.105},
abstract = {Eclipse is a leading development environment that provides a rich set of features
supporting Java development. However, little data is available about its usage. Usage
data from 41 developers using Java and Eclipse shows that they're using advanced features
such as refactoring and are extending the environment using third-party tools. However,
they rarely use some of the other features, such as bookmarking places in the code.
The article also includes briefly describes the authors' Eclipse-based open-source
analysis framework. Open-source projects such as Eclipse should be gathering and analyzing
more usage data to ensure the tools they're building evolve to meet user communities'
needs.},
journal = {IEEE Softw.},
month = jul,
pages = {76–83},
numpages = {8},
keywords = {programming environments, construction tools, coding tools and techniques}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% FROM MENDELEY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
@article{Karampatsis2019,
    title = {{Maybe deep neural networks are the best choice for modeling source code}},
    year = {2019},
    journal = {arXiv},
    author = {Karampatsis, Rafael Michael and Sutton, Charles},
    number = {Lm},
    issn = {23318422},
    arxivId = {1903.05734},
    keywords = {BPE, Code, Language models, Naturalness, Neural network, Software tools}
}

@inproceedings{White:MSR15,
    title = {{Toward Deep Learning Software Repositories}},
    year = {2015},
    booktitle = {Proceedings of the 12th IEEE Working Conference on Mining Software Repositories (MSR'15)},
    author = {White, Martin and Vendome, Christopher and Linares-V{\'{a}}squez, Mario and Poshyvanyk, Denys},
    pages = {334--345},
    series = {MSR '15},
    publisher = {IEEE Press},
    url = {http://dl.acm.org/citation.cfm?id=2820518.2820559},
    address = {Piscataway, NJ, USA},
    isbn = {978-0-7695-5594-2},
    keywords = {deep learning, machine learning, n-grams, neural networks, software language models, software repositories}
}

@article{Arcuri2014AEngineering,
    title = {{A Hitchhiker's guide to statistical tests for assessing randomized algorithms in software engineering}},
    year = {2014},
    journal = {Software Testing Verification and Reliability},
    author = {Arcuri, Andrea and Briand, Lionel},
    number = {3},
    pages = {219--250},
    volume = {24},
    doi = {10.1002/stvr.1486},
    issn = {10991689},
    keywords = {Bonferroni adjustment, confidence interval, effect size, nonparametric test, parametric test, statistical difference, survey, systematic review}
}

@article{Torkar2021AEngineering,
    title = {{A Method to Assess and Argue for Practical Significance in Software Engineering}},
    year = {2021},
    journal = {IEEE Transactions on Software Engineering},
    author = {Torkar, Richard and Furia, Carlo Alberto and Feldt, Robert and Gomes de Oliveira Neto, Francisco and Gren, Lucas and Lenberg, Per and Ernst, Neil A.},
    number = {X},
    pages = {1--13},
    volume = {XX},
    doi = {10.1109/TSE.2020.3048991},
    issn = {19393520},
    arxivId = {1809.09849},
    keywords = {Analytical models, Bayes methods, Bayesian analysis, Data models, Decision making, Software engineering, Statistical analysis, Testing, empirical software engineering, practical significance, statistical significance}
}

@article{Bengio2003AModel,
    title = {{A neural probabilistic language model}},
    year = {2003},
    journal = {Advances in Neural Information Processing Systems},
    author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal},
    pages = {1137--1155},
    volume = {3},
    isbn = {0262122413},
    issn = {10495258},
    keywords = {artificial neural networks, curse of dimensionality, distributed representation, statistical language modeling}
}

@article{Hellendoorn2017AreCode,
    title = {{Are deep neural networks the best choice for modeling source code?}},
    year = {2017},
    author = {Hellendoorn, Vincent J. and Devanbu, Premkumar},
    pages = {763--773},
    isbn = {9781450351058},
    doi = {10.1145/3106237.3106290},
    keywords = {2017, acm reference format, are deep neural, hellendoorn and premkumar devanbu, language models, naturalness, software tools, vincent j}
}

@article{Furia2019BayesianResearch,
    title = {{Bayesian Data Analysis in Empirical Software Engineering Research}},
    year = {2019},
    journal = {IEEE Transactions on Software Engineering},
    author = {Furia, Carlo Alberto and Feldt, Robert and Torkar, Richard},
    pages = {1--26},
    doi = {10.1109/TSE.2019.2935974},
    issn = {19393520},
    arxivId = {1811.05422},
    keywords = {Bayesian data analysis, empirical software engineering, statistical analysis, statistical hypothesis testing}
}

@article{Doshi-Velez2018ConsiderationsLearning,
    title = {{Considerations for Evaluation and Generalization in Interpretable Machine Learning}},
    year = {2018},
    author = {Doshi-Velez, Finale and Kim, Been},
    pages = {3--17},
    doi = {10.1007/978-3-319-98131-4{\_}1}
}

@article{Hernan2019DataTasks.,
    title = {{Data science is science’s second chance to get causal inference right. A classification of data science tasks.}},
    year = {2019},
    journal = {arXiv},
    author = {Hernan, Miguel A. and Hsu, John and Brian, Healy},
    number = {1},
    pages = {3--4},
    volume = {56}
}

@article{Sharma2021DoWhyAssumptions,
    title = {{DoWhy : Addressing Challenges in Expressing and Validating Causal Assumptions}},
    year = {2021},
    author = {Sharma, Amit and Syrgkanis, Vasilis and Zhang, Cheng and Kıcıman, Emre}
}

@article{PaulRalphNaumanbinAliSebastianBaltesDomenicoBianculliJessicaDiazYvonneDittrichNeilErnstMichaelFeldererRobertFeldtAntonioFilieriBrenoBernardNicolaudeFrancaCarloAlbertoFuriaGregGayNicolasGoldDanielGraziotinP2020EmpiricalResearch,
    title = {{Empirical Standards for Software Engineering Research}},
    year = {2020},
    journal = {arXiv},
    author = {Paul Ralph, Nauman bin Ali, Sebastian Baltes, Domenico Bianculli, Jessica Diaz, Yvonne Dittrich, Neil Ernst, Michael Felderer, Robert Feldt, Antonio Filieri, Breno Bernard Nicolau de Fran{\c{c}}a, Carlo Alberto Furia, Greg Gay, Nicolas Gold, Daniel Graziotin, P, Sira Vegas},
    pages = {1--20}
}

@article{Velmurugan2020EvaluatingApproach,
    title = {{Evaluating Explainable Methods for Predictive Process Analytics: A Functionally-Grounded Approach}},
    year = {2020},
    author = {Velmurugan, Mythreyi and Ouyang, Chun and Moreira, Catarina and Sindhgatta, Renuka},
    url = {http://arxiv.org/abs/2012.04218},
    arxivId = {2012.04218},
    keywords = {evaluation, explainable ai, explanation fidelity, explanation stability, metrics, predictive process analytics}
}

@article{Chen2021EvaluatingCode,
    title = {{Evaluating Large Language Models Trained on Code}},
    year = {2021},
    author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
    url = {http://arxiv.org/abs/2107.03374},
    arxivId = {2107.03374}
}

@article{Gilpin2019ExplainingLearning,
    title = {{Explaining explanations: An overview of interpretability of machine learning}},
    year = {2019},
    journal = {Proceedings - 2018 IEEE 5th International Conference on Data Science and Advanced Analytics, DSAA 2018},
    author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
    pages = {80--89},
    isbn = {9781538650905},
    doi = {10.1109/DSAA.2018.00018},
    arxivId = {1806.00069},
    keywords = {Deep learning and deep analytics, Fairness and transparency in data science, Machine learning theories, Models and systems}
}

@article{Kim2018InterpretabilityTCAV,
    title = {{Interpretability beyond feature attribution: Quantitative Testing with Concept Activation Vectors (TCAV)}},
    year = {2018},
    journal = {35th International Conference on Machine Learning, ICML 2018},
    author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
    pages = {4186--4195},
    volume = {6},
    isbn = {9781510867963},
    arxivId = {1711.11279}
}

@article{Molnar2020InterpretableChallenges,
    title = {{Interpretable Machine Learning -- A Brief History, State-of-the-Art and Challenges}},
    year = {2020},
    author = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
    number = {01},
    pages = {1--15},
    url = {http://arxiv.org/abs/2010.09337},
    arxivId = {2010.09337},
    keywords = {explainable artificial in-, interpretable machine learning}
}

@book{Bender2021OnBig,
    title = {{On the dangers of stochastic parrots: Can language models be too big?}},
    year = {2021},
    booktitle = {FAccT 2021 - Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
    author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
    number = {1},
    pages = {610--623},
    volume = {1},
    publisher = {Association for Computing Machinery},
    isbn = {9781450383097},
    doi = {10.1145/3442188.3445922}
}

@article{Bommasani2021OnModels,
    title = {{On the Opportunities and Risks of Foundation Models}},
    year = {2021},
    author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Kohd, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and R{\'{e}}, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tram{\`{e}}r, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
    pages = {1--212},
    url = {http://arxiv.org/abs/2108.07258},
    arxivId = {2108.07258}
}

@article{Karampatsis2020Open-VocabularyAbstract,
    title = {{Open-Vocabulary Models for Source Code (Extended Abstract)}},
    year = {2020},
    journal = {Proceedings - 2020 ACM/IEEE 42nd International Conference on Software Engineering: Companion, ICSE-Companion 2020},
    author = {Karampatsis, Rafael Michael and Babii, Hlib and Robbes, Romain and Sutton, Charles and Janes, Andrea},
    pages = {294--295},
    isbn = {9781450371223},
    doi = {10.1145/3377812.3390806},
    issn = {02705257},
    keywords = {Byte-Pair Encoding, Naturalness of code, Neural Language Models}
}

@article{Svajlenko2015EvaluatingBigCloneBench,
    title = {{Evaluating clone detection tools with BigCloneBench}},
    year = {2015},
    journal = {2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings},
    author = {Svajlenko, Jeffrey and Roy, Chanchal K.},
    pages = {131--140},
    publisher = {IEEE},
    isbn = {9781467375320},
    doi = {10.1109/ICSM.2015.7332459},
    keywords = {Benchmark testing, Big data, Cloning, Data mining, Java, Software systems}
}

@article{Tufano2019LearningBug-Fixes,
    title = {{Learning How to Mutate Source Code from Bug-Fixes}},
    year = {2019},
    journal = {Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019},
    author = {Tufano, Michele and Watson, Cody and Bavota, Gabriele and Di Penta, Massimiliano and White, Martin and Poshyvanyk, Denys},
    pages = {301--312},
    isbn = {9781728130941},
    doi = {10.1109/ICSME.2019.00046},
    arxivId = {1812.10772},
    keywords = {Mutation testing, deep learning, neural networks}
}

@article{Karampatsis2020BigCode,
    title = {{Big code != big vocabulary: Open-vocabulary models for source code}},
    year = {2020},
    journal = {Proceedings - International Conference on Software Engineering},
    author = {Karampatsis, Rafael Michael and Babii, Hlib and Robbes, Romain and Sutton, Charles and Janes, Andrea},
    pages = {1073--1085},
    isbn = {9781450371216},
    doi = {10.1145/3377811.3380342},
    issn = {02705257},
    arxivId = {2003.07914},
    keywords = {Byte-pair encoding, Naturalness of code, Neural language models}
}

@article{Austin2021ProgramModels,
    title = {{Program Synthesis with Large Language Models}},
    year = {2021},
    author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles},
    pages = {1--34},
    url = {http://arxiv.org/abs/2108.07732},
    arxivId = {2108.07732}
}

@article{Clement2020PyMT5:Transformers,
    title = {{PyMT5: multi-mode translation of natural language and Python code with transformers}},
    year = {2020},
    author = {Clement, Colin and Drain, Dawn and Timcheck, Jonathan and Svyatkovskiy, Alexey and Sundaresan, Neel},
    pages = {9052--9065},
    doi = {10.18653/v1/2020.emnlp-main.728},
    arxivId = {2010.03150}
}

@article{Agarwal2020QualityTranslation,
    title = {{Quality estimation {\&} interpretability for code translation}},
    year = {2020},
    journal = {arXiv},
    author = {Agarwal, Mayank and Talamadupula, Kartik and Houde, Stephanie and Martinez, Fernando and Muller, Michael and Richards, John and Ross, Steven and Weisz, Justin D.},
    issn = {23318422},
    arxivId = {2012.07581}
}

@article{Amrhein2019RetireSignatories,
    title = {{Retire statistical significance Valentin Amrhein, Sander Greenland, Blake McShane and more than 800 signatories}},
    year = {2019},
    journal = {Nature},
    author = {Amrhein, Valentin and Greenland, Sander and McShane, Blake},
    pages = {305--307},
    volume = {567}
}

@book{Leblanc2015SoftwareEngineers,
    title = {{Software Metrics Fundamentals of Dependable Computing for Software Engineers}},
    year = {2015},
    author = {Leblanc, Richard and Dingle, Adair and Hagar, Jon Duncan and Knight, John},
    isbn = {9781439838235}
}

@article{Mastropaolo2021StudyingTasks,
    title = {{Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks}},
    year = {2021},
    author = {Mastropaolo, Antonio and Scalabrino, Simone and Cooper, Nathan and Nader Palacio, David and Poshyvanyk, Denys and Oliveto, Rocco and Bavota, Gabriele},
    pages = {336--347},
    doi = {10.1109/icse43902.2021.00041},
    arxivId = {2102.02017}
}

@article{Doshi-Velez2017TowardsLearning,
    title = {{Towards A Rigorous Science of Interpretable Machine Learning}},
    year = {2017},
    author = {Doshi-Velez, Finale and Kim, Been},
    number = {Ml},
    pages = {1--13},
    url = {http://arxiv.org/abs/1702.08608},
    arxivId = {1702.08608}
}

@article{Hermans2013TrainingNetworks,
    title = {{Training and Analyzing Deep Recurrent Neural Networks}},
    year = {2013},
    journal = {Advances in Neural Information Processing Systems},
    author = {Hermans, Michiel and Schrauwen, Benjamin},
    issn = {04706625},
    pmid = {5404171}
}

@article{DAmour2020UnderspecificationLearning,
    title = {{Underspecification Presents Challenges for Credibility in Modern Machine Learning}},
    year = {2020},
    author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
    url = {http://arxiv.org/abs/2011.03395},
    arxivId = {2011.03395}
}

@article{Karpathy2015VisualizingNetworks,
    title = {{Visualizing and Understanding Recurrent Networks}},
    year = {2015},
    author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
    pages = {1--12},
    url = {http://arxiv.org/abs/1506.02078},
    arxivId = {1506.02078}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END FROM MENDELEY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@book{Pearl2009Causality,
    title = {{Causality: models, reasoning, and inference}},
    year = {2009},
    author = {Pearl, Judea},
    isbn = {978-0-521-89560-0}
}

@book{Pearl2016Causality,
    title = {{Causal Inference in Statistics, A Primer}},
    year = {2016},
    author = {Pearl, Judea and Glymour, Madelyn and P.Jewell, Nicholas},
    isbn = {978-0-521-89560-0}
}

@book{Pearl2018Causality,
    title = {{The book of why: The New Science of Cause and Effect}},
    year = {2018},
    author = {Pearl, Judea and Mackenzie, Dana},
    isbn = {978-0465097609}
}

@manual{aniche-ck,
  title={Java code metrics calculator (CK)},
  author={Maurício Aniche},
  year={2015},
  note={Available in https://github.com/mauricioaniche/ck/}
}

@misc{dowhy,
author={Sharma, Amit and Kiciman, Emre and others},
title={Do{W}hy: {A Python package for causal inference}},
howpublished={https://github.com/microsoft/dowhy},
year={2019}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 